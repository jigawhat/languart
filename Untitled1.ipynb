{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ac8dd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLNetTokenizer, XLNetLMHeadModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30269930",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizerr = XLNetTokenizer.from_pretrained('xlnet-large-cased')\n",
    "modell = XLNetLMHeadModel.from_pretrained('xlnet-large-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7293e879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We show how to setup inputs to predict a next token using a bi-directional context.\n",
    "# input_ids = torch.tensor(tokenizerr.encode(\"Hello, my dog is very<mask>\", add_special_tokens=False)).unsqueeze(0) \n",
    "input_ids = torch.tensor(tokenizerr.encode(\"Hello, my name<mask>\", add_special_tokens=False)).unsqueeze(0) \n",
    "perm_mask = torch.zeros((1, input_ids.shape[1], input_ids.shape[1]), dtype=torch.float)\n",
    "perm_mask[:, :, -1] = 1.0  # Previous tokens don't see last token\n",
    "target_mapping = torch.zeros((1, 1, input_ids.shape[1]), dtype=torch.float) # Shape [1, 1, seq_length] => let's predict one token\n",
    "target_mapping[0, 0, -1] = 1.0  # Our first (and only) prediction will be the last token of the sequence (the masked token)\n",
    "\n",
    "outputs = modell(input_ids, perm_mask=perm_mask, target_mapping=target_mapping)\n",
    "next_token_logits = outputs[0]  # Output has shape [target_mapping.size(0), target_mapping.size(1), config.vocab_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c7fca07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'like I welcomeâ€™ Not As? being two your because simply with but actually Qin very called be just too information<eop> you please of) about the so in were only  a or for alsos her not: to as and was. are is,'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizerr.decode(torch.argsort(next_token_logits[0, 0])[-50:].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e8c5ceb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'happy wonderful serious to great is you a, and  super. very'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizerr.decode(torch.argsort(next_token_logits[0, 0])[-14:].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b02cd405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-139.62346,\n",
       " -139.51682,\n",
       " -139.48384,\n",
       " -139.46986,\n",
       " -139.39355,\n",
       " -139.25244,\n",
       " -138.94429,\n",
       " -138.60022,\n",
       " -138.21074,\n",
       " -138.11418,\n",
       " -138.10332,\n",
       " -137.27269,\n",
       " -136.51256,\n",
       " -133.5401]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(torch.sort(next_token_logits[0, 0])[0][-14:].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2b46d3cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.encode(\"Hello, my dog is very <mask>\", add_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7f0ec047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32000])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sort(next_token_logits[0, 0])[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4b4f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736cff16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "32d708eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizer(name_or_path='xlnet-large-cased', vocab_size=32000, model_max_len=1000000000000000019884624838656, is_fast=False, padding_side='left', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '<sep>', 'pad_token': '<pad>', 'cls_token': '<cls>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['<eop>', '<eod>']})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d418c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
