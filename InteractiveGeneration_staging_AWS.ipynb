{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A fine-tuned language model for pictionary word list completion (topic/category phrase + example words -> list of 30 examples). For example, one may complete the list\n",
    "\n",
    "\"A list of round fruits: peach, apricot, lime, plum,\"\n",
    "\n",
    "with\n",
    "\n",
    "\"mango, cherry, pineapple, strawberry, pumpkin, watermelon, orange, pomegranate, melon, apple, pear, grapefruit, papaya, lemon, kiwi, passionfruit, blueberry, raspberry, blackberry, cantaloupe, nectarine, pitaya, persimmon, durian, guava, jackfruit, avocado, lychee, soursop, guarana, mangosteen, blackcurrant, cranberry\"\n",
    "\n",
    "using this model. These words should be compatible with pictionary/skribbl.io; i.e., \"sketchable\" within a few minutes, and easily recognisable. Scroll to the end for more examples, or see the file `data/examples.txt`. Sketchability parameter estimation examples can also be found in `data/data.tsv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import openai\n",
    "# with open('../../openai-api-org.txt', 'r') as f: openai.organization = f.read()\n",
    "# with open('../../openai-api-key.txt', 'r') as f: openai.api_key      = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import time\n",
    "import glob\n",
    "import string\n",
    "import jsonlines\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as pt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "import matplotlib.pyplot as plt\n",
    "from bayes_opt import BayesianOptimization\n",
    "from IPython.utils import io\n",
    "from IPython.display import clear_output\n",
    "from transformers import GPT2ForSequenceClassification, GPT2LMHeadModel, GPTNeoForCausalLM, ReformerModelWithLMHead, \\\n",
    "                         get_linear_schedule_with_warmup, GPT2TokenizerFast, AutoTokenizer, XLNetLMHeadModel\n",
    "from pytorch_transformers import GPT2Tokenizer\n",
    "from Learning import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                  ###   Options   ###\n",
    "model_name = \"ernst_one\"\n",
    "# model_name = \"unfinetuned\"\n",
    "modelkey = \"xlnet-large-cased\"                   # Pretrained model to start from\n",
    "# modelkey = \"gpt2-xl\"\n",
    "modelclass = \"XLNetLMHeadModel\"\n",
    "# modelclass = \"GPT2LMHeadModel\"\n",
    "val_frac, test_frac = 0.25, 0.25    # Fraction of samples to keep as separate validation/test set (word lists)\n",
    "TsN = 200                           # Number of randomly generated prompts for each sample when validating model\n",
    "log_period_batches = 10             # Batches per iteration\n",
    "# learning_rate = 5e-7              # Adam learning rate (default is 5e-5, sentiment classification example had 2e-5)\n",
    "learning_rate = 2e-5\n",
    "# learning_rate = 4e-6\n",
    "adam_epsilon = 1e-8                 # Adam epsilon (default is 1e-8)\n",
    "n_sched_warmup = 0                  # Linear scheduler for optimizer number of warmup steps\n",
    "batch_size = bsz = 64               # Samples per batch\n",
    "# batch_size = bsz = 32\n",
    "# batch_size = bsz = 4\n",
    "# N_train_batches = int(1e7 / bsz)  # Total number of batches to show model\n",
    "N_train_batches = 600\n",
    "n_unfreeze = \"all\"                  # Number of model layers to fine tune, in addition to the last output linear layer\n",
    "# max_len = 1024                    # Max n. tokens applied prior to rng_train (number of phrases range)\n",
    "max_len = 32\n",
    "train_phrase_log_pctile = 0.0       # Phrase generation probability percentile excluded from training dataset\n",
    "lidstone_eps = 0.01                 # Smoothing epsilon for possible words/subwords which are not in the missing list words set\n",
    "lastcomma_repl = ',' # 'EOS', ','   # Token optionally used to replace the final comma that ends the generated phrase\n",
    "use_correct_nouns = True            # Whether to use only correct singular or plural form of category nouns for the given prompt\n",
    "swap_noun = False                   # Whether to swap plural and singular nouns in prompt\n",
    "# rng_train = [0, 512]              # Range of prompt list lengths (number of phrases) to generate for training data\n",
    "rng_train = [3, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = \"cuda\" if pt.cuda.is_available() else \"cpu\"  # Setup torch device(s)\n",
    "d = device = pt.device(dev)\n",
    "# world_size = 1\n",
    "# rank = 0\n",
    "# def setup(rank, world_size): \n",
    "#     os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
    "#     os.environ['MASTER_PORT'] = find_free_port()\n",
    "#     dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)  # initialize the process group\n",
    "# def cleanup():\n",
    "#     dist.destroy_process_group()\n",
    "# # mp.spawn(setup, args=(rank, world_size), nprocs=world_size)\n",
    "# setup(rank, world_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cats, cats_sing, phrases = Listset().load()  # Import word lists dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([79, 649, 20, 866, 10164, 60, 18907, 19, 6090, 23, 19, 25571, 23, 19, 4, 3],\n",
       " [79, 649, 20, 866, 10164, 60, 18907, 19, 4, 3],\n",
       " [6090, 23, 19, 25571, 23, 19, 4, 3],\n",
       " [32, 1351, 286, 2835, 15921, 25, 22514, 11, 48389, 11, 279, 4127, 11],\n",
       " [32, 1351, 286, 2835, 15921, 25, 22514, 11],\n",
       " [273, 6231, 11, 279, 4127, 11])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt3_tokenizer = GPT2TokenizerFast.from_pretrained((\"gpt2-large\"))\n",
    "with io.capture_output() as captured:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(modelkey.replace(\"-xl\", \"-large\"))\n",
    "tokenizer.encode(\"A list of round fruits: apples, oranges, pears,\"), \\\n",
    "    tokenizer.encode(\"A list of round fruits: apples,\"), tokenizer.encode(\"oranges, pears,\"), \\\n",
    "  gpt3_tokenizer.encode(\"A list of round fruits: apples, oranges, pears,\"), \\\n",
    "    gpt3_tokenizer.encode(\"A list of round fruits: apples,\"), \\\n",
    "    gpt3_tokenizer.encode(\"oranges, pears,\") # Note: gpt-3 tokenizes words differently if they are at the start of the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DerivTokenizer(tokenizer.__class__):  # Wrap the tokenizer so that we don't produce the special tokens (NLG case)\n",
    "    def __init__(self, tokenizer): self.tokenizer = tokenizer\n",
    "    def __len__(self): return self.tokenizer.__len__()\n",
    "    def encode(self, *args, **kwargs): return self.tokenizer.encode(*args, **{**kwargs, **{\"add_special_tokens\": False}})\n",
    "    def decode(self, *args, **kwargs): return self.tokenizer.decode(*args, **kwargs)\n",
    "tokenizer, gpt3_tokenizer = DerivTokenizer(tokenizer), DerivTokenizer(gpt3_tokenizer)\n",
    "lprompts_sing = [p for p in lprompts if ((\"types of\" in p) ^ swap_noun)]\n",
    "enc_prompts = lambda tknzr, prmts: [pt.tensor(tknzr.encode(p)).to(d) for p in prmts]\n",
    "enc_listset = lambda tknzr, Xs: [[[pt.tensor(tknzr.encode(p + suffix)).to(d) for p in ps] for ps in X] for (X, suffix) in Xs]\n",
    "lprompts_encoded, lprompts_encoded3 = enc_prompts(tokenizer, lprompts), enc_prompts(gpt3_tokenizer, lprompts)\n",
    "lprompts_sing_encoded, lprompts_sing_encoded3 =enc_prompts(tokenizer, lprompts_sing),enc_prompts(gpt3_tokenizer, lprompts_sing)\n",
    "Xs = (cats, ': '), (cats_sing, ': '), (phrases, ', ')\n",
    "(cats_e,cats_sing_e,phrases_e), (cats_e3,cats_sing_e3,phrases_e3) = enc_listset(tokenizer, Xs), enc_listset(gpt3_tokenizer, Xs)\n",
    "comma_token = pt.tensor(tokenizer.encode(\"a,\")[1], device=d)\n",
    "N_tokens = len(tokenizer)\n",
    "N_wordlists = len(cats)\n",
    "lidstone_eps = pt.tensor(lidstone_eps, device=d) if not isinstance(lidstone_eps, pt.Tensor) else lidstone_eps\n",
    "lidstone_value = lidstone_eps / N_tokens\n",
    "y_zero = (lidstone_value).repeat(N_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_phrases = list(set(sum(phrases, [])))\n",
    "vowels = 'aeiuo'\n",
    "# phrase_logs = get_s_avglogs([('An' if p[0].lower() in vowels else 'A') + ' ' + p for p in all_phrases])\n",
    "phrase_logs = ()# todo: function to get log probability of phrase (n-gram) occuring in the pretraining dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "use_max_plog = train_phrase_log_pctile > 0.0\n",
    "if use_max_plog:\n",
    "    max_phrase_log = np.percentile(phrase_logs, 100 * (1.0 - train_phrase_log_pctile))\n",
    "    phrase_incls = [p > max_phrase_log for p in phrase_logs]\n",
    "    phrase_incl = dict(zip(all_phrases, phrase_incls))\n",
    "    all_phrases_enc = sum([[tuple(p_.cpu().detach().numpy().tolist()) for p_ in p] for p in phrases_e], [])\n",
    "    enc_phrase_log_incl = dict(zip(list(set(all_phrases_enc)), phrase_incls))\n",
    "    plt.hist(phrase_logs, bins=250)\n",
    "    plt.axvline(x=max_phrase_log, color='red')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a fixed test set and save to disk. This function defines the next list token prediction problem\n",
    "def gen_truncated_list(prmt, p, ra=True, rng=rng_train, mlen=max_len):  # prmt = prompt tokens, p = list phrase/word tokens\n",
    "    tkzs, sent, tkix, wordix = [], [], 0, -1   # rng is the inclusive range of list lengths to generate (number of phrases)\n",
    "    min_nw, max_nw, min_nt, max_nt = rng[0], int(rng[1]), 0, int(mlen) - len(prmt)\n",
    "    incl_words = np.random.choice(len(p), min(len(p), max_nw), replace=False)\n",
    "    if use_max_plog:\n",
    "        p_ra = [i for i in range(len(p)) if enc_phrase_log_incl[tuple(p[i].cpu().detach().numpy().tolist())]]\n",
    "        p_incl = [i for i in range(len(p)) if i not in p_ra]\n",
    "        p_ra_chosen = [i for i in incl_words if i in p_ra]\n",
    "        min_nw -= len(p_ra_chosen)\n",
    "        incl_words = [i for i in incl_words if i in p_incl]\n",
    "    if len(incl_words) == 0:#rare when train_phrase_log_pctile is low enough and min_nw is high enough (temporary optimisation)\n",
    "        return gen_truncated_list(prmt, p, ra=ra, rng=rng, mlen=mlen)\n",
    "    for phz_i in incl_words:\n",
    "        phz_enc, wordix = p[phz_i], wordix + 1\n",
    "        tkzs.append((tkix, phz_enc)), sent.append(phz_enc)\n",
    "        tkix += len(phz_enc)\n",
    "        if wordix < min_nw: min_nt = tkix\n",
    "        if tkix >= max_nt:\n",
    "            tkix = max_nt\n",
    "            break\n",
    "    if min_nt - 1 >= tkix:  # rare when max_len is large enough (0.5x max possible total list length) (temporary optimisation)\n",
    "        return gen_truncated_list(prmt, p, ra=ra, rng=rng, mlen=mlen)\n",
    "    sent = pt.hstack(sent)[:max_nt]\n",
    "    # If we don't want to include outliers in target\n",
    "    missing_w = [p[i] for i in range(len(p)) if (i not in incl_words) and (True if (ra or not use_max_plog) else (i in p_incl))]\n",
    "    trunc_ix = np.random.randint(max(min_nt - 1, 0), tkix)\n",
    "    trunc_n = min([(trunc_ix - ix) for (ix, enc) in tkzs if ix <= trunc_ix])  # N. end phrase tokens\n",
    "    missing_w += [enc for (ix, enc) in tkzs if ix >= (trunc_ix - trunc_n)]\n",
    "    missing_matches = missing_w\n",
    "    if trunc_n > 0:\n",
    "        phr_start = trunc_ix - trunc_n\n",
    "        partial_phr = sent[phr_start:trunc_ix]\n",
    "        missing_matches = [enc for enc in missing_w if len(enc) >= trunc_n and all(enc[:trunc_n] == partial_phr)]\n",
    "    next_tokens = [enc[trunc_n] for enc in missing_matches]\n",
    "    norm = len(next_tokens) * (1.0 + lidstone_eps)\n",
    "    tunit, y_ = pt.tensor(1 / norm, device=d), y_zero.clone()\n",
    "    for token in next_tokens: y_[token] += tunit\n",
    "    return pt.hstack([prmt, sent[:trunc_ix]]), y_\n",
    "def stac_sample(stac, n):   # Create batches by random permutations (maximise diversity and uniformity) (shuffling done later)\n",
    "    r, mode, total = [], tuple(np.unique(stac)), stac.shape[0]\n",
    "    while n > 0:\n",
    "        if mode == (0,) or mode == (1,) or mode == (-1,):\n",
    "            if n >= total:\n",
    "                new = sum([list(range(total)) for _ in range(n // total)], [])\n",
    "                r += new\n",
    "                n -= len(new)\n",
    "            else:\n",
    "                new = np.random.choice(total, n, replace=False)\n",
    "                stac[new] = (mode[0] + 1) if mode != (1,) else -1\n",
    "                r += new.tolist()\n",
    "                n = 0\n",
    "        else:\n",
    "            old_val, new_val = mode[0] if mode != (-1, 1) else 1, mode[1] if mode != (-1, 1) else -1\n",
    "            old_i = np.nonzero(stac == old_val)[0]\n",
    "            if n >= old_i.shape[0]:\n",
    "                stac[old_i] = new_val\n",
    "                r += old_i.tolist()\n",
    "                n -= old_i.shape[0]\n",
    "            else:\n",
    "                new = np.random.choice(old_i, n, replace=False)\n",
    "                stac[new] = new_val\n",
    "                r += new.tolist()\n",
    "                n = 0\n",
    "        mode = tuple(np.unique(stac))\n",
    "    return r\n",
    "def gen_listname(cp, cp_cs, prompt, prmt, tknzr=tokenizer):\n",
    "    cat_ix = np.random.randint(len(cp_cs))            # First uniformly sample a category title\n",
    "    sing = use_correct_nouns and (cat_ix >= len(cp))  # Singular vs plural prompt prefix\n",
    "    if prompt is None:                                # Uniformly sample a list beginning phrase (\"A list of...\") if not given\n",
    "        lprmpts = ((lprompts_sing_encoded if sing else lprompts_encoded) if tknzr is tokenizer else \\\n",
    "                   (lprompts_sing_encoded3 if sing else lprompts_encoded3)) if tknzr is not None else \\\n",
    "                   (lprompts_sing if sing else lprompts)\n",
    "        prmt = lprmpts[np.random.randint(len(lprmpts))]\n",
    "    return cp_cs[cat_ix], prmt\n",
    "def gen_listnames_uniform(xcp, xcs, xp, n, prompt=None, tknzr=tokenizer, verbose=False, stac=None):\n",
    "    prmts, cats, ps, j, prmt = [], [], [], 0, None\n",
    "    if prompt is not None and tknzr is not None: prmt = pt.tensor(tknzr.encode(prompt), device=d)\n",
    "    stac_, stac = stac_sample(stac, n) if (stac is not None) else None, stac is not None\n",
    "    if stac: np.random.shuffle(stac_)\n",
    "    for i in (range(len(xcp)) if not stac else stac_):\n",
    "        prmts_, cats_ = [], []\n",
    "        cp, cs, p = xcp[i], xcs[i], xp[i]\n",
    "        cp_cs = cp + cs\n",
    "        for m in range(n if not stac else 1):\n",
    "            cat, prmt = gen_listname(cp, cp_cs, prompt, prmt, tknzr=tknzr)\n",
    "            prmts_.append(prmt), cats_.append(cat)\n",
    "            j += 1\n",
    "            if verbose and j % 100 == 0: sys_print(\"\\rGenerating list names, done: \" + str(j))\n",
    "        ps.append(p), prmts.append(prmts_), cats.append(cats_)\n",
    "    if verbose: sys_print(\"\\rGenerating list names, done: \" + str(j) + \", finished!\\n\")\n",
    "    return prmts, cats, ps, stac\n",
    "def gen_samples_uniform(xcp, xcs, xp, n,              # Weight testing samples (word lists) exactly uniformly\n",
    "                        ra=True, rng=rng_train, prompt=None, tknzr=tokenizer, verbose=False, inds=False, stac=None, mlen=1e9):\n",
    "    xs, ys, sqlens, j, prmt = [], [], [], 0, None\n",
    "    prmts, cats, ps, stac = gen_listnames_uniform(xcp, xcs, xp, n, prompt=prompt, tknzr=tknzr, verbose=verbose, stac=stac)\n",
    "    for i in range(len(prmts)):\n",
    "        x, y, sqlen, p = [], [], [], ps[i]\n",
    "        for k in range(len(prmts[i])):\n",
    "            x_, y_ = gen_truncated_list(pt.hstack([prmts[i][k], cats[i][k]]), p, ra=ra, rng=rng, mlen=mlen)\n",
    "            x.append(x_), y.append(y_), sqlen.append(len(x_))\n",
    "            j += 1\n",
    "            if verbose and j % 100 == 0: sys_print(\"\\rGenerating list elements, done: \" + str(j))\n",
    "        xs.append(x), ys.append(y), sqlens.append(sqlen)\n",
    "    if inds or stac: xs, ys, sqlens = sum(xs, []), sum(ys, []), sum(sqlens, [])\n",
    "    if verbose: sys_print(\"\\rGenerating list elements, done: \" + str(j) + \", finished!\\n\")\n",
    "    return (xs, ys, sqlens, np.arange(len(xcp)).repeat(n)) if inds else (xs, ys, sqlens)\n",
    "def gen_samples(xcp, xcs, xp, n,\n",
    "                ra=True, rng=rng_train, prompt=None, tknzr=tokenizer, inds=False, mlen=1e9):\n",
    "    xs, ys, sqlens, j, prmt = [], [], [], 0, None   \n",
    "    if prompt is not None and tknzr is not None: prmt = pt.tensor(tknzr.encode(prompt), device=d)\n",
    "    xs, ys, sqlens, j = [], [], [], 0\n",
    "    n_sets, indices = len(xcp), []\n",
    "    for m in range(n):  # Maximise per-batch training diversity by randomly sampling the word lists\n",
    "        i = np.random.randint(n_sets)\n",
    "        cp, cs, p = xcp[i], xcs[i], xp[i]\n",
    "        cp_cs = cp + cs\n",
    "        cat_ix, prmt = gen_listname(cp, cp_cs, prompt, prmt)\n",
    "        x_, y_ = gen_truncated_list(pt.hstack([prmt, cp_cs[cat_ix]]), p, ra=ra, rng=rng, mlen=mlen)\n",
    "        xs.append(x_), ys.append(y_), sqlens.append(len(x_)), indices.append(i)\n",
    "    return (xs, ys, sqlens, np.asarray(indices)) if inds else (xs, ys, sqlens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2  8 21  4  7  5 10 25] [14 11  1 19 24 31 18 32]\n",
      "Train\n",
      " ['round fruits', 'microorganisms', 'outback experiences', 'buildings', 'holed pasta', 'rod shaped pasta', 'sounds of a building', 'biological examples of math in nature', 'non-biological examples of math in nature', 'handcrafts', 'communication media', 'storage media', 'scientific principles behind showers', 'scientific principles behind rain showers', 'spacecraft types', 'real spacecrafts', 'interpersonal tokens of trust'] \n",
      "Validation\n",
      " ['construction sounds', 'hats', 'wild animals', 'woodland ecoregions', 'winds', 'physical tokens that confer trust', 'timbers', 'digital tokens that confer trust'] \n",
      "Test\n",
      " ['chemical elements', 'dramatic and literature elements', 'vehicles referred to as crafts', 'music', 'scientific cycles', 'machine learning algorithms', 'glassware', 'windings']\n",
      "Generating list names, done: 1600, finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/ipykernel_launcher.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating list elements, done: 1600, finished!\n",
      "Generating list names, done: 1600, finished!\n",
      "Generating list elements, done: 1600, finished!\n"
     ]
    }
   ],
   "source": [
    "N_test, N_val = int(test_frac * N_wordlists), int(val_frac * N_wordlists)\n",
    "N_train = N_wordlists - N_test\n",
    "# test_idx = np.random.choice(N_train, N_test, replace=False)\n",
    "test_idx = np.asarray([ 2,  8, 21,  4,  7,  5, 10, 25])\n",
    "save_ld(test_idx, \"test_idx\")\n",
    "# test_idx = load_ld(\"test_idx\")\n",
    "trval_idx = np.asarray([i for i in range(N_wordlists) if i not in test_idx])\n",
    "# val_idx = np.random.choice(trval_idx, N_val, replace=False)\n",
    "val_idx = np.asarray([14, 11,  1, 19, 24, 31, 18, 32])\n",
    "save_ld(val_idx, \"val_idx\")\n",
    "# val_idx = load_ld(\"val_idx\")\n",
    "train_idx = np.asarray([i for i in trval_idx if i not in val_idx])\n",
    "print(test_idx, val_idx)\n",
    "print(*sum([[c+'\\n',[cats[i][0] for i in ix]]for(c,ix)in[[\"Train\",train_idx],[\"\\nValidation\",val_idx],[\"\\nTest\",test_idx]]],[]))\n",
    "index_listset, listset_iXs = lambda inds, Xs: [[X[i] for i in inds] for X in Xs],(\"trval_idx\",\"train_idx\",\"val_idx\",\"test_idx\")\n",
    "phase_listsets = {None: {k[:-4]: index_listset(globals()[k], (cats, cats_sing, phrases)) for k in listset_iXs},\n",
    "             \"default\": {k[:-4]: index_listset(globals()[k], (cats_e, cats_sing_e, phrases_e)) for k in listset_iXs},\n",
    "                \"gpt3\": {k[:-4]: index_listset(globals()[k], (cats_e3, cats_sing_e3, phrases_e3)) for k in listset_iXs}}\n",
    "cats_e_test, cats_sing_e_test, phrases_e_test = phase_listsets[\"default\"][\"test\"]\n",
    "cats_e_val, cats_sing_e_val, phrases_e_val = phase_listsets[\"default\"][\"val\"]\n",
    "val_cats = [cats[i][0] for i in val_idx]\n",
    "test_xs,test_ys,test_sqlens= gen_samples_uniform(cats_e_test, cats_sing_e_test, phrases_e_test, TsN, mlen=max_len, verbose=True)\n",
    "val_xs, val_ys, val_sqlens = gen_samples_uniform(cats_e_val, cats_sing_e_val, phrases_e_val, TsN, mlen=max_len, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write function that takes a prompt, existing list and sampling params and returns gpt3's next token probs\n",
    "default_msp = {\n",
    "  \"best_of\": 1,\n",
    "}\n",
    "default_sp = {\n",
    "  \"temperature\": 1.0,\n",
    "  \"top_p\": 1.0,\n",
    "#   \"top_k\": -1.0,                 # todo: add code to apply this to gpt3 output (top100), max k ~=90, min = 2?\n",
    "  \"presence_penalty\": 0.0,\n",
    "  \"frequency_penalty\": 0.0,\n",
    "}\n",
    "default_params = {\n",
    "  \"engine\": \"davinci\",\n",
    "  \"model\": None,\n",
    "  \"max_tokens\": 1,\n",
    "  \"temperature\": 1.0,\n",
    "  \"top_p\": 1.0,\n",
    "#   \"top_k\": -1.0,\n",
    "  \"presence_penalty\": 0.0,\n",
    "  \"frequency_penalty\": 0.0,\n",
    "  \"n\": 1,\n",
    "  \"stream\": False,\n",
    "  \"logprobs\": 100,\n",
    "#       \"logit_bias\": {\"50256\": -100},\n",
    "  \"stop\": [\",\", \"\\n\"],\n",
    "}\n",
    "# Define and test the OpenAI API next token probability request (response-token-efficient streaming version)\n",
    "def format_gpt3_probs(choice, tokenize):\n",
    "    res, r = [], sorted([(np.e**v, k) for (k, v) in choice[\"logprobs\"][\"top_logprobs\"][0].items()])[::-1]\n",
    "    for i in range(len(r)):\n",
    "        k = gpt3_tokenizer.encode(r[i][1])\n",
    "        if len(k) == 1: res.append((r[i][0], k if tokenize else r[i][1]))\n",
    "    return res\n",
    "def p_req(s, tokenize=False, **kwargs):\n",
    "    use_stream = \"max_tokens\" in kwargs and kwargs[\"max_tokens\"] != 1\n",
    "    kwargs[\"prompt\"], kwargs[\"stream\"] = s, use_stream\n",
    "    with io.capture_output() as captured:\n",
    "        response, result = openai.Completion.create(**{**default_params, **kwargs}), []\n",
    "    return [(np.e**resp[\"choices\"][0][\"logprobs\"][\"token_logprobs\"][0], resp[\"choices\"][0][\"logprobs\"][\"tokens\"][0],\n",
    "             format_gpt3_probs(resp[\"choices\"][0], tokenize)) for resp in (response if use_stream else [response])]\n",
    "# todo: version to handle multiple choices for phrase level evaluation (response-token-expensive)\n",
    "def p_req_m(s, tokenize=False, **kwargs):\n",
    "    if \"max_tokens\" not in kwargs: kwargs[\"max_tokens\"] = 8\n",
    "    if \"n\" not in kwargs: kwargs[\"n\"] = 5\n",
    "    if \"best_of\" in kwargs:\n",
    "        kwargs[\"best_of\"] = int(round(kwargs[\"best_of\"]))\n",
    "        if kwargs[\"n\"] != 1: kwargs[\"best_of\"], kwargs[\"n\"] = kwargs[\"n\"], kwargs[\"best_of\"]\n",
    "    kwargs[\"prompt\"] = s\n",
    "    with io.capture_output() as captured:\n",
    "        response, tokens, probs = openai.Completion.create(**{**default_params, **kwargs}), [], []\n",
    "    for choice in response[\"choices\"]:\n",
    "        tks = [np.e**v for v in choice[\"logprobs\"][\"token_logprobs\"]]\n",
    "        tks = [(choice[\"logprobs\"][\"tokens\"][i], tks[i]) for i in range(len(tks))]\n",
    "        tokens.append(tks), probs.append(format_gpt3_probs(choice, tokenize))\n",
    "    return tokens, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# a = p_req(\"A list of cyclic phenomena: day and night, induction coil, self-oscillation, tornado, runaway greenhouse effect,\")\n",
    "# b = p_req_m(\"A list of cyclic phenomena: day and night, induction coil, self-oscillation, tornado, runaway greenhouse effect,\")\n",
    "# print(sum([a_[0] for a_ in a[0][2]]))\n",
    "# print(','.join([''.join([b__[0] for b__ in b_]) for b_ in b[0]] + [''.join([a_[1] for a_ in a])]).replace('\\n', '⏎'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that takes a completion distribution (top 100) and target next token distribution (multinomial) and computes the\n",
    "# probability that the completion produces a desired output token\n",
    "def prob_corr(pred_p, target_p):\n",
    "    r = 0\n",
    "    if isinstance(pred_p, list):\n",
    "        for (p, token) in pred_p:\n",
    "            if target_p[token] > (lidstone_value + 1e-10): r += p\n",
    "    else:\n",
    "        r = np.sum(target_p[np.nonzero(pred_p > (lidstone_value + 1e-10))[0]])\n",
    "    return r\n",
    "# directly computes the similarity between target and predicted token distributions\n",
    "def score_corr(pred_p, target_p, distance=\"cross-entropy\", redistribute_mass=False, include_negatives=False):  \n",
    "    r = 0\n",
    "    if isinstance(pred_p, list) and not redistribute_mass:\n",
    "        for (p, token) in pred_p:\n",
    "            targ = target_p[token]\n",
    "            if targ > (lidstone_value + 1e-10) or include_negatives:\n",
    "                if   distance == \"unnormalized\":  r -= p * targ\n",
    "                elif distance == \"cross-entropy\": r -= p * np.log(targ)\n",
    "                elif distance == \"kl-divergence\": r += p * np.log(p / targ)\n",
    "                elif distance == \"bhattacharyya\": r += np.sqrt(p * targ)\n",
    "        if distance == \"bhattacharyya\": r = -np.log(r)\n",
    "    else:\n",
    "        p = pred_p\n",
    "        if isinstance(pred_p, list):\n",
    "            p_ = np.asarray([p for (p, _) in pred_p])\n",
    "            ts = np.asarray([t for (_, t) in pred_p])\n",
    "            unaccounted_mass = 1.0 - sum(p_)\n",
    "            n_missing_tokens = N_tokens - len(pred_p)\n",
    "            p = np.repeat(unaccounted_mass / n_missing_tokens, N_tokens)\n",
    "            p[ts] = p_\n",
    "        if not include_negatives:\n",
    "            pos = np.nonzero(target_p > (lidstone_value + 1e-10))[0]\n",
    "            p, target_p = p[pos], target_p[pos]\n",
    "        if   distance == \"unnormalized\":  r = -np.sum(p * targ)\n",
    "        elif distance == \"cross-entropy\": r = -np.sum(p * np.log(target_p))\n",
    "        elif distance == \"kl-divergence\": r =  np.sum(p * np.log(p / target_p))\n",
    "        elif distance == \"bhattacharyya\": r = -np.log(np.sum(np.sqrt(p * target_p)))\n",
    "    return -r\n",
    "# probability that a completion phrase is a desired missing list entry\n",
    "def prob_msp(outs, missing):\n",
    "    correct = 0\n",
    "    missing = set([phrase.lower() for phrase in missing])\n",
    "    for i in range(len(outs)):\n",
    "        out = outs[i].strip().lower()\n",
    "        if out not in missing and (out[:4] == 'the '): out = out[4:]\n",
    "        if out in missing: correct += 1\n",
    "    return correct / len(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   (   'filati cu lu pirtuso,',\n",
      "        array([   17, 10291,  4807,    17,  3525,    17,  2311,    17,  9261,\n",
      "        7077,   155,    19]),\n",
      "        12)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens = sum([[len(p) for p in p_ if len(p) < 1000] for p_ in phrases_e], [])  # Print longest list elements in dataset, get max\n",
    "phrs = sum([[p for p in p_ if len(p) < 1000] for p_ in phrases_e], [])\n",
    "m = np.max(lens)\n",
    "inds = [i for i in range(len(phrs)) if lens[i] == m]\n",
    "ree = [(tokenizer.decode(phrs[i].cpu().detach().numpy()), phrs[i].cpu().detach().numpy(), lens[i]) for i in inds]\n",
    "phrl_max = len(ree[0][1]) - 1\n",
    "pr(ree)\n",
    "phrl_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that takes sampling parameters, then generates n random incomplete list prompts (of length l), obtains completion\n",
    "# distributions (top 100 tokens or full multinomial) and evaluates the average score across the n prompts. n = 20 by default\n",
    "# All samples generated are stored fully for later training of sample-dependent sampling parameter (mixture) distribution\n",
    "sps_ = [\"top_p\", \"temperature\", \"presence_penalty\", \"frequency_penalty\"]  # sampling params                          #top_k\n",
    "msps_= sps_#[\"best_of\"] + sps_                                                 # meta sampling params\n",
    "create_folder(data_dir + learning_data_dir)\n",
    "create_folder(data_dir + learning_data_dir + \"sp_samples\")\n",
    "create_folder(data_dir + learning_data_dir + \"sp_samples_test\")\n",
    "create_folder(data_dir + learning_data_dir + \"msp_samples_nb\")\n",
    "create_folder(data_dir + learning_data_dir + \"msp_samples_nb_test\")\n",
    "phaseIx = lambda phase: globals()[phase + '_idx']\n",
    "def save_modeloutput(idx, dname, pnames, params, r, min_l, max_l, mdl, xs=None, ys=None, sqlens=None, inds=None, d=None):\n",
    "    engine_str = ','.join([str(params[k]) for k in [\"engine\", \"model\"] if k in params])\n",
    "    for i_raw in range(len(idx)):\n",
    "        i = idx[i_raw]\n",
    "        create_folder(data_dir + learning_data_dir + dname + \"/\" + str(i))\n",
    "        ix, mdl_name = np.nonzero(inds == i_raw)[0], mdl if isinstance(mdl, str) else mdl[\"name\"]\n",
    "        fn = str(time.time()) + '_' + '_'.join([str(v) for v in [params[k] for k in pnames] + [min_l, max_l]]) + '_' + mdl_name\n",
    "        input_data = [d[j] for j in ix] if d else [xs[ix], ys[ix], sqlens[ix]]\n",
    "        save_ld((params, input_data, ix, [r[j] for j in ix], str(mdl)), dname + \"/\" + str(i) + \"/\" + fn, compress=9)\n",
    "def eval_sp(params, min_l=0, max_l=1e9, n=20, prmt=None, phase=\"train\", uniform=True, mdl='gpt3'):\n",
    "    res, max_l = [], int(max_l), \n",
    "    tknzr = gpt3_tokenizer if mdl == \"gpt3\" else tokenizer\n",
    "    xcp, xcs, xp = phase_listsets[\"gpt3\" if mdl == \"gpt3\" else \"default\"][phase]\n",
    "    xs, ys, sqlens, inds = gen_samples_uniform(xcp, xcs, xp, n, prompt=prmt, tknzr=tknzr, inds=True, rng=[min_l, max_l]) \\\n",
    "           if uniform else gen_samples        (xcp, xcs, xp, n, prompt=prmt, tknzr=tknzr, inds=True, rng=[min_l, max_l])\n",
    "    if mdl == 'gpt3': r = [p_req(gpt3_tokenizer.decode(x_.detach().cpu().numpy()), **params) for x_ in xs] \n",
    "    else:             r = mdl[\"probabilities\"](xs, ys, sqlens, **params)\n",
    "    save_modeloutput(phaseIx[phase][np.unique(inds)], \"sp_samples\", sps_, params, r, min_l, max_l, mdl, xs, ys, sqlens, inds)\n",
    "    return np.mean([score_corr(r[i], ys[i]) for i in range(len(r))])\n",
    "def eval_sp_conv(params, tol=0.01, **kwargs):\n",
    "    center, samples = np.inf, []\n",
    "    while True:\n",
    "        samples.append(eval_sp(params, n=2, **kwargs))\n",
    "        new_center = np.mean(samples)\n",
    "        if abs(center - new_center) < tol: return new_center, samples\n",
    "        new_center = center\n",
    "# This metric differs depending on tokenisation, so for the testing of models, a full phrase accuracy function is required\n",
    "def gen_phraselevel_samples_uniform(phase, min_l, max_l, n, prmt, ra=False):\n",
    "    xcp, xcs, xp = phase_listsets[None][phase]\n",
    "    d, (prmts, cats, ps, _) = [], gen_listnames_uniform(xcp, xcs, xp, n, prompt=prmt, tknzr=None)\n",
    "    for i in range(len(xcp)):\n",
    "        x, y, sqlen, p = [], [], [], ps[i]\n",
    "        for m in range(n):\n",
    "            prompt, cat = prmts[i][m], cats[i][m]\n",
    "            phr_ix, phr_incl = [], []\n",
    "            while len(phr_ix) < 1:\n",
    "                phr_ix = np.random.choice(len(p), np.random.randint(min_l, min(max_l, len(p) - 1)), replace=False)\n",
    "                if use_max_plog:\n",
    "                    phr_ra = [j for j in range(len(p)) if phrase_log_incl[p[j]]]\n",
    "                    phr_incl = [j for j in range(len(p)) if j not in phr_ra]\n",
    "                    phr_ix = [i for i in phr_ix if i in phr_incl]\n",
    "            missing_ix= [k for k in range(len(p)) if (k not in phr_ix)and(True if (ra or not use_max_plog) else k in phr_incl)]\n",
    "            prompt = (prompt + ' ' + cat + ': ' + ''.join([p[j] + ', ' for j in phr_incl]))[:-1]\n",
    "            d.append([prompt, [p[j] for j in missing_ix]])\n",
    "    return d, np.arange(len(xcp)).repeat(n)\n",
    "strip_the = lambda x: x[4:] if x.lower()[:4] == 'the ' else x\n",
    "strip_tl = lambda x: strip_the(x.strip().lower())\n",
    "strip_comma = lambda x: [x_.strip() for x_ in (x[:-1] if len(x) > 1 else x)]\n",
    "strip_lower = lambda x: [strip_tl(x_) for x_ in (x[:-1] if len(x) > 1 else x)]\n",
    "def ensemble_two_models_results(m2ensemble_frac, r, r2):\n",
    "    bof = len(r[0])\n",
    "    n_replace = int(bof * m2ensemble_frac)\n",
    "    r_new = [[strip_comma(''.join([r___[0] for r___ in r__]).split(',')) for r__ in r_] for r_ in r]\n",
    "    for i in range(len(r)):\n",
    "        cur_pool = set(sum([strip_lower(''.join([r__[0] for r__ in r_]).split(',')) for r_ in r[i][:bof - n_replace]], []))\n",
    "        new_pool = sum([strip_comma(''.join([r__[0] for r__ in r_]).split(',')) for r_ in r2[i]], [])\n",
    "        for j in range(n_replace):\n",
    "            n_phrases = len(r[i][j])\n",
    "            add = []\n",
    "            while len(add) < n_phrases and len(new_pool) > 0:\n",
    "                new_phr = new_pool[0]\n",
    "                stripped_new_phr = strip_tl(new_phr)\n",
    "                if stripped_new_phr not in cur_pool:\n",
    "                    add.append(new_phr), cur_pool.add(stripped_new_phr)\n",
    "                new_pool = new_pool[1:]\n",
    "            if len(add) > 0:\n",
    "                r_new[i][j + bof - n_replace] = add\n",
    "    return [sum(r_, []) for r_ in r_new]\n",
    "def test_sp(params, min_l=0, max_l=1e9, n1=3, n2=10, prmt=None, phase=\"train\", uniform=True, max_tokens=phrl_max,\n",
    "            mdl='gpt3', mdl2=None, d=None, d_test=None, inds=None, inds_test=None, m2ensemble_frac=0.4, return_test_acc=True):\n",
    "    max_l, test_acc, dn = int(max_l), None, \"msp_samples_nb\"\n",
    "    if d is None: d, inds = gen_phraselevel_samples_uniform(phase, min_l, max_l, n1, prmt)\n",
    "    params = {**default_sp, **params, **{'n': n2, 'max_tokens': max_tokens}} #**default_msp,\n",
    "    if mdl == 'gpt3': r = [p_req_m(d_[0], **params)[0] for d_ in d]  # Request predictions from OpenAI\n",
    "    else:             r = mdl[\"completions\"]([d_[0] for d_ in d], **params)\n",
    "    save_modeloutput(phaseIx(phase), dn, msps_, params, r, min_l, max_l, mdl, d=d, inds=inds)\n",
    "    if mdl2 is not None:\n",
    "        if mdl2 == 'gpt3': r2 = [p_req_m(d_[0], **params)[0] for d_ in d]# Ensemble with 2nd model according to m2ensemble_frac\n",
    "        else:              r2 = mdl2[\"completions\"]([d_[0] for d_ in d], **params)# This loads precomputed outputs so no resave\n",
    "        r = ensemble_two_models_results(m2ensemble_frac, r, r2)\n",
    "    else:\n",
    "        r = [sum([strip_comma(''.join([r___[0] for r___ in r__]).split(',')) for r__ in r_], []) for r_ in r]\n",
    "    #     r = [sum([strip_comma(''.join([r___[0] for r___ in r__]).split(','))[:max_l - min_l] for r__ in r_], []) for r_ in r]\n",
    "    acc = np.mean([prob_msp(r[i], d[i][1]) for i in range(len(r))])\n",
    "\n",
    "    if phase != \"test\":  # if not testing finalised parameters, also output the test set accuracy\n",
    "        listset_fracs = {\"train\": 1 - (val_frac + test_frac), \"trval\": 1 - test_frac, \"val\": val_frac, \"test\": test_frac}\n",
    "        n1_ = n1 * int(listset_fracs[phase] / test_frac)  # Use approximately enough samples to converge\n",
    "        if d_test is None: d_test, inds_test = gen_phraselevel_samples_uniform(\"test\", min_l, max_l, n1_, prmt)\n",
    "        if mdl == 'gpt3': r_test = [p_req_m(d_[0], **params)[0] for d_ in d_test]\n",
    "        else:             r_test = mdl[\"completions\"]([d_[0] for d_ in d_test], **params)\n",
    "        save_modeloutput(phaseIx(\"test\"), dn+\"_test\", msps_, params, r_test, min_l, max_l, mdl, d=d_test, inds=inds_test)\n",
    "        if mdl2 is not None:\n",
    "            if mdl2 == 'gpt3': r2_test = [p_req_m(d_[0], **params)[0] for d_ in d_test]\n",
    "            else:              r2_test = mdl2[\"completions\"]([d_[0] for d_ in d_test], **params)\n",
    "            r_test = ensemble_two_models_results(m2ensemble_frac, r_test, r2_test)\n",
    "        else:\n",
    "            r_test = [sum([strip_comma(''.join([r___[0] for r___ in r__]).split(',')) for r__ in r_], []) for r_ in r_test]\n",
    "#r_test =[sum([strip_comma(''.join([r___[0] for r___ in r__]).split(','))[:max_l - min_l] for r__ in r_], []) for r_ in r_test]\n",
    "        samps = np.asarray([prob_msp(r_test[i], d_test[i][1]) for i in range(len(r_test))])\n",
    "        test_sd = np.std(100*samps)\n",
    "        test_acc = np.mean(samps)\n",
    "        if not return_test_acc:\n",
    "            print(\"Test acc:\", 100*test_acc, \"sd:\", test_sd)\n",
    "\n",
    "    return (acc, test_acc) if return_test_acc else acc\n",
    "def test_sp_conv(params, tol=0.01, **kwargs):\n",
    "    center, ys, i, steps_nochange = np.inf, [], 1, 0\n",
    "    while True:\n",
    "        ys.append(test_sp(params, n1=1, **kwargs))\n",
    "        new_center = np.mean([s[0] for s in ys])\n",
    "        if abs(center - new_center) < tol: steps_nochange += 1\n",
    "        else:                              steps_nochange = 0\n",
    "        if steps_nochange >= 3 and i >= 5:\n",
    "            if ys[0][1] is not None: print([s[1] for s in ys])\n",
    "            print([s[0] for s in ys])\n",
    "            j = 1 if ys[0][1] is not None else 0\n",
    "            sys_print(str((\"Test acc:\", 100*np.mean([s[j] for s in ys]), \"sd:\", np.std([100*s[j] for s in ys])))+\"\\n\", False)\n",
    "            return new_center, ys\n",
    "        center = new_center\n",
    "        i += 1\n",
    "# eval_sp(default_ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "Model device: cuda:0\n",
      "Pretrained parameters loaded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "835"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if 'model' in locals():  # Load pretrained weights\n",
    "    del model\n",
    "print(gc.collect()), pt.cuda.empty_cache()\n",
    "create_folder(\"models\")\n",
    "create_folder(\"models/pretrained\")\n",
    "create_folder(\"models/pretrained/\" + modelkey)\n",
    "model_ = locals()[modelclass].from_pretrained(modelkey, output_hidden_states=False, output_attentions=False, \n",
    "    cache_dir=\"models/pretrained/\" + modelkey)\n",
    "if pt.cuda.device_count() > 1 and \"gpt\" in modelkey:\n",
    "    device_map = {0: [0, 1, 2],\n",
    "                  1: [3, 4, 5, 6, 7, 8],\n",
    "                  2: [9, 10, 11, 12, 13, 14],\n",
    "                  3: [15, 16, 17, 18, 19, 20],\n",
    "                  4: [21, 22, 23, 24, 25, 26, 27],\n",
    "                  5: [28, 29, 30, 31, 32, 33, 34],\n",
    "                  6: [35, 36, 37, 38, 39, 40, 41],\n",
    "                  7: [42, 43, 44, 45, 46, 47],\n",
    "                 }\n",
    "    model_.parallelize(device_map)\n",
    "else:\n",
    "    model_ = model_.to(d)\n",
    "print(\"Model device:\", model_.device)\n",
    "if \"gpt2\" in modelkey:\n",
    "    model_.resize_token_embeddings(N_tokens)\n",
    "    pad_token = model_.config.pad_token_id = model_.config.eos_token_id\n",
    "    pad_token = pt.tensor(pad_token, device=d)\n",
    "    repl_token = pt.tensor(tokenizer.encode(lastcomma_repl)[0], device=d) if lastcomma_repl != 'EOS' else pad_token\n",
    "    n_embd = pt.tensor(model_.config.n_embd, device=d)\n",
    "else:\n",
    "    pad_token = model_.config.pad_token_id\n",
    "    pad_token = pt.tensor(pad_token, device=d)\n",
    "    mask_token = pt.tensor(tokenizer.encode(\"<mask>\")).to(d)\n",
    "# model = nn.parallel.DistributedDataParallel(model_, device_ids=[d])\n",
    "# model = nn.DataParallel model_, device_ids=list(range(pt.cuda.device_count()))) if dev != \"cpu\" else model_\n",
    "model = model_\n",
    "mname_fn = modelkey\n",
    "multimask_model = \"xlnet\" in modelkey\n",
    "print(\"Pretrained parameters loaded\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine tuning the last 23 of 23 model layers plus the output linear layer\n"
     ]
    }
   ],
   "source": [
    "max_layer = max([int(name.split('transformer.layer.')[1].split('.')[0]) for (name, _) in model.named_parameters() \\\n",
    "                 if \"transformer.layer.\" in name])\n",
    "if n_unfreeze == \"all\": n_unfreeze = max_layer\n",
    "print(\"Fine tuning the last \" + str(n_unfreeze) + \" of \" + str(max_layer) + \" model layers plus the output linear layer\")\n",
    "for (name, v) in model.named_parameters():\n",
    "    if name == \"transformer.mask_emb\":              continue      # torch.Size([1, 1, 768])\n",
    "    if name == \"transformer.word_embedding.weight\": continue      # torch.Size([32000, 768])\n",
    "    if name == \"lm_loss.bias\":                      continue      # torch.Size([32000])\n",
    "    else:\n",
    "        layer_n = int(name.split('transformer.layer.')[1].split('.')[0])\n",
    "        if (max_layer - layer_n) < n_unfreeze: v.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define next batch function\n",
    "curr_ri = np.zeros(len(train_idx), dtype=int)\n",
    "def next_batch(sz):\n",
    "    global phase_listsets, curr_ri\n",
    "    cats_, cats_sing_, phrases_ = phase_listsets[\"default\"][\"train\"]\n",
    "    return adapt_form(*gen_samples_uniform(cats_, cats_sing_, phrases_, sz, ra=False, stac=curr_ri, mlen=max_len))\n",
    "#     return adapt_form(*gen_samples(cats_, cats_sing_, phrases_, sz, ra=False, mlen=max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also create a gpt3 prompt-completion-based regression model to predict values/densities of sampling parameters (might work if\n",
    "# it's possible to find a humanlike transliteration of the problem statement that gpt3 can bootstrap on to find the params, or\n",
    "# to find some (possibly entirely textual) representation of a params-correlating multidimensional metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entirely textual representation attempts:\n",
    "# for example: Some round fruits: apple, orange, pear. As a percentage of all round fruits, this list represents <blank>\n",
    "# with: Some round fruits: apple, orange, pear. Does this list use a relatively strict definition of round fruits? <blank>\n",
    "# and Some round fruits: apple, orange, pear. To what degree is this a representative list? <blank>  davinci-instruct priming\n",
    "# etc... in some sense it reformulates it as a multi sentiment classification that's able to predict optimal PLM sampling params\n",
    "# From early experiments this may be possible however the noise is difficult to deal without some heavy improvements on priming\n",
    "# Alternatively, we could use the top-p numerical token instead of <blank> and find a priming prompt that outputs these best\n",
    "# This way we can \"fine-tune\" GPT3-davinci (which is much better than curie but has no fine tuning API) by \"priming recursion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "llayer = None #nn.Linear(n_embd, N_tokens, bias=False).to(d)#.cpu()                  # Model definition\n",
    "# nn.init.xavier_uniform_(llayer.weight)\n",
    "# llayer = nn.DataParallel(llayer, device_ids=list(range(pt.cuda.device_count()))) if dev != \"cpu\" else llayer\n",
    "# llayer = nn.parallel.DistributedDataParallel(llayer, device_ids=list(range(pt.cuda.device_count()))).to(d)\n",
    "# softmax = nn.Softmax()\n",
    "bcewl_loss = nn.BCEWithLogitsLoss()#.to(d)#.cpu()\n",
    "# bcewl_loss = nn.DataParallel(bcewl_loss, device_ids=list(range(pt.cuda.device_count()))).to(d) if dev != \"cpu\" else bcewl_loss\n",
    "# bcewl_loss = nn.parallel.DistributedDataParallel(bcewl_loss, device_ids=list(range(pt.cuda.device_count()))).to(d)\n",
    "# nll_loss = nn.NLLLoss()\n",
    "# kl_loss = nn.KLDivLoss()\n",
    "optimizer = pt.optim.AdamW(model.parameters(), lr=learning_rate, eps=adam_epsilon)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=n_sched_warmup, num_training_steps=N_train_batches)\n",
    "def sequence_mask(lengths, maxlen=None, dtype=pt.int):\n",
    "    if maxlen is None:\n",
    "        maxlen = lengths.max()\n",
    "    row_vector = pt.arange(0, maxlen, 1, device=d)\n",
    "    matrix = pt.unsqueeze(lengths, dim=-1)\n",
    "    mask = row_vector < matrix\n",
    "\n",
    "    mask = mask.type(dtype)\n",
    "    return mask\n",
    "def multimask_model(model, x=None, sqlens=None, past=None, seq_maxlen=None, add=None, many_inp=None, **kwargs):\n",
    "    x = pt.cat([x, pad_token.repeat((x.shape[0], 1))], axis=1)\n",
    "    if add == 0: \n",
    "        x[pt.arange(x.shape[0]).to(d), sqlens] = mask_token\n",
    "    else:\n",
    "        x[:, -1] = mask_token\n",
    "    perm_mask = pt.zeros((x.shape[0], seq_maxlen + add + 1, seq_maxlen + add + 1), dtype=pt.float).to(d)\n",
    "    for i in range(sqlens.shape[0]):\n",
    "        l = sqlens[i] + add\n",
    "        perm_mask[i, :, -(x.shape[1] - l):seq_maxlen + 2] = 1.0  # Previous tokens don't see last token or padding tokens\n",
    "        if add > 0:\n",
    "            perm_mask[i, :, seq_maxlen + 1] = 0.0\n",
    "            perm_mask[i, :, -1] = 1.0\n",
    "    target_mapping = pt.zeros((x.shape[0], 1, x.shape[1]), dtype=pt.float).to(d)\n",
    "    if add == 0:\n",
    "        target_mapping[pt.arange(x.shape[0]).to(d), 0, sqlens] = 1.0\n",
    "    else:\n",
    "        target_mapping[:, 0, -1] = 1.0\n",
    "    return model(x, perm_mask=perm_mask, target_mapping=target_mapping, mems=past)\n",
    "# next_token_logits = outputs[0]  # Output has shape [target_mapping.size(0), target_mapping.size(1), config.vocab_size]\n",
    "def singlemask_model(model, x=None, sqlens=None, past=None, return_states=None, seq_maxlen=None, add=None, many_inp=None, **kw):\n",
    "    mask = sequence_mask(sqlens, seq_maxlen) if (many_inp or add != 0) else None\n",
    "    if add > 0: mask = pt.cat([mask, pt.ones(mask.shape[0], add).to(d)], dim=1)  # Append mask entry for new stream token\n",
    "    return model(x.long(), attention_mask=mask, use_cache=None if not return_states else True, past_key_values=past)\n",
    "def inference(x, sqlens, past=None, return_states=False, seq_maxlen=max_len, add=0, return_fulloutput=False):\n",
    "    global model\n",
    "    many_inp = x.shape[1] > 1\n",
    "    outputs = multimask_model(model, **locals()) if multimask_model else singlemask_model(model, **locals())\n",
    "    if return_fulloutput: return outputs\n",
    "    logits = outputs[0].squeeze(1) if multimask_model else \\\n",
    "            (outputs[0][[pt.arange(x.shape[0]), sqlens - 1]] if many_inp else outputs[0].squeeze(1))\n",
    "\n",
    "    return (logits, outputs[1]) if return_states else logits  # Optionally return the past states needed to restore the stream\n",
    "def adapt_form(xs, ys, sqlens, mlen=max_len, repl_finalcomma=True):\n",
    "    xs = pt.vstack([F.pad(x, (0, max(0, mlen - len(x))), mode='constant', value=pad_token)[:mlen] for x in xs])\n",
    "    _ys = pt.vstack(ys) if ys is not None else None\n",
    "    if repl_finalcomma and (lastcomma_repl != ',') and ys is not None:\n",
    "        _ys[:, repl_token] += _ys[:, comma_token] - lidstone_value\n",
    "        _ys[:, comma_token] = lidstone_value\n",
    "    return xs, _ys, pt.tensor(sqlens, device=d)\n",
    "def train_step():\n",
    "    global model, llayer, bcewl_loss, optimizer, bsz, scheduler\n",
    "    x_batch, y_batch, sqlens_batch = next_batch(bsz)\n",
    "\n",
    "    model.zero_grad()\n",
    "    mask = sequence_mask(sqlens_batch, max_len)\n",
    "    outputs = model(x_batch.long(), attention_mask=mask)  # Get logits\n",
    "    logits = outputs[0][[pt.arange(x_batch.shape[0]), sqlens_batch - 1]]\n",
    "\n",
    "#     logsofts = pt.log(softmax(logits))\n",
    "    loss = bcewl_loss(logits, y_batch.float())\n",
    "    loss = loss.mean()\n",
    "    correct = pt.mean((y_batch[pt.arange(batch_size), pt.argmax(logits, axis=1)] > (lidstone_value + 1e-10)).float())\n",
    "    loss_, correct_ = loss.detach().cpu().numpy(), correct.detach().cpu().numpy()\n",
    "    \n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    return loss_, correct_\n",
    "def eval_test(x, y, sqlens):\n",
    "    global bcewl_loss\n",
    "\n",
    "    with pt.no_grad():\n",
    "        logits = inference(x, sqlens)\n",
    "        loss = bcewl_loss(logits, y.float())\n",
    "        loss = loss.mean()\n",
    "        correct = pt.mean((y[pt.arange(x.shape[0]), pt.argmax(logits, axis=1)] > (lidstone_value + 1e-10)).float())\n",
    "        loss_, correct_ = loss.detach().cpu().numpy(), correct.detach().cpu().numpy()\n",
    "    return loss_, correct_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_i = -1  # Batch 0 is the first iteration, where validation occurs without any training\n",
    "best_acc, best_loss = 0, np.inf\n",
    "best_acc_idx = -1\n",
    "out_str = ''\n",
    "create_folder(\"models\")\n",
    "create_folder(\"model_logs\")\n",
    "create_folder(\"models/\" + model_name)\n",
    "graphs_folder = \"graphs\"\n",
    "create_folder(graphs_folder)\n",
    "train_loss, train_accuracy, val_loss, val_accuracy = [], [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_training(verbose=True):  # Training loop function\n",
    "    global model, batch_i, best_acc, best_loss, best_acc_idx, train_loss, train_accuracy, val_loss, val_accuracy\n",
    "    \n",
    "    model.train()\n",
    "    iter_loss, iter_accuracy, b_no_inp = [], [], 0\n",
    "    while batch_i < N_train_batches:\n",
    "        batch_i += 1\n",
    "        if batch_i > 0:\n",
    "            gc.collect()\n",
    "            if dev != \"cpu\": pt.cuda.empty_cache()\n",
    "            b_loss, b_accuracy = train_step()\n",
    "            mname_fn = model_name\n",
    "            if verbose:\n",
    "                sys_print('\\rLoss, accuracy: ' + str(np.mean(b_loss)) + ', ' + str(np.mean(b_accuracy)) + \\\n",
    "                          ' @ batch '+ str(batch_i) + ' (' + str(batch_i * batch_size) + ' samples) complete.                  ')\n",
    "            iter_loss.append(b_loss), iter_accuracy.append(b_accuracy)\n",
    "\n",
    "        if batch_i % log_period_batches == 0:  # Test on val set\n",
    "            model.eval()\n",
    "            loss, accuracy = [], []\n",
    "            out_str = '\\n'\n",
    "            for i in range(N_val):\n",
    "                val_X, val_Y, val_Sqlens = adapt_form(val_xs[i], val_ys[i], val_sqlens[i], repl_finalcomma=batch_i > 0)\n",
    "                feed_batches = [range(len(val_X))[i * bsz:(i + 1) * bsz] for i in range((len(val_X) // bsz) + \\\n",
    "                                                                                        (1 if (len(val_X) % bsz) != 0 else 0))]\n",
    "                if dev != \"cpu\": pt.cuda.empty_cache()\n",
    "                ls, cs = zip(*[eval_test(val_X[inds], val_Y[inds], val_Sqlens[inds]) for inds in feed_batches])\n",
    "                loss.append(np.mean(ls)), accuracy.append(np.mean(cs))\n",
    "                out_str += val_cats[i] + ': ' + str(loss[-1]) + ', ' + str(accuracy[-1]) + '\\n'\n",
    "            \n",
    "            val_l, val_a = np.mean(loss), np.mean(accuracy)\n",
    "            val_loss.append(val_l), val_accuracy.append(val_a)\n",
    "            if batch_i == 0: iter_loss, iter_accuracy = [val_l], [val_a]\n",
    "            train_l, train_a = np.mean(iter_loss), np.mean(iter_accuracy)\n",
    "            train_loss.append(train_l), train_accuracy.append(train_a)\n",
    "            iter_loss, iter_accuracy = [], []\n",
    "\n",
    "#             if ((val_a > best_acc and val_a >= train_a) or \\\n",
    "            if ((val_a > best_acc) or \\\n",
    "              (batch_i // log_period_batches) == 1) and batch_i > 0:      # Save best accuracy model\n",
    "                best_acc = val_a\n",
    "                best_loss = val_l\n",
    "                best_acc_idx = batch_i // log_period_batches\n",
    "                pt.save({\"model\": model.state_dict(),\n",
    "#                          \"llayer\": llayer.state_dict(),\n",
    "#                          \"softmax\": softrmax.state_dict(),\n",
    "                         \"bcewl_loss\": bcewl_loss.state_dict(),\n",
    "#                          \"nll_loss\": nll_loss.state_dict(),\n",
    "#                          \"kl_loss\": kl_loss.state_dict(),\n",
    "                         \"optimizer\": optimizer.state_dict(),\n",
    "                         \"scheduler\": scheduler.state_dict(),\n",
    "                         }, \"./models/\" + model_name + '/' + model_name)\n",
    "                b_no_inp = 0\n",
    "            else:\n",
    "                b_no_inp += log_period_batches\n",
    "                \n",
    "            if verbose:\n",
    "                clear_output()\n",
    "                print(out_str + \"Batch\", batch_i, ':', train_a, val_a, \"loss:\", train_l, val_l, \\\n",
    "                      \"Best:\", best_acc, best_loss, 'idx:', best_acc_idx)\n",
    "                fig = plt.figure()\n",
    "                fig.set_size_inches(16, 5)\n",
    "                g = fig.add_subplot(1,2,1)\n",
    "                g.grid()\n",
    "                g.plot(train_accuracy, label='train acc')\n",
    "                g.plot(val_accuracy, label='val acc')\n",
    "                g.legend(loc='lower right')\n",
    "#                 g.axhline(y=0.714, ls='--', color='grey')\n",
    "\n",
    "                g = fig.add_subplot(1,2,2)\n",
    "                g.grid()\n",
    "                g.plot(train_loss, label='train loss')\n",
    "                plt.yscale(\"log\")\n",
    "                g.plot(val_loss, label='val loss')\n",
    "                plt.yscale(\"log\")\n",
    "                g.legend(loc='upper right')\n",
    "\n",
    "                save_ld((train_accuracy, val_accuracy, train_loss, val_loss),\n",
    "                        \"model_logs/\" + model_name + '_log_latest', pad=False)\n",
    "                plt.savefig(graphs_folder + '/' + model_name + \"_curve_latest\" + '.pdf', format='pdf')\n",
    "                plt.show()\n",
    "\n",
    "            model.train()\n",
    "    return best_acc, best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "construction sounds: 2.5593734, 0.74609375\n",
      "hats: 8.377678, 0.61328125\n",
      "wild animals: 4.460703, 0.7109375\n",
      "woodland ecoregions: 2.544066, 0.69140625\n",
      "winds: 1.5158024, 0.5625\n",
      "physical tokens that confer trust: 5.6130595, 0.52734375\n",
      "timbers: 3.8316135, 0.6015625\n",
      "digital tokens that confer trust: 4.213208, 0.6484375\n",
      "Batch 220 : 0.140625 0.6376953 loss: 0.22788194 4.1394377 Best: 0.65185547 4.4667377 idx: 14\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/ipykernel_launcher.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss, accuracy: 0.0050032036, 0.15625 @ batch 230 (14720 samples) complete.                  "
     ]
    }
   ],
   "source": [
    "print(gc.collect())  # Train ernst model (enumeration representation neural subtext transfer)\n",
    "iterate_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(gc.collect())\n",
    "iterate_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pt.cuda.empty_cache(), gc.collect()  # Load best model (checkpoint)\n",
    "cpoint = pt.load(\"./models/\" + model_name + '/' + model_name)\n",
    "model.load_state_dict(cpoint['model'])\n",
    "bcewl_loss.load_state_dict(cpoint['bcewl_loss'])\n",
    "optimizer.load_state_dict(cpoint['optimizer'])\n",
    "scheduler.load_state_dict(cpoint['scheduler'])\n",
    "# llayer.load_state_dict(cpoint['llayer'])\n",
    "mname_fn = model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_filtering(logits, tcounts=None, filter_value=-float('Inf'),  # Function to tune the output token distribution\n",
    "                  top_k=0, top_p=0.0, temperature=1.0, frequency_penalty=0.0, presence_penalty=0.0):\n",
    "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
    "        Args:\n",
    "            logits: logits distribution shape (vocabulary size)\n",
    "            top_k >0: keep only top k tokens with highest probability (top-k filtering).\n",
    "            top_p >0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
    "                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
    "    \"\"\"\n",
    "    assert logits.dim() == 1  # batch size 1 for now - could be updated for more but the code would be less clear\n",
    "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
    "    if tcounts is not None: logits -= (tcounts * frequency_penalty) + ((tcounts > 0) * presence_penalty)\n",
    "    logits /= temperature\n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the last token of the top-k\n",
    "        indices_to_remove = logits < pt.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p > 0.0:\n",
    "        sorted_logits, sorted_indices = pt.sort(logits, descending=True)\n",
    "        cumulative_probs = pt.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "        \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gprobs(s, past=None, return_sts=False, tcounts=None, add=0, **kwargs):  # Inference and sampling for tokens\n",
    "    global model\n",
    "    xs, mlen = None, None\n",
    "    if isinstance(s, tuple):    # s either list of token tensors or tuple of preformatted 2d tensors\n",
    "        xs, _, sqlen = s\n",
    "        mlen = max(sqlen)\n",
    "    else:\n",
    "        sqlen = [len(s_) for s_ in s]\n",
    "        mlen = max(sqlen)\n",
    "        xs, _, sqlen = adapt_form([pt.tensor(s_).to(d) for s_ in s], None, sqlen, mlen=mlen)\n",
    "    model.eval()\n",
    "    y_hat = inference(xs, sqlen, seq_maxlen=mlen, add=add, past=past, return_states=return_sts)\n",
    "    if return_sts: y_hat, states = y_hat\n",
    "    y_hat = pt.vstack([F.softmax(top_filtering(y_hat[i], tcounts[i] if tcounts is not None else None,\n",
    "                                               **kwargs), dim=0) for i in range(len(xs))])\n",
    "    return (y_hat, states) if return_sts else y_hat\n",
    "def append_next_token(sent, olen=None, top_k=-1, top_p=0.9, temperature=1.0):  # Interface for field testing\n",
    "    print(\"k =\", top_k, \", p =\", top_p, \", temp =\", temperature)\n",
    "    tokens = tokenizer.encode(sent)\n",
    "    ou = tokens.copy()\n",
    "    tcounts = pt.zeros(N_tokens, dtype=int).to(d)\n",
    "    for token in tokens: tcounts[token] += 1\n",
    "    probs = gprobs([tokens], top_k=top_k, top_p=top_p, temperature=temperature, tcounts=[tcounts])[0]\n",
    "    token = pt.multinomial(probs, 1).detach().cpu().numpy()[0]\n",
    "    ou += [token]\n",
    "    prev_len = len(sent) if olen is None else olen\n",
    "    sent_new = tokenizer.decode(ou)\n",
    "    print(sent[:prev_len] + '➡' + sent_new[prev_len:])\n",
    "    return sent_new\n",
    "def gen_probs(s, **kwargs):  # Adapter for strings\n",
    "    inp = [tokenizer.encode(s_) for s_ in s]\n",
    "    return gprobs(inp, **kwargs)\n",
    "def gen_completions(s, n=1, max_tokens=phrl_max, best_of=1, **kwargs):  # Completion generator equivalent to OpenAI's for GPT3\n",
    "    gpu_multiplier = 0.25\n",
    "    n_bats, best_of, outputs = int(np.ceil(len(s) / int(bsz * gpu_multiplier))), int(round(best_of)), []\n",
    "    if n == 1 and best_of != 1: n, best_of = best_of, n\n",
    "    if best_of == 1 and n != 1: best_of = n\n",
    "    gc.collect()\n",
    "    for i in range(n_bats):\n",
    "        s_batch, tc_b = s[i * int(bsz * gpu_multiplier):(i + 1) * int(bsz * gpu_multiplier)], []\n",
    "        for s_ in s_batch:\n",
    "            tc = pt.zeros(N_tokens, dtype=int).to(d)\n",
    "            for t in tokenizer.encode(s_): tc[t] += 1\n",
    "            tc_b.append(tc)\n",
    "        p, sts = gen_probs(s_batch, return_sts=True, tcounts=tc_b, **kwargs)\n",
    "        sql_b = [len(tokenizer.encode(s_)) for s_ in s_batch]\n",
    "        mlen = max(sql_b)\n",
    "        sql_b = pt.tensor(sql_b).to(d)\n",
    "        # First use the (as yet undiverged) token distribution (multinomial) to generate n tokens for each sample\n",
    "        tokens = pt.multinomial(p, n, replacement=True) \n",
    "        outs, avg_logprobs = [], []\n",
    "        for j in range(n):\n",
    "            tks, tc_b_itr = tokens[:, j], [t.clone() for t in tc_b]\n",
    "            for j in range(len(tc_b_itr)): tc_b_itr[j][tks[j]] += 1\n",
    "            gc.collect(), pt.cuda.empty_cache()\n",
    "            p, st = gprobs((pt.unsqueeze(tks, -1), None, sql_b), past=sts, return_sts=True, tcounts=tc_b_itr, add=1, **kwargs)\n",
    "            su, ls, out = pt.log(p[pt.arange(p.shape[0]), tks]), pt.ones(p.shape[0]).to(d), [tks]\n",
    "            for token_i in range(max_tokens - 1):\n",
    "                t = pt.multinomial(p, 1).to(d)[:, 0]\n",
    "                out.append(t)\n",
    "                for j in range(len(tc_b_itr)): tc_b_itr[j][t[j]] += 1\n",
    "                cont = t != pad_token\n",
    "                ls += cont.int()\n",
    "                su += cont * pt.log(p[pt.arange(p.shape[0]), t])\n",
    "                if token_i == max_tokens - 1: break\n",
    "                p, st = gprobs((pt.unsqueeze(t,-1),None,sql_b), past=st,return_sts=True,tcounts=tc_b_itr,add=token_i+2,**kwargs)\n",
    "            outs.append(pt.vstack(out).T), avg_logprobs.append(su / ls)\n",
    "        gc.collect(), pt.cuda.empty_cache()\n",
    "        outs = pt.stack(outs, 1)\n",
    "        avg_logprobs = pt.vstack(avg_logprobs).T\n",
    "        s1 = outs.shape[0]\n",
    "        idx = pt.argsort(avg_logprobs, axis=1)[:, :best_of].repeat_interleave(max_tokens, 1).reshape(s1, best_of, max_tokens)\n",
    "        outs = pt.gather(outs, 1, idx)\n",
    "        outputs += [[[(tokenizer.decode([x_]),) for x_ in x] for x in o] for o in outs.cpu().detach().numpy()]\n",
    "#     pr([[''.join([x_[0] for x_ in x]) for x in o] for o in outputs])\n",
    "    return outputs\n",
    "mdl = {\"completions\": gen_completions, \"probabilities\": gprobs, \"name\": mname_fn + ',' + modelkey, \"mstr\": str(model)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# optimise fine-tuned model sampling params\n",
    "lgroups_ft = [[4, 5]]\n",
    "bounds_modelf = {\n",
    "  \"temperature\": [0.0001, 1.0],  # a temp of 0 selects the top completion every time due to the infinitely larger logit\n",
    "  \"top_p\": [0.0001, 1.0],        # same with this but more obvious\n",
    "#   \"top_k\": [1, 100],        # fix this to be a log of the input value so that lower values are sampled with high resolution?\n",
    "  \"presence_penalty\": [0.0, 2.0],   # both presence and frequency penalty have optimal values\n",
    "  \"frequency_penalty\": [0.0, 2.0],  # this can also go down to -2.0, probably not useful for our case...\n",
    "#   \"best_of\": [0.51, 5.49], # to do\n",
    "}\n",
    "optimizers_modelf, results_modelf = [], []\n",
    "for lgroup in lgroups_ft:\n",
    "    min_l, max_l = min(lgroup) - 1, max(lgroup)\n",
    "    def fun(temperature, top_p, presence_penalty, frequency_penalty):\n",
    "        global min_l, max_l\n",
    "        ps = locals()\n",
    "        ps[\"best_of\"] = 5.0\n",
    "        return test_sp_conv(ps, min_l=min_l, max_l=max_l, tol=0.005, phase=\"val\", uniform=True, mdl=mdl)[0]\n",
    "    optimizers_modelf.append(BayesianOptimization(f=fun, pbounds=bounds_modelf, verbose=1000))\n",
    "#     optimizers_modelf[-1].probe(params={\"temperature\": 1.0, \"top_p\": 1.0, \"presence_penalty\": 0.0})\n",
    "    optimizers_modelf[-1].maximize(init_points=5, n_iter=10)\n",
    "    results_modelf.append(optimizers_modelf[-1].max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizers_modelf[-1].max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # optimise fine-tuned model params with refined bounds\n",
    "# lgroups_ft = [[4, 5]]\n",
    "# bounds_gptxlf_rf = {\n",
    "#   \"temperature\": [0.0001, 0.003],  # a temp of 0 selects the top completion every time due to the infinitely larger logit\n",
    "#   \"top_p\": [0.001, 1.0],        # same with this but more obvious\n",
    "# #   \"top_k\": [1, 100],        # fix this to be a log of the input value so that lower values are sampled with high resolution?\n",
    "#   \"presence_penalty\": [0.1, 1.0],   # both presence and frequency penalty have optimal values\n",
    "# #   \"frequency_penalty\": [0.0, 1.0],  # this can also go down to -2.0, probably not useful for our case...\n",
    "# #   \"best_of\": [0.51, 5.49], # to do\n",
    "# }\n",
    "# optimizers_gpt2xlf_rf, results_gpt2xlf_rf = [], []\n",
    "# for lgroup in lgroups_ft:\n",
    "#     min_l, max_l = min(lgroup) - 1, max(lgroup)\n",
    "#     def fun(temperature, top_p, presence_penalty):\n",
    "#         global min_l, max_l\n",
    "#         ps = locals()\n",
    "#         ps[\"best_of\"] = 5.0\n",
    "#         return test_sp_conv(ps, min_l=min_l, max_l=max_l, tol=0.002, phase=\"val\", uniform=True, mdl=mdl)[0]\n",
    "#     optimizers_gpt2xlf_rf.append(BayesianOptimization(f=fun, pbounds=bounds_gptxlf_rf, verbose=1000))\n",
    "#     optimizers_gpt2xlf_rf[-1].probe(params={\"temperature\": 0.001, \"top_p\": 0.001, \"presence_penalty\": 0.4052})\n",
    "#     optimizers_gpt2xlf_rf[-1].maximize(init_points=2, n_iter=8)\n",
    "#     results_gpt2xlf_rf.append(optimizers_gpt2xlf_rf[-1].max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizers_gpt2xlf_rf[-1].probe(params={\"temperature\": 0.001, \"top_p\": 0.001, \"presence_penalty\": 0.4052})\n",
    "# optimizers_gpt2xlf_rf[-1].maximize(n_iter=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizers_gpt2xlf[-1].max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load predictions from the top performing fine-tuned model, and create the ensemble mdl object\n",
    "# optimal_params = {\"temperature\": ?, \"top_p\": ?, \"presence_penalty\": ?, \"frequency_penalty\": ?}\n",
    "# optimal_params = optimizers_gpt2xlf[-1].max[\"params\"]\n",
    "optimal_params = {**default_sp, **{'frequency_penalty': 0.0727219392109375,\n",
    "                                   'presence_penalty': 0.3910543467725913,\n",
    "                                   'temperature': 0.057551002098534226,\n",
    "                                   'top_p': 0.25933633649499327}}\n",
    "def approx_eq(x, y, tol=1e-7):\n",
    "    return abs(x - y) < tol\n",
    "def get_sp_samples(params, test=False):  # Get all datapoints which match these parameters (todo: for each length group)\n",
    "    dn = 'msp_samples_nb' + (\"_test\" if test else '') + '/'\n",
    "    dname = 'data/learning_data/' + dn\n",
    "    D, R, inds = [], [], []  # Gathered input data and model results\n",
    "    for i in range(len(cats)):\n",
    "        idx_set = train_idx if i in train_idx else (val_idx if i in val_idx else test_idx)\n",
    "        D_ = []\n",
    "        fns = glob.glob(dname + str(i) + '/*')\n",
    "        if len(fns) == 0: continue\n",
    "        fns = [fn.split('/')[-1].split('\\\\')[-1] for fn in fns][::-1]  # Most recent first\n",
    "        load_fns = []\n",
    "        for fn in fns:\n",
    "            fn_split = fn.split('_')\n",
    "            params = {sps_[i]: float(fn_split[i + 1]) for i in range(len(sps_))}\n",
    "            if (\"ernst_one\" in fn) and all([approx_eq(params[k], optimal_params[k]) for k in sps_]): load_fns.append(fn)\n",
    "        for fn in load_fns[:15]:\n",
    "            params, d, _, r, mdl_str = load_ld(dn + str(i) + '/' + fn.split('.data')[0])\n",
    "            D_ += d\n",
    "            R += r\n",
    "        D += D_\n",
    "        i_in_set = idx_set.tolist().index(i)\n",
    "        inds += [i_in_set for _ in range(len(D_))]\n",
    "    return D, R, np.asarray(inds)\n",
    "mdl2_d, mdl2_r, mdl2_inds = get_sp_samples(optimal_params)\n",
    "mdl2_d_test, mdl2_r_test, mdl2_inds_test = get_sp_samples(optimal_params, test=True)\n",
    "mdl2_d_x, mdl2_d_test_x = [d_[0] for d_ in mdl2_d], [d_[0] for d_ in mdl2_d_test]\n",
    "def precomputed_completions(d_x, **kwargs):\n",
    "    return mdl2_r if (d_x == mdl2_d_x) else (mdl2_r_test if d_x == mdl2_d_test_x else None)\n",
    "mdl2 = {\"completions\": precomputed_completions, \"name\": \"precomputed_one\", \"mstr\": \"precomputed_one_mstr\"}\n",
    "len(mdl2_d), len(mdl2_d_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate ensemble of fine-tuned and original model\n",
    "lgroups_ft = [[4, 5]]\n",
    "bounds_gptxlfe = {\n",
    "  \"temperature\": [0.0001, 1.0],  # a temp of 0 selects the top completion every time due to the infinitely larger logit\n",
    "  \"top_p\": [0.0001, 1.0],        # same with this but more obvious\n",
    "#   \"top_k\": [1, 100],        # fix this to be a log of the input value so that lower values are sampled with high resolution?\n",
    "  \"presence_penalty\": [0.0, 2.0],   # both presence and frequency penalty have optimal values\n",
    "  \"frequency_penalty\": [0.0, 2.0],  # this can also go down to -2.0, probably not useful for our case...\n",
    "#   \"best_of\": [0.51, 5.49], # to do\n",
    "}\n",
    "optimizers_gpt2xlfe, results_gpt2xlfe = [], []\n",
    "for lgroup in lgroups_ft:\n",
    "    min_l, max_l = min(lgroup) - 1, max(lgroup)\n",
    "    def fun(temperature, top_p, presence_penalty, frequency_penalty):\n",
    "        global min_l, max_l\n",
    "        ps = locals()\n",
    "        ps[\"best_of\"] = 5.0\n",
    "        return test_sp(ps, n1=1, min_l=min_l, max_l=max_l, phase=\"val\", uniform=True, mdl=mdl, mdl2=mdl2, return_test_acc=False,\n",
    "                       d=mdl2_d, d_test=mdl2_d_test, inds=mdl2_inds, inds_test=mdl2_inds_test)\n",
    "    optimizers_gpt2xlfe.append(BayesianOptimization(f=fun, pbounds=bounds_gptxlfe, verbose=1000))\n",
    "    optimizers_gpt2xlfe[-1].probe(params={\"temperature\": 1.0, \"top_p\": 1.0, \"presence_penalty\": 0.0, \"frequency_penalty\": 0.0})\n",
    "    optimizers_gpt2xlfe[-1].maximize(init_points=8, n_iter=10)\n",
    "    results_gpt2xlfe.append(optimizers_gpt2xlfe[-1].max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizers_gpt2xlfe[-1].max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizers_gpt2xlfe[-1].res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |   iter    |  target   | freque... | presen... | temper... |   top_p   |\n",
    "# -------------------------------------------------------------------------\n",
    "# [0.14608262108262107, 0.1544311013061013, 0.1774588258963259]\n",
    "# [0.5712862692862692, 0.5167355977355977, 0.5436751026751027]\n",
    "# ('Test acc:', 15.932418276168276, 'sd:', 1.326833930141234)\n",
    "# |  1        |  0.5439   |  0.7545   |  0.8837   |  1.164    |  0.1678   |\n",
    "# [0.006944444444444444, 0.008333333333333333, 0.025000000000000005]\n",
    "# [0.008, 0.0, 0.014666666666666668]\n",
    "# ('Test acc:', 1.3425925925925926, 'sd:', 0.8203724604939516)\n",
    "# |  2        |  0.007556 |  1.563    |  1.115    |  1.345    |  0.4973   |\n",
    "# [0.21354131979131977, 0.173926362988863, 0.13297558922558925, 0.25522775835275835]\n",
    "# [0.5312100122100122, 0.6170219780219779, 0.5921578421578422, 0.5759225219225219]\n",
    "# ('Test acc:', 19.39177575896326, 'sd:', 4.543568017003007)\n",
    "# |  3        |  0.5791   |  1.185    |  1.479    |  0.3418   |  0.9886   |\n",
    "# [0.35648148148148145, 0.3520833333333333, 0.3180147058823529, 0.332175925925926, 0.2954656862745098]\n",
    "# [0.6432820512820513, 0.7113333333333333, 0.7075555555555556, 0.8067179487179486, 0.738952380952381]\n",
    "# ('Test acc:', 33.08442265795207, 'sd:', 2.247834350297864)\n",
    "# |  4        |  0.7216   |  0.5497   |  0.4131   |  0.04958  |  0.5878   |\n",
    "# [0.24212986087986085, 0.20677008177008174, 0.2121680402930403, 0.24574632543382546]\n",
    "# [0.5814134754134754, 0.5753290043290044, 0.6005385725385726, 0.5997070707070707]\n",
    "# ('Test acc:', 22.67035770942021, 'sd:', 1.73869387881864)\n",
    "# |  5        |  0.5892   |  0.989    |  1.323    |  0.6668   |  0.5829   |\n",
    "# [0.006944444444444444, 0.0, 0.0]\n",
    "# [0.0, 0.0, 0.0]\n",
    "# ('Test acc:', 0.23148148148148145, 'sd:', 0.32736425054932755)\n",
    "# |  6        |  0.0      |  1.268    |  1.645    |  1.636    |  0.3853   |\n",
    "# [0.029563492063492066, 0.05555555555555556, 0.03511904761904762, 0.024537037037037038, 0.01636904761904762]\n",
    "# [0.07957142857142857, 0.13923809523809524, 0.07619047619047618, 0.15666666666666665, 0.1219047619047619]\n",
    "# ('Test acc:', 3.222883597883598, 'sd:', 1.319310339155671)\n",
    "# |  7        |  0.1147   |  1.617    |  0.524    |  0.9253   |  0.9548   |\n",
    "# [0.0, 0.0, 0.0]\n",
    "# [0.0, 0.008, 0.008]\n",
    "# ('Test acc:', 0.0, 'sd:', 0.0)\n",
    "# |  8        |  0.005333 |  1.573    |  0.1914   |  1.962    |  0.2006   |\n",
    "# [0.29166666666666663, 0.3055555555555555, 0.3819444444444444, 0.24305555555555555, 0.3506944444444444]\n",
    "# [0.6366666666666666, 0.7466666666666666, 0.7399999999999999, 0.7999999999999998, 0.73]\n",
    "# ('Test acc:', 31.458333333333332, 'sd:', 4.809247136994663)\n",
    "# |  9        |  0.7307   |  0.001    |  1.477    |  0.001    |  0.001    |\n",
    "# [0.2643718553548275, 0.2827585030710031, 0.27731273356273356, 0.2543742511573394]\n",
    "# [0.5516450836744954, 0.5830235059058588, 0.544432178932179, 0.5589042819925172]\n",
    "# ('Test acc:', 26.97043357864759, 'sd:', 1.1087671560167434)\n",
    "# |  10       |  0.5595   |  0.001    |  1.146    |  0.5046   |  1.0      |\n",
    "# [0.40625, 0.2916666666666667, 0.34722222222222215]\n",
    "# [0.55, 0.6666666666666665, 0.6066666666666667]\n",
    "# ('Test acc:', 34.83796296296296, 'sd:', 4.678560863751791)\n",
    "# |  11       |  0.6078   |  0.001    |  0.001    |  0.4932   |  0.001    |\n",
    "# [0.32638888888888884, 0.2222222222222222, 0.2916666666666667, 0.2534722222222222, 0.34722222222222215, 0.20138888888888887, 0.21527777777777776, 0.23611111111111108, 0.2847222222222222]\n",
    "# [0.74, 0.6666666666666665, 0.6866666666666665, 0.6133333333333333, 0.7266666666666666, 0.7866666666666666, 0.78, 0.6466666666666666, 0.6933333333333332]\n",
    "# ('Test acc:', 26.427469135802472, 'sd:', 4.8236109901505495)\n",
    "# |  12       |  0.7044   |  1.045    |  1.107    |  0.001    |  0.001    |\n",
    "# [0.2708333333333333, 0.2569444444444444, 0.25, 0.18055555555555555]\n",
    "# [0.62, 0.7666666666666666, 0.6266666666666666, 0.6666666666666665]\n",
    "# ('Test acc:', 23.958333333333332, 'sd:', 3.4895401462225317)\n",
    "# |  13       |  0.67     |  0.7444   |  2.0      |  0.001    |  0.001    |\n",
    "# [0.3333333333333333, 0.3263888888888889, 0.37152777777777773, 0.3020833333333333]\n",
    "# [0.6166666666666667, 0.72, 0.7941025641025641, 0.7266666666666666]\n",
    "# ('Test acc:', 33.33333333333333, 'sd:', 2.4917882108346023)\n",
    "# |  14       |  0.7144   |  0.2284   |  2.0      |  0.001    |  1.0      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate un-fine-tuned model\n",
    "lgroups_ft = [[4, 5]]\n",
    "bounds_gptxl = {\n",
    "  \"temperature\": [0.0001, 1.1],  # a temp of 0 selects the top completion every time due to the infinitely larger logit\n",
    "  \"top_p\": [0.0001, 1.0],        # same with this but more obvious\n",
    "#   \"top_k\": [1, 100],        # fix this to be a log of the input value so that lower values are sampled with high resolution?\n",
    "  \"presence_penalty\": [0.0, 2.0],   # both presence and frequency penalty have optimal values\n",
    "  \"frequency_penalty\": [0.0, 2.0],  # this can also go down to -2.0, probably not useful for our case...\n",
    "#   \"best_of\": [0.51, 5.49], # to do\n",
    "}\n",
    "optimizers_gpt2xl, results_gpt2xl = [], []\n",
    "for lgroup in lgroups_ft:\n",
    "    min_l, max_l = min(lgroup) - 1, max(lgroup)\n",
    "    def fun(temperature, top_p, presence_penalty, frequency_penalty):\n",
    "        global min_l, max_l\n",
    "        ps = locals()\n",
    "        ps[\"best_of\"] = 5.0\n",
    "        return test_sp_conv(ps, min_l=min_l, max_l=max_l, tol=0.005, phase=\"val\", uniform=True, mdl=mdl)[0]\n",
    "    optimizers_gpt2xl.append(BayesianOptimization(f=fun, pbounds=bounds_gptxl, verbose=1000))\n",
    "    optimizers_gpt2xl[-1].maximize(init_points=8, n_iter=10)\n",
    "    results_gpt2xl.append(optimizers_gpt2xl[-1].max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test accuracy for the top performing (training accuracy ) sampling parameters for gpt2-\n",
    "# \n",
    "# |   iter    |  target   | freque... | presen... | temper... |   top_p   |\n",
    "# -------------------------------------------------------------------------\n",
    "# [0.049999999999999996, 0.025000000000000005, 0.061111111111111116]\n",
    "# [0.008, 0.03333333333333333, 0.029333333333333336]\n",
    "# ('Test acc:', 4.537037037037037, 'sd:', 1.5101394842870455)\n",
    "# |  1        |  0.02356  |  0.7232   |  0.9207   |  1.792    |  0.1562   |\n",
    "# [0.2111111111111111, 0.13333333333333333, 0.23576388888888888, 0.3333333333333333]\n",
    "# [0.1965714285714286, 0.11199999999999999, 0.111, 0.12]\n",
    "# ('Test acc:', 22.838541666666668, 'sd:', 7.141744752411932)\n",
    "# |  2        |  0.1349   |  0.6561   |  0.6181   |  0.6707   |  0.1418   |\n",
    "# [0.0, 0.016666666666666666, 0.0]\n",
    "# [0.0, 0.0, 0.0]\n",
    "# ('Test acc:', 0.5555555555555556, 'sd:', 0.7856742013183862)\n",
    "# |  3        |  0.0      |  1.035    |  0.6253   |  1.287    |  0.9439   |\n",
    "# [0.5208333333333334, 0.375, 0.3854166666666667, 0.4930555555555555, 0.34027777777777773, 0.40625, 0.3194444444444444]\n",
    "# [0.32, 0.38, 0.21333333333333332, 0.28, 0.16666666666666663, 0.2, 0.2733333333333333]\n",
    "# ('Test acc:', 40.57539682539682, 'sd:', 6.965317270864225)\n",
    "# |  4        |  0.2619   |  0.4573   |  0.1734   |  0.6163   |  0.02537  |\n",
    "# [0.025000000000000005, 0.0, 0.03333333333333333]\n",
    "# [0.008, 0.024000000000000004, 0.008]\n",
    "# ('Test acc:', 1.9444444444444444, 'sd:', 1.4163943093313291)\n",
    "# |  5        |  0.01333  |  1.27     |  1.156    |  1.195    |  0.7833   |\n",
    "# [0.008333333333333333, 0.0, 0.0]\n",
    "# [0.0, 0.0, 0.0]\n",
    "# ('Test acc:', 0.2777777777777778, 'sd:', 0.3928371006591931)\n",
    "# |  6        |  0.0      |  1.297    |  1.846    |  1.938    |  0.7382   |\n",
    "# [0.013888888888888888, 0.0, 0.0]\n",
    "# [0.014666666666666668, 0.008, 0.0]\n",
    "# ('Test acc:', 0.4629629629629629, 'sd:', 0.6547285010986551)\n",
    "# |  7        |  0.007556 |  0.2306   |  1.936    |  1.592    |  0.7024   |\n",
    "# [0.2511354386354386, 0.3434765466015466, 0.30643372830872834, 0.3152858715358715]\n",
    "# [0.32786868686868686, 0.3295645530939648, 0.3073928293928294, 0.3104887334887335]\n",
    "# ('Test acc:', 30.408289627039625, 'sd:', 3.349002098569766)\n",
    "# |  8        |  0.3188   |  0.1998   |  0.08897  |  0.9428   |  0.6384   |\n",
    "# [0.4106481481481481, 0.26304563492063493, 0.31493055555555555, 0.3008207070707071, 0.37310600279350276, 0.38941300733580136]\n",
    "# [0.3358236208236208, 0.1989130869130869, 0.21833333333333332, 0.34352380952380945, 0.3320714285714286, 0.3045044955044955]\n",
    "# ('Test acc:', 34.19940093040583, 'sd:', 5.258394155542585)\n",
    "# |  9        |  0.2889   |  0.01     |  0.01     |  0.1864   |  0.669    |\n",
    "# [0.4861111111111111, 0.3020833333333333, 0.2604166666666667, 0.2916666666666667, 0.3055555555555555, 0.34722222222222215, 0.3541666666666667, 0.2708333333333333]\n",
    "# [0.3833333333333333, 0.20666666666666664, 0.3161904761904762, 0.38, 0.3466666666666666, 0.4137777777777778, 0.29333333333333333, 0.35]\n",
    "# ('Test acc:', 32.72569444444444, 'sd:', 6.743512364375678)\n",
    "# |  10       |  0.3362   |  0.01     |  0.01     |  1.212    |  0.01     |\n",
    "# [0.0, 0.0, 0.0]\n",
    "# [0.0, 0.0, 0.0]\n",
    "# ('Test acc:', 0.0, 'sd:', 0.0)\n",
    "# |  11       |  0.0      |  0.01     |  0.01     |  2.0      |  1.0      |\n",
    "# [0.5181517556517556, 0.2377946127946128, 0.4213624338624338, 0.3735119047619048]\n",
    "# [0.27763369963369966, 0.32315873015873015, 0.23523076923076924, 0.2697142857142857]\n",
    "# ('Test acc:', 38.77051767676768, 'sd:', 10.102443647288354)\n",
    "# |  12       |  0.2764   |  0.01     |  0.01     |  0.6837   |  0.1847   |\n",
    "# [0.19999999999999998, 0.23750000000000002, 0.2722222222222222, 0.375, 0.20833333333333334, 0.0625, 0.16666666666666666, 0.325]\n",
    "# [0.22, 0.08, 0.10400000000000001, 0.264, 0.12, 0.096, 0.2, 0.16]\n",
    "# ('Test acc:', 23.09027777777778, 'sd:', 9.035987186191242)\n",
    "# |  13       |  0.1555   |  0.8694   |  0.01     |  0.01     |  1.0      |\n",
    "# [0.5590277777777778, 0.3819444444444444, 0.31527777777777777, 0.3541666666666667, 0.37152777777777773, 0.375, 0.518287037037037, 0.34027777777777773, 0.4236111111111111]\n",
    "# [0.2338095238095238, 0.19422222222222224, 0.26, 0.25666666666666665, 0.3053333333333333, 0.31, 0.305, 0.36, 0.256]\n",
    "# ('Test acc:', 40.434670781893004, 'sd:', 7.765741054904079)\n",
    "# |  14       |  0.2757   |  0.01     |  0.8582   |  0.01     |  1.0      |\n",
    "# [0.35763888888888884, 0.3756944444444444, 0.33796296296296297, 0.38055555555555554, 0.2824074074074074, 0.3171296296296296, 0.43402777777777773]\n",
    "# [0.3680952380952381, 0.23466666666666666, 0.2222222222222222, 0.22, 0.32, 0.14, 0.22666666666666666]\n",
    "# ('Test acc:', 35.50595238095238, 'sd:', 4.524186608251292)\n",
    "# |  15       |  0.2474   |  0.01     |  2.0      |  0.01     |  1.0      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test accuracy for the top performing (training accuracy 0.2026) sampling parameters for gpt2-small (after the full 18 runs)\n",
    "# np.mean([0.3353320494864612,0.3277777777777778,0.21805555555555556,0.3402514152514152,0.28348214285714285,0.13194444444444445])# = 0.27280723089546616"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With about half an hour of fine-tuning using 7 training samples on an NVidia 980 Ti (showing improvements):\n",
    "# Initial input words on first line, those eventually found by repeatedly querying model + recursively adding words on 2nd & 3rd\n",
    "input_sentence = (\"pace, silence, diversion, attention, plot twist, cliffhanger, surprise, fate, fourth wall, monologue\" + \\\n",
    "\"reinterpretation, harmony, character progression, reading circle\")\n",
    "# \"monolith, elevator effect, time loop, survival, desert resort, town watchman\")\n",
    "# \"monolith, allegro, soundtrack, chord, classical, opera\")\n",
    "input_sentence = input_sentence.split(', ')\n",
    "np.random.shuffle(input_sentence)\n",
    "input_sentence = \"A list of types of element of drama and writing: \"+', '.join(input_sentence[:10])+', '\n",
    "olen = len(input_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_sentence = append_next_token(input_sentence, top_k=25, top_p=0.6, temperature=15.0, olen=olen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # With about half an hour of fine-tuning using 7 training samples on an NVidia 980 Ti (showing improvements):\n",
    "# # Initial input words on first line, those eventually found by repeatedly querying model + recursively adding words on 2nd & 3rd\n",
    "# input_sentence = (\"pace, silence, diversion, attention, plot twist, cliffhanger, surprise, fate, fourth wall, monologue\" + \\\n",
    "# \"reinterpretation, harmony, character progression, reading circle\")\n",
    "# # \"monolith, elevator effect, time loop, survival, desert resort, town watchman\")\n",
    "# # \"monolith, allegro, soundtrack, chord, classical, opera\")\n",
    "# input_sentence = input_sentence.split(', ')\n",
    "# np.random.shuffle(input_sentence)\n",
    "# input_sentence = \"A list of types of element of drama and writing: \"+', '.join(input_sentence[:10])+', '\n",
    "# olen = len(input_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# input_sentence = append_next_token(input_sentence, top_k=25, top_p=0.6, temperature=15.0, olen=olen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # With about half an hour of fine-tuning using 7 training samples on an NVidia 980 Ti (showing improvements):\n",
    "# # Initial input words on first line, those eventually found by repeatedly querying model + recursively adding words on 2nd & 3rd\n",
    "# input_sentence = (\"coffee, water, tea, coke, lemonade, milkshake, \" + \\\n",
    "# \"watermelon juice, cherry juice, blackcurrant mixture, kava, orangeade, lemon juice, cherryade, cranberry juice\")\n",
    "# input_sentence = input_sentence.split(', ')\n",
    "# np.random.shuffle(input_sentence)\n",
    "# input_sentence = \"A list of types of element of drama and writing: \"+', '.join(input_sentence[:10])+', '\n",
    "# olen = len(input_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# input_sentence = append_next_token(input_sentence, top_k=10, top_p=0.8, temperature=5.0, olen=olen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # With about half an hour of fine-tuning using 7 training samples on an NVidia 980 Ti (showing improvements):\n",
    "# # Initial input words on first line, those eventually found by repeatedly querying model + recursively adding words on 2nd & 3rd\n",
    "# input_sentence = (\"pace, silence, diversion, attention, plot twist, cliffhanger, surprise, fate, fourth wall, monologue\" + \\\n",
    "# \"\")\n",
    "# input_sentence = input_sentence.split(', ')\n",
    "# np.random.shuffle(input_sentence)\n",
    "# input_sentence = \"A list of types of element of drama and writing: \"+', '.join(input_sentence[:10])+', ' # Randomly generate shuffled drinks sublist\n",
    "# olen = len(input_sentence)\n",
    "# input_sentence = append_next_token(input_sentence, top_k=5, top_p=0.9, temperature=5.0, olen=olen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With about half an hour of fine-tuning using 7 training samples on an NVidia 980 Ti (showing improvements):\n",
    "# Initial input words on first line, those eventually found by repeatedly querying model + recursively adding words on 2nd & 3rd\n",
    "# input_sentence = (\"coffee, water, tea, coke, lemonade, milkshake, \" + \\\n",
    "# \"milk, soda\")\n",
    "# input_sentence = input_sentence.split(', ')\n",
    "# np.random.shuffle(input_sentence)\n",
    "# input_sentence = \"A list of types of drink: \"+', '.join(input_sentence[:10])+', ' # Randomly generate shuffled drinks sublist\n",
    "# olen = len(input_sentence)\n",
    "# input_sentence = append_next_token(input_sentence, top_k=25, top_p=0.7, temperature=2.0, olen=olen)\n",
    "# input_sentence = append_next_token(input_sentence, top_k=5, top_p=0.9, temperature=5.0, olen=olen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # With about half an hour of fine-tuning using 7 training samples on an NVidia 980 Ti (showing improvements):\n",
    "# # Initial input words on first line, those eventually found by repeatedly querying model + recursively adding words on 2nd & 3rd\n",
    "# input_sentence = (\"coffee, water, tea, coke, lemonade, milkshake\" + \\\n",
    "# \", liquor, wine, juice, beer, milk, soft drink, whiskey, vodka, spirits, soda, ice water, ice cold beer, cider, yoghurt, soda pop, rum, chocolate milk, hot cocoa, alcohol\")\n",
    "# input_sentence = input_sentence.split(', ')\n",
    "# np.random.shuffle(input_sentence)\n",
    "# input_sentence = \"A list of types of drink: \"+', '.join(input_sentence[:10])+', ' # Randomly generate shuffled drinks sublist\n",
    "# olen = len(input_sentence)\n",
    "# input_sentence = append_next_token(input_sentence, top_k=25, top_p=0.7, temperature=2.0, olen=olen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # With about half an hour of fine-tuning using 7 training samples on an NVidia 980 Ti (showing improvements):\n",
    "# # Initial input words on first line, those eventually found by repeatedly querying model + recursively adding words on 2nd & 3rd\n",
    "# input_sentence = (\"coffee, water, tea, coke, lemonade, milkshake\" + \\\n",
    "# \", liquor, wine, juice, beer, milk, soft drink, whiskey, vodka, spirits, soda, ice water, ice cold beer, cider\" + \\\n",
    "# \", yoghurt, soda pop, rum, chocolate milk, hot cocoa, alcohol\")\n",
    "# input_sentence = input_sentence.split(', ')\n",
    "# np.random.shuffle(input_sentence)\n",
    "# input_sentence = \"A list of types of drink: \"+', '.join(input_sentence[:10])+', ' # Randomly generate shuffled drinks sublist\n",
    "# olen = len(input_sentence)\n",
    "# input_sentence = append_next_token(input_sentence, top_k=25, top_p=0.7, temperature=2.0, olen=olen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changelog 16/04/2021 5pm: Using log_period=1 (max_len=96) reliably finds improvement, need more data and larger gpt2 (medium+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With about half an hour of fine-tuning using 7 training samples on an NVidia 980 Ti (showing improvements):\n",
    "# Initial input words on first line, those eventually found by repeatedly querying model + recursively adding words on 2nd & 3rd\n",
    "# input_sentence = (\"coffee, water, tea, coke, lemonade, milkshake,\" + \\\n",
    "# \" milk, vodka, beer, ice water, soda, lassi, juice, alcohol, whiskey\")\n",
    "# input_sentence = input_sentence.split(', ')\n",
    "# np.random.shuffle(input_sentence)\n",
    "# input_sentence = \"A list of types of drink: \"+', '.join(input_sentence[:10])+', ' # Randomly generate shuffled drinks sublist\n",
    "# olen = len(input_sentence)\n",
    "# input_sentence = append_next_token(input_sentence, top_k=25, top_p=0.9, temperature=2.0, olen=olen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With about half an hour of fine-tuning using 7 training samples on an NVidia 980 Ti (showing improvements):\n",
    "# Initial input words on first line, those eventually found by repeatedly querying model + recursively adding words on 2nd & 3rd\n",
    "# input_sentence = (\"coffee, water, tea, coke, lemonade, milkshake,\" + \\\n",
    "# \" wine, soda, alcohol, beer, liquor, apple cider, whiskey, milk, bourbon, vodka, cider, lemon juice\")\n",
    "# input_sentence = input_sentence.split(', ')\n",
    "# np.random.shuffle(input_sentence)\n",
    "# input_sentence = \"A list of types of drink: \"+', '.join(input_sentence[:6])+', ' # Randomly generate shuffled drinks sublist\n",
    "# olen = len(input_sentence)\n",
    "# input_sentence = append_next_token(input_sentence, top_k=30, top_p=0.7, temperature=3.0, olen=olen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With about half an hour of fine-tuning using 7 training samples on an NVidia 980 Ti (showing improvements):\n",
    "# Initial input words on first line, those eventually found by repeatedly querying model + recursively adding words on 2nd & 3rd\n",
    "# input_sentence = (\"coffee, water, tea, coke, lemonade, milkshake,\" + \\\n",
    "# \" beer, milk, juice, wine, spirits, alcohol, soda, whiskey, brandy, apple juice, liquor, ice tea, watermelon juice, vodka,\" + \\\n",
    "# \" lemon tea, apple cider, ale, lager, fruit tea, lime cider, cocktail, mocha, red wine, apple soda\")\n",
    "# input_sentence = input_sentence.split(', ')\n",
    "# np.random.shuffle(input_sentence)\n",
    "# input_sentence = \"A list of types of drink: \"+', '.join(input_sentence[:10])+', ' # Randomly generate shuffled drinks sublist\n",
    "# olen = len(input_sentence)\n",
    "# input_sentence = append_next_token(input_sentence, top_k=30, top_p=0.7, temperature=2.0, olen=olen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changelog 16/04/2021 5am: lr=1e-5, max_len=80, log_period_batches=5 increased accuracy by 8%. Need to add a bunch more data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With about half an hour of fine-tuning using 5 training samples on an NVidia 980 Ti (showing improvements):\n",
    "# Initial input words on first line, those eventually found by repeatedly querying model + recursively adding words on 2nd & 3rd\n",
    "# input_sentence = (\"coffee, water, tea, coke, lemonade, milkshake, \" + \\\n",
    "# \"soda, milk, juice, wine, vodka, gin, lime juice, beer, hot chocolate, cider, whiskey, fruit juice, cocktail, liquor, \" + \\\n",
    "# \"spirits, watermelon juice, martini, rum, chocolate milk, orangeade\")\n",
    "# input_sentence = input_sentence.split(', ')\n",
    "# np.random.shuffle(input_sentence)\n",
    "# input_sentence = \"A list of types of drink: \"+', '.join(input_sentence[:10])+', ' # Randomly generate shuffled drinks sublist\n",
    "# olen = len(input_sentence)\n",
    "# input_sentence = append_next_token(input_sentence, top_k=25, top_p=0.9, temperature=1.89, olen=olen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without fine tuning (regular GPT-2):\n",
    "# Initial input words on first line, those eventually found by repeatedly querying model on 2nd\n",
    "# input_sentence = (\"coffee, water, tea, coke, lemonade, milkshake, \" + \\\n",
    "# \"milk, juice, fruit juice, soda, wine, beer, hot chocolate, chocolate milk, alcohol, cider, ice tea, liquor, spirit\")\n",
    "# input_sentence.split(', ')\n",
    "# np.random.shuffle(input_sentence)\n",
    "# input_sentence = \"A list of types of drink: \"+', '.join(input_sentence[:10])+', ' # Randomly generate shuffled drinks sublist\n",
    "# olen = len(input_sentence)\n",
    "# input_sentence = append_next_token(input_sentence, top_k=25, top_p=0.75, temperature=1.89, olen=olen) # k = 25 also used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training for too long (in this case ~6:30 hours) overfits\n",
    "# input_sentence = (\"coffee, water, tea, coke, lemonade, milkshake, \" + \\\n",
    "# \"beer, juice, soda, liquor, wine, tequila, spirits, alcohol, cocktail, martini, whiskey, rum, vodka\")\n",
    "# input_sentence = input_sentence.split(', ')\n",
    "# np.random.shuffle(input_sentence)\n",
    "# input_sentence = \"A list of types of drink: \"+', '.join(input_sentence[:10])+', ' # Randomly generate shuffled drinks sublist\n",
    "# olen = len(input_sentence)\n",
    "# input_sentence = append_next_token(input_sentence, top_k=25, top_p=0.9, temperature=1.89, olen=olen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changelog 15/04/2021 11am: Found learning rate 1e-7, max_listlen 15, min_nw 0.7, max_nw 0.9, lidstone_eps 0.01 works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No fine tuning (testing gpt2) (old append function):\n",
    "# input_sentence = \"A list of types of drink: coffee, water, tea, coke, lemonade, milkshake\"\n",
    "# input_sentence = append_next_token(input_sentence, top_k=25, top_p=0.75, temperature=1.89)\n",
    "# A list of types of drink: juice, tea, cider, lemonade, milk, beer, hot chocolate,➡ ice cold milk. Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working (reproducible) examples using various non-fine-tuned models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT3 (via AI Dungeon) (Randomness = 2.0, model = Dragon):\n",
    "# sentence = \"A list of ML algorithms: inverse reinforcement learning, ELMo, decision tree, LDA, \"\n",
    "# expected_completion = \"MLP, MLL, MMM. You can't believe you're actually using these things!\"\n",
    "\n",
    "# sentence = \"A list of animals seen in the wild: wolffish, woodlouse, sheep, zebra, yak, \"\n",
    "# expected_completion = \"goat, fox, dog, rat. You're guessing that a lot of other animals have been seen as well; maybe even all the animals on your list except for wolf and rat?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT2 (via Write with Transformer) (Top-p = 0.67, temperature = 1.89, max time = 1.9):\n",
    "# sentence = \"A list of round fruits: peach, apricot, lime, plum, blackberry, cantaloupe, nectarine, pitaya, persimmon, \"\n",
    "# expected_completion = \"mango, papaya and raspberry, as also many\"\n",
    "\n",
    "# sentence = \"A list of chemical elements: hydrogen, carbon, oxygen, nitrogen, gold, \"\n",
    "# expected_completion = \"silver, aluminum, potassium and phosphorus; atomic number.\"\n",
    "\n",
    "# sentence = \"A list of microbes found on earth: bacteria, virus, prokaryote, amoeba, \"\n",
    "# expected_completion = \"archaea, algae, nematode, euk\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_s_avglogs(X, use_log_probs=True):\n",
    "#     tokens = [pt.tensor(tokenizer.encode(x)).to(d) for x in X]\n",
    "#     sqlens = [len(x) for x in tokens]\n",
    "#     n_bats = (len(sqlens) // batch_size) + 1\n",
    "#     res = []\n",
    "#     for i in range(n_bats):\n",
    "#         tokens_b, sqlens_b = tokens[i * bsz:(i + 1) * bsz], sqlens[i * bsz:(i + 1) * bsz]\n",
    "#         xs, _, sqlen = adapt_form(tokens_b, None, sqlens_b, mlen=max(sqlens_b))\n",
    "#         logits = inference(xs, sqlen, seq_maxlen=max(sqlens_b), return_fulloutput=True)[0]\n",
    "#         pt.cuda.empty_cache()\n",
    "# #         logits = pt.stack(logits, 0)\n",
    "#         for i in range(len(sqlens_b)):\n",
    "#             x_logits = [logits[i][j] for j in range(sqlens_b[i])]\n",
    "#             x_logs = ([pt.log(F.softmax(logits[i][j]))[tokens_b[i][j]].cpu().detach().numpy() for j in range(sqlens_b[i])] if \\\n",
    "#                       use_log_probs else \\\n",
    "#                       [logits[i][j][tokens_b[i][j]].cpu().detach().numpy() for j in range(sqlens_b[i])])\n",
    "#             res.append(np.mean(x_logs[1:]))  # ignore initial prefix token 'A'/'An'\n",
    "#     return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = openai.Completion.create(**{**default_params,\n",
    "#   \"prompt\": \"A list of cyclic phenomena: day and night, induction coil, self-oscillation, tornado, runaway greenhouse effect,\",\n",
    "#   \"temperature\": 1.5,\n",
    "#   \"top_p\": 1.0,\n",
    "#   \"n\": 5,\n",
    "#   \"best_of\": 20,\n",
    "#   \"max_tokens\": 7,\n",
    "#   \"stop\": [\",\", \"\\n\"],\n",
    "# })\n",
    "# for choice in response[\"choices\"]:\n",
    "#     d = {}\n",
    "#     tokens = choice[\"logprobs\"][\"tokens\"]\n",
    "#     t_i = -1\n",
    "#     for t in tokens:\n",
    "#         t_i += 1\n",
    "#         r = [(np.e**v, k) for (k, v) in choice[\"logprobs\"][\"top_logprobs\"][t_i].items()]\n",
    "#         r.sort(reverse=True)\n",
    "#         print(sum([v for (v, k) in r]))\n",
    "#         rd = dict([(k, v) for (v, k) in r])\n",
    "#         r = [k for (v, k) in r]\n",
    "#         d[t] = (rd, r, np.e**choice[\"logprobs\"][\"token_logprobs\"][t_i])\n",
    "#     print('|'.join([' '.join((s.replace(\"\\n\", \"⏎\"),'%.2f' % (d[s][2] * 100),\n",
    "#                               str(d[s][1].index(s) + 1) if s in d[s][1] else \"<100\")) for s in tokens]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
