{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A fine-tuned language model for pictionary word list completion (topic/category phrase + example words -> list of 30 examples). For example, one may complete the list\n",
    "\n",
    "\"A list of round fruits: peach, apricot, lime, plum,\"\n",
    "\n",
    "with\n",
    "\n",
    "\"mango, cherry, pineapple, strawberry, pumpkin, watermelon, orange, pomegranate, melon, apple, pear, grapefruit, papaya, lemon, kiwi, passionfruit, blueberry, raspberry, blackberry, cantaloupe, nectarine, pitaya, persimmon, durian, guava, jackfruit, avocado, lychee, soursop, guarana, mangosteen, blackcurrant, cranberry\"\n",
    "\n",
    "using this model. These words should be compatible with pictionary/skribbl.io; i.e., \"sketchable\" within a few minutes, and easily recognisable. Scroll to the end for more examples, or see the file `data/examples.txt`. Sketchability parameter estimation examples can also be found in `data/data.tsv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "with open('../../openai-api-org.txt', 'r') as f: openai.organization = f.read()\n",
    "with open('../../openai-api-key.txt', 'r') as f: openai.api_key      = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0924 08:48:32.360087 40692 modeling_bert.py:226] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
      "I0924 08:48:32.369063 40692 modeling_xlnet.py:339] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\alfew\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Complete.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import time\n",
    "import string\n",
    "import jsonlines\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as pt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "import matplotlib.pyplot as plt\n",
    "from bayes_opt import BayesianOptimization\n",
    "from IPython.utils import io\n",
    "from IPython.display import clear_output\n",
    "from transformers import GPT2ForSequenceClassification, GPT2LMHeadModel, GPTNeoForCausalLM, ReformerModelWithLMHead, \\\n",
    "                         get_linear_schedule_with_warmup\n",
    "from pytorch_transformers import GPT2Tokenizer\n",
    "from transformers import GPT2TokenizerFast\n",
    "from Learning import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                  ###   Options   ###\n",
    "model_name = \"ernst_one\"\n",
    "gpt2_modelkey = \"gpt2-xl\"               # Pretrained model to start from\n",
    "# gpt2_modelkey = \"gpt2\"\n",
    "test_set_frac = 0.25                 # Fraction of samples to keep as separate test set (word lists)\n",
    "TsN = 200                            # Number of randomly generated prompts for each sample when validating model\n",
    "log_period_batches = 10              # Batches per iteration\n",
    "# learning_rate = 5e-7               # Adam learning rate (default is 5e-5, sentiment classification example had 2e-5)\n",
    "learning_rate = 7e-5\n",
    "# learning_rate = 4e-6\n",
    "adam_epsilon = 1e-8                  # Adam epsilon (default is 1e-8)\n",
    "n_sched_warmup = 0                   # Linear scheduler for optimizer number of warmup steps\n",
    "batch_size = bsz = 64              # Samples per batch\n",
    "# batch_size = bsz = 32\n",
    "# batch_size = bsz = 4\n",
    "N_train_batches = int(1e7 / bsz)     # Total number of batches to show model\n",
    "# max_len = 1024                     # Max n. tokens applied prior to *max_nw (tokens)\n",
    "max_len = 32\n",
    "lidstone_e = 0.01                    # Smoothing for possible words/subwords which are not in the missing list words set\n",
    "lastcomma_repl = ',' # 'EOS', ','    # Token optionally used to replace the final comma that ends the generated phrase\n",
    "use_correct_nouns = True             # Whether to use only correct singular or plural form of category nouns for the given prompt\n",
    "swap_noun = False                    # Whether to swap plural and singular nouns in prompt\n",
    "# rng_train = [0, 512]               # Range of prompt list lengths (number of phrases) to generate for training data\n",
    "rng_train = [3, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = \"cuda\" if pt.cuda.is_available() else \"cpu\"  # Setup torch device(s)\n",
    "d = device = pt.device(dev)\n",
    "# world_size = 1\n",
    "# rank = 0\n",
    "# def setup(rank, world_size): \n",
    "#     os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
    "#     os.environ['MASTER_PORT'] = find_free_port()\n",
    "#     dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)  # initialize the process group\n",
    "# def cleanup():\n",
    "#     dist.destroy_process_group()\n",
    "# # mp.spawn(setup, args=(rank, world_size), nprocs=world_size)\n",
    "# setup(rank, world_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cats, cats_sing, phrases = Listset().load()  # Import word lists dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([317, 1351, 286, 2835, 15921, 25, 22514, 11, 48389, 11, 279, 4127, 11],\n",
       " [317, 1351, 286, 2835, 15921, 25, 22514, 11],\n",
       " [48389, 11, 279, 4127, 11],\n",
       " [32, 1351, 286, 2835, 15921, 25, 22514, 11, 48389, 11, 279, 4127, 11],\n",
       " [32, 1351, 286, 2835, 15921, 25, 22514, 11],\n",
       " [273, 6231, 11, 279, 4127, 11])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt3_tokenizer = GPT2TokenizerFast.from_pretrained((\"gpt2-large\"), padding=True)\n",
    "with io.capture_output() as captured:\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(gpt2_modelkey.replace(\"-xl\", \"-large\"), padding=True)\n",
    "tokenizer.encode(\"A list of round fruits: apples, oranges, pears,\"), \\\n",
    "    tokenizer.encode(\"A list of round fruits: apples,\"), \\\n",
    "    tokenizer.encode(\"oranges, pears,\"), \\\n",
    "  gpt3_tokenizer.encode(\"A list of round fruits: apples, oranges, pears,\"), \\\n",
    "      gpt3_tokenizer.encode(\"A list of round fruits: apples,\"), \\\n",
    "      gpt3_tokenizer.encode(\"oranges, pears,\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lprompts_encoded = [[tokenizer.encode(prompt), \"types of\" in prompt] for prompt in lprompts]\n",
    "cats_e = [[tokenizer.encode(c + ': ') for c in cs] for cs in cats]\n",
    "cats_sing_e = [[tokenizer.encode(c + ': ') for c in cs] for cs in cats_sing]\n",
    "phrases_e = [[tokenizer.encode(p + ', ') for p in ps] for ps in phrases]\n",
    "comma_token = pt.tensor(tokenizer.encode(\",\")[0], device=d)\n",
    "lprompts_encoded3 = [[gpt3_tokenizer.encode(prompt), \"types of\" in prompt] for prompt in lprompts]\n",
    "cats_e3 = [[gpt3_tokenizer.encode(c + ': ') for c in cs] for cs in cats]\n",
    "cats_sing_e3 = [[gpt3_tokenizer.encode(c + ': ') for c in cs] for cs in cats_sing]\n",
    "phrases_e3 = [[gpt3_tokenizer.encode(p + ', ') for p in ps] for ps in phrases]\n",
    "comma_token3 = pt.tensor(gpt3_tokenizer.encode(\",\")[0], device=d)\n",
    "N_tokens = len(tokenizer)\n",
    "N_wordlists = len(cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lprompts_encoded = [[pt.tensor(prmt, device=d), pt.tensor(typesof, device=d)] for (prmt, typesof) in lprompts_encoded]\n",
    "cats_e = [[pt.tensor(c, device=d) for c in cs] for cs in cats_e]\n",
    "cats_sing_e = [[pt.tensor(c, device=d) for c in cs] for cs in cats_sing_e]\n",
    "phrases_e = [[pt.tensor(p, device=d) for p in ps] for ps in phrases_e]\n",
    "lprompts_sing_encoded = [(prmpt, typesof) for (prmpt, typesof) in lprompts_encoded if typesof ^ swap_noun]\n",
    "lprompts_sing = [tokenizer.decode(prmpt[0].detach().cpu().numpy()) for prmpt in lprompts_sing_encoded]\n",
    "lprompts_encoded3 = [[pt.tensor(prmt, device=d), pt.tensor(typesof, device=d)] for (prmt, typesof) in lprompts_encoded3]\n",
    "cats_e3 = [[pt.tensor(c, device=d) for c in cs] for cs in cats_e3]\n",
    "cats_sing_e3 = [[pt.tensor(c, device=d) for c in cs] for cs in cats_sing_e3]\n",
    "phrases_e3 = [[pt.tensor(p, device=d) for p in ps] for ps in phrases_e3]\n",
    "lprompts_sing3_encoded = [(prmpt, typesof) for (prmpt, typesof) in lprompts_encoded3 if typesof ^ swap_noun]\n",
    "lidstone_e = pt.tensor(lidstone_e, device=d)\n",
    "lid_val = lidstone_e / N_tokens\n",
    "y_zero = (lid_val).repeat(N_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a fixed test set and save to disk. This function defines the next list token prediction problem\n",
    "def gen_truncated_list(prmt, p, rng=rng_train, mlen=max_len):  # prmt = prompt tokens, p = list word/phrase tokens\n",
    "    tkzs, sent, tkix, wordix = [], [], 0, -1   # rng is the inclusive range of list lengths to generate (number of phrases)\n",
    "    min_nw, max_nw, min_nt, max_nt = rng[0], int(rng[1]), 0, int(mlen) - len(prmt)\n",
    "    incl_words = np.random.choice(len(p), min(len(p), max_nw), replace=False)\n",
    "    for phz_i in incl_words:\n",
    "        phz_enc, wordix = p[phz_i], wordix + 1\n",
    "        tkzs.append((tkix, phz_enc))\n",
    "        tkix += len(phz_enc)\n",
    "        sent.append(phz_enc)\n",
    "        if wordix < min_nw: min_nt = tkix\n",
    "        if tkix >= max_nt:\n",
    "            tkix = max_nt\n",
    "            break\n",
    "    if min_nt - 1 >= tkix:  # rare when max_len is large enough (0.5x max possible total list length) (temporary optimisation)\n",
    "        return gen_truncated_list(prmt, p, rng=rng, mlen=mlen)\n",
    "    sent = pt.hstack(sent)[:max_nt]\n",
    "    missing_w = [p[i] for i in range(len(p)) if i not in incl_words]\n",
    "    trunc_ix = np.random.randint(min_nt - 1, tkix)\n",
    "    trunc_n = min([(trunc_ix - ix) for (ix, enc) in tkzs if ix <= trunc_ix])  # N. end phrase tokens\n",
    "    missing_w += [enc for (ix, enc) in tkzs if ix >= (trunc_ix - trunc_n)]\n",
    "    missing_matches = missing_w\n",
    "    if trunc_n > 0:\n",
    "        phr_start = trunc_ix - trunc_n\n",
    "        partial_phr = sent[phr_start:trunc_ix]\n",
    "        missing_matches = [enc for enc in missing_w if len(enc) >= trunc_n and all(enc[:trunc_n] == partial_phr)]\n",
    "    next_tokens = [enc[trunc_n] for enc in missing_matches]\n",
    "    norm = len(next_tokens) * (1.0 + lidstone_e)\n",
    "    tunit, y_ = pt.tensor(1 / norm, device=d), y_zero.clone()\n",
    "    for token in next_tokens: y_[token] += tunit\n",
    "    return pt.hstack([prmt, sent[:trunc_ix]]), y_\n",
    "def stac_sample(stac, n):   # Create batches by random permutations (maximise diversity and uniformity) (shuffling done later)\n",
    "    r, mode, total = [], tuple(np.unique(stac)), stac.shape[0]\n",
    "    while n > 0:\n",
    "        if mode == (0,) or mode == (1,) or mode == (-1,):\n",
    "            if n >= total:\n",
    "                new = sum([list(range(total)) for _ in range(n // total)], [])\n",
    "                r += new\n",
    "                n -= len(new)\n",
    "            else:\n",
    "                new = np.random.choice(total, n, replace=False)\n",
    "                stac[new] = (mode[0] + 1) if mode != (1,) else -1\n",
    "                r += new.tolist()\n",
    "                n = 0\n",
    "        else:\n",
    "            old_val, new_val = mode[0] if mode != (-1, 1) else 1, mode[1] if mode != (-1, 1) else -1\n",
    "            old_i = np.nonzero(stac == old_val)[0]\n",
    "            if n >= old_i.shape[0]:\n",
    "                stac[old_i] = new_val\n",
    "                r += old_i.tolist()\n",
    "                n -= old_i.shape[0]\n",
    "            else:\n",
    "                new = np.random.choice(old_i, n, replace=False)\n",
    "                stac[new] = new_val\n",
    "                r += new.tolist()\n",
    "                n = 0\n",
    "        mode = tuple(np.unique(stac))\n",
    "    return r\n",
    "def gen_samples_uniform(xcp, xcs, xp, n,\n",
    "                        rng=rng_train, prompt=None, tknzr=tokenizer, verbose=False, inds=False, stac=None, mlen=1e9):\n",
    "    xs, ys, sqlens, j, prmt = [], [], [], 0, None              # Weight testing samples (word lists) exactly uniformly\n",
    "    if prompt is not None: prmt = pt.tensor(tknzr.encode(prompt), device=d)\n",
    "    stac_, stac = stac_sample(stac, n) if (stac is not None) else None, stac is not None\n",
    "    if stac: np.random.shuffle(stac_)\n",
    "    for i in (range(len(xcp)) if not stac else stac_):\n",
    "        x, y, sqlen = [], [], []\n",
    "        cp, cs, p = xcp[i], xcs[i], xp[i]\n",
    "        cp_cs = cp + cs\n",
    "        for m in range(n if not stac else 1):\n",
    "            cat_ix = np.random.randint(len(cp_cs))  # First uniformly sample a category title\n",
    "            sing = cat_ix >= len(cp)\n",
    "            if prompt is None:\n",
    "                lprmpts = lprompts_sing_encoded if sing else lprompts_encoded\n",
    "                prmt, _ = lprmpts[np.random.randint(len(lprmpts))]\n",
    "            x_, y_ = gen_truncated_list(pt.hstack([prmt, cp_cs[cat_ix]]), p, rng=rng, mlen=mlen)\n",
    "            x.append(x_)\n",
    "            y.append(y_)\n",
    "            sqlen.append(len(x_))\n",
    "            j += 1\n",
    "            if verbose and j % 100 == 0:\n",
    "                sys_print(\"\\rDone: \" + str(j))\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "        sqlens.append(sqlen)\n",
    "    if inds or stac: xs, ys, sqlens = sum(xs, []), sum(ys, []), sum(sqlens, [])\n",
    "    if verbose: sys_print(\"\\rDone: \" + str(j) + \", finished!\\n\")\n",
    "    return (xs, ys, sqlens, np.arange(len(xcp)).repeat(n)) if inds else (xs, ys, sqlens)\n",
    "def gen_samples(xcp, xcs, xp, n,\n",
    "                rng=rng_train, prompt=None, tknzr=tokenizer, inds=False, mlen=1e9):\n",
    "    xs, ys, sqlens, j, prmt = [], [], [], 0, None   \n",
    "    if prompt is not None: prmt = pt.tensor(tokenizer.encode(prompt), device=d)\n",
    "    xs, ys, sqlens, j = [], [], [], 0\n",
    "    n_sets, indices = len(xcp), []\n",
    "    for m in range(n):  # Maximise per-batch training diversity by randomly sampling the word lists (experimental)\n",
    "        i = np.random.randint(n_sets)\n",
    "        cp, cs, p = xcp[i], xcs[i], xp[i]\n",
    "        cp_cs = cp + cs\n",
    "        cat_ix = np.random.randint(len(cp_cs))  # First uniformly sample a category title\n",
    "        sing = cat_ix >= len(cp)\n",
    "        if prompt is None:\n",
    "            lprmpts = lprompts_sing_encoded if sing else lprompts_encoded\n",
    "            prmt, _ = lprmpts[np.random.randint(len(lprmpts))]\n",
    "        x_, y_ = gen_truncated_list(pt.hstack([prmt, cp_cs[cat_ix]]), p, rng=rng, mlen=mlen)\n",
    "        xs.append(x_)\n",
    "        ys.append(y_)\n",
    "        sqlens.append(len(x_))\n",
    "        indices.append(i)\n",
    "    return (xs, ys, sqlens, np.asarray(indices)) if inds else (xs, ys, sqlens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2  8 21  4  7  5 10 25] \n",
      "Train:\n",
      "['round fruits', 'wild animals', 'microorganisms', 'outback experiences', 'buildings', 'hats', 'holed pasta', 'rod shaped pasta', 'construction sounds', 'sounds of a building', 'biological examples of math in nature', 'non-biological examples of math in nature', 'timbers', 'woodland ecoregions', 'handcrafts', 'communication media', 'storage media', 'winds', 'scientific principles behind showers', 'scientific principles behind rain showers', 'spacecraft types', 'real spacecrafts', 'interpersonal tokens of trust', 'physical tokens that confer trust', 'digital tokens that confer trust']\n",
      "Test:\n",
      "['chemical elements', 'dramatic and literature elements', 'vehicles referred to as crafts', 'music', 'scientific cycles', 'machine learning algorithms', 'glassware', 'windings']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python36\\lib\\site-packages\\ipykernel_launcher.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: 1600, finished!\n"
     ]
    }
   ],
   "source": [
    "N_test = int(test_set_frac * N_wordlists)\n",
    "N_train = N_wordlists - N_test\n",
    "test_idx = np.random.choice(N_wordlists, N_test, replace=False)\n",
    "# save_ld(test_idx, \"test.data\")\n",
    "test_idx = load_ld(\"test.data\")\n",
    "# test_idx = np.array([0, 2])  # Round fruits and chemical elements\n",
    "train_idx = [i for i in range(N_wordlists) if i not in test_idx]\n",
    "print(test_idx, \"\\nTrain:\")\n",
    "print([cats[i][0] for i in train_idx])\n",
    "print(\"Test:\")\n",
    "print([cats[i][0] for i in test_idx])\n",
    "cats_train, cats_test = [cats[i] for i in train_idx], [cats[i] for i in test_idx]\n",
    "cats_sing_train, cats_sing_test = [cats_sing[i] for i in train_idx], [cats_sing[i] for i in test_idx]\n",
    "phrases_train, phrases_test = [phrases[i] for i in train_idx], [phrases[i] for i in test_idx]\n",
    "cats_e_test, cats_sing_e_test = [cats_e[i] for i in test_idx], [cats_sing_e[i] for i in test_idx]\n",
    "phrases_e_test = [phrases_e[i] for i in test_idx]\n",
    "cats_e_train, cats_sing_e_train = [cats_e[i] for i in train_idx], [cats_sing_e[i] for i in train_idx]\n",
    "phrases_e_train = [phrases_e[i] for i in train_idx]\n",
    "test_cats = [cats[i][0] for i in test_idx]\n",
    "test_xs, test_ys, test_sqlens= gen_samples_uniform(cats_e_test, cats_sing_e_test, phrases_e_test, TsN, mlen=max_len,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write function that takes a prompt, existing list and sampling params and returns gpt3's next token probs\n",
    "default_msp = {\n",
    "  \"best_of\": 1,\n",
    "}\n",
    "default_sp = {\n",
    "  \"temperature\": 1.0,\n",
    "  \"top_p\": 1.0,\n",
    "#   \"top_k\": -1.0,                 # todo: add code to apply this to gpt3 output (top100), max k ~=90, min = 2?\n",
    "  \"presence_penalty\": 0.0,\n",
    "  \"frequency_penalty\": 0.0,\n",
    "}\n",
    "default_params = {\n",
    "  \"engine\": \"davinci\",\n",
    "  \"model\": None,\n",
    "  \"max_tokens\": 1,\n",
    "  \"temperature\": 1.0,\n",
    "  \"top_p\": 1.0,\n",
    "#   \"top_k\": -1.0,\n",
    "  \"presence_penalty\": 0.0,\n",
    "  \"frequency_penalty\": 0.0,\n",
    "  \"n\": 1,\n",
    "  \"stream\": False,\n",
    "  \"logprobs\": 100,\n",
    "#       \"logit_bias\": {\"50256\": -100},\n",
    "  \"stop\": [\",\", \"\\n\"],\n",
    "}\n",
    "stop_tokens_e = tokenizer.encode(''.join(default_params[\"stop\"]))\n",
    "# Define and test the OpenAI API next token probability request (response-token-efficient streaming version)\n",
    "def format_gpt3_probs(choice, tokenize):\n",
    "    res, r = [], sorted([(np.e**v, k) for (k, v) in choice[\"logprobs\"][\"top_logprobs\"][0].items()])[::-1]\n",
    "    for i in range(len(r)):\n",
    "        k = gpt3_tokenizer.encode(r[i][1])\n",
    "        if len(k) == 1: res.append((r[i][0], k if tokenize else r[i][1]))\n",
    "    return res\n",
    "def p_req(s, tokenize=False, **kwargs):\n",
    "    use_stream = \"max_tokens\" in kwargs and kwargs[\"max_tokens\"] != 1\n",
    "    kwargs[\"prompt\"], kwargs[\"stream\"] = s, use_stream\n",
    "    with io.capture_output() as captured:\n",
    "        response, result = openai.Completion.create(**{**default_params, **kwargs}), []\n",
    "    return [(np.e**resp[\"choices\"][0][\"logprobs\"][\"token_logprobs\"][0], resp[\"choices\"][0][\"logprobs\"][\"tokens\"][0],\n",
    "             format_gpt3_probs(resp[\"choices\"][0], tokenize)) for resp in (response if use_stream else [response])]\n",
    "# todo: version to handle multiple choices for phrase level evaluation (response-token-expensive)\n",
    "def p_req_m(s, tokenize=False, **kwargs):\n",
    "    if \"max_tokens\" not in kwargs: kwargs[\"max_tokens\"] = 8\n",
    "    if \"n\" not in kwargs: kwargs[\"n\"] = 5\n",
    "    if \"best_of\" in kwargs:\n",
    "        kwargs[\"best_of\"] = int(round(kwargs[\"best_of\"]))\n",
    "        if kwargs[\"n\"] != 1: kwargs[\"best_of\"], kwargs[\"n\"] = kwargs[\"n\"], kwargs[\"best_of\"]\n",
    "    kwargs[\"prompt\"] = s\n",
    "    with io.capture_output() as captured:\n",
    "        response, tokens, probs = openai.Completion.create(**{**default_params, **kwargs}), [], []\n",
    "    for choice in response[\"choices\"]:\n",
    "        tks = [np.e**v for v in choice[\"logprobs\"][\"token_logprobs\"]]\n",
    "        tks = [(choice[\"logprobs\"][\"tokens\"][i], tks[i]) for i in range(len(tks))]\n",
    "        tokens.append(tks)\n",
    "        probs.append(format_gpt3_probs(choice, tokenize))\n",
    "    return tokens, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# a = p_req(\"A list of cyclic phenomena: day and night, induction coil, self-oscillation, tornado, runaway greenhouse effect,\")\n",
    "# b = p_req_m(\"A list of cyclic phenomena: day and night, induction coil, self-oscillation, tornado, runaway greenhouse effect,\")\n",
    "# print(sum([a_[0] for a_ in a[0][2]]))\n",
    "# print(','.join([''.join([b__[0] for b__ in b_]) for b_ in b[0]] + [''.join([a_[1] for a_ in a])]).replace('\\n', 'âŽ'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that takes a completion distribution (top 100) and target next token distribution (multinomial) and computes the\n",
    "# probability that the completion produces a desired output token\n",
    "def prob_corr(pred_p, target_p):\n",
    "    r = 0\n",
    "    if isinstance(pred_p, list):\n",
    "        for (p, token) in pred_p:\n",
    "            if target_p[token] > (lid_val + 1e-10): r += p\n",
    "    else:\n",
    "        r = np.sum(target_p[np.nonzero(pred_p > (lid_val + 1e-10))[0]])\n",
    "    return r\n",
    "# directly computes the similarity between target and predicted token distributions\n",
    "def score_corr(pred_p, target_p, distance=\"cross-entropy\", redistribute_mass=False, include_negatives=False):  \n",
    "    r = 0\n",
    "    if isinstance(pred_p, list) and not redistribute_mass:\n",
    "        for (p, token) in pred_p:\n",
    "            targ = target_p[token]\n",
    "            if targ > (lid_val + 1e-10) or include_negatives:\n",
    "                if   distance == \"unnormalized\":  r -= p * targ\n",
    "                elif distance == \"cross-entropy\": r -= p * np.log(targ)\n",
    "                elif distance == \"kl-divergence\": r += p * np.log(p / targ)\n",
    "                elif distance == \"bhattacharyya\": r += np.sqrt(p * targ)\n",
    "        if distance == \"bhattacharyya\": r = -np.log(r)\n",
    "    else:\n",
    "        p = pred_p\n",
    "        if isinstance(pred_p, list):\n",
    "            p_ = np.asarray([p for (p, _) in pred_p])\n",
    "            ts = np.asarray([t for (_, t) in pred_p])\n",
    "            unaccounted_mass = 1.0 - sum(p_)\n",
    "            n_missing_tokens = N_tokens - len(pred_p)\n",
    "            p = np.repeat(unaccounted_mass / n_missing_tokens, N_tokens)\n",
    "            p[ts] = p_\n",
    "        if not include_negatives:\n",
    "            pos = np.nonzero(target_p > (lid_val + 1e-10))[0]\n",
    "            p, target_p = p[pos], target_p[pos]\n",
    "        if   distance == \"unnormalized\":  r = -np.sum(p * targ)\n",
    "        elif distance == \"cross-entropy\": r = -np.sum(p * np.log(target_p))\n",
    "        elif distance == \"kl-divergence\": r =  np.sum(p * np.log(p / target_p))\n",
    "        elif distance == \"bhattacharyya\": r = -np.log(np.sum(np.sqrt(p * target_p)))\n",
    "    return -r\n",
    "# probability that a completion phrase is a desired missing list entry\n",
    "def prob_msp(outs, missing):\n",
    "    correct = 0\n",
    "    missing = set([phrase.lower() for phrase in missing])\n",
    "    for i in range(len(outs)):\n",
    "        out = outs[i].strip().lower()\n",
    "        if out not in missing and (out[:4] == 'the '): out = out[4:]\n",
    "        if out in missing: correct += 1\n",
    "    return correct / len(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   (   ' Fermi-Pasta-Ulam problem,',\n",
      "        array([  376,  7780,    72,    12, 34533,    64,    12,    52,  2543,\n",
      "        1917,    11], dtype=int64),\n",
      "        11),\n",
      "    (   ' maccheroncini di campofilone,',\n",
      "        array([8352, 2044,  261,   66, 5362, 2566, 1413, 1659,  346,  505,   11],\n",
      "      dtype=int64),\n",
      "        11)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens = sum([[len(p) for p in p_ if len(p) < 1000] for p_ in phrases_e], [])  # Print longest list elements in dataset, get max\n",
    "phrs = sum([[p for p in p_ if len(p) < 1000] for p_ in phrases_e], [])\n",
    "m = np.max(lens)\n",
    "inds = [i for i in range(len(phrs)) if lens[i] == m]\n",
    "ree = [(tokenizer.decode(phrs[i].cpu().detach().numpy()), phrs[i].cpu().detach().numpy(), lens[i]) for i in inds]\n",
    "phrl_max = len(ree[0][1]) - 1\n",
    "pr(ree)\n",
    "phrl_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that takes sampling parameters, then generates n random incomplete list prompts (of length l), obtains completion\n",
    "# distributions (top 100 tokens or full multinomial) and evaluates the average score across the n prompts. n = 20 by default\n",
    "# All samples generated are stored fully for later training of sample-dependent sampling parameter (mixture) distribution\n",
    "sps_ = [\"top_p\", \"temperature\", \"presence_penalty\", \"frequency_penalty\"]  # sampling params                          #top_k\n",
    "msps_= sps_#[\"best_of\"] + sps_                                                 # meta sampling params\n",
    "create_folder(data_dir + learning_data_dir)\n",
    "create_folder(data_dir + learning_data_dir + \"sp_samples\")\n",
    "create_folder(data_dir + learning_data_dir + \"sp_samples_test\")\n",
    "create_folder(data_dir + learning_data_dir + \"msp_samples_nb\")\n",
    "create_folder(data_dir + learning_data_dir + \"msp_samples_nb_test\")\n",
    "def eval_sp(params, min_l=0, max_l=1e9, n=20, prmt=None, phase=\"train\", uniform=True, mdl='gpt3'):\n",
    "    res, max_l, engine_str = [], int(max_l), ','.join([str(params[k]) for k in [\"engine\", \"model\"]])\n",
    "    tknzr = gpt3_tokenizer if mdl == \"gpt3\" else tokenizer\n",
    "    xcp, xcs, xp = \\\n",
    "      ((cats_e3_test,  cats_sing_e3_test,  phrases_e3_test) if phase == \"test\" else \\\n",
    "       (cats_e3_train, cats_sing_e3_train, phrases_e3_train)) if mdl == \"gpt3\" else \\\n",
    "      ((cats_e_test,  cats_sing_e_test,  phrases_e_test) if phase == \"test\" else \\\n",
    "       (cats_e_train, cats_sing_e_train, phrases_e_train))\n",
    "    xs, ys, sqlens, inds = gen_samples_uniform(xcp, xcs, xp, n, prompt=prmt, tknzr=tknzr, inds=True, rng=[min_l, max_l]) \\\n",
    "           if uniform else gen_samples        (xcp, xcs, xp, n, prompt=prmt, tknzr=tknzr, inds=True, rng=[min_l, max_l])\n",
    "    if mdl == 'gpt3': r = [p_req(gpt3_tokenizer.decode(x_.detach().cpu().numpy()), **params) for x_ in xs] \n",
    "    else:             r = mdl[\"probabilities\"](xs, ys, sqlens, **params)\n",
    "    for i in np.unique(inds):\n",
    "        create_folder(data_dir + learning_data_dir + \"sp_samples/\" + str(i))\n",
    "        ix, mdl_name = np.nonzero(inds == i)[0], (mdl if isinstance(mdl, str) else mdl[\"name\"]) + ',' + engine_str\n",
    "        fn = str(time.time()) + '_' + '_'.join([str(v) for v in [params[k] for k in sps_] + [min_l, max_l]]) + '_' + mdl_name\n",
    "        save_ld((params, xs[ix], ys[ix], sqlens[ix], [r[j] for j in ix], str(mdl)), \"sp_samples/\"+ str(i) +\"/\" + fn, compress=9)\n",
    "    return np.mean([score_corr(r[i], ys[i]) for i in range(len(r))])\n",
    "def eval_sp_conv(params, tol=0.01, **kwargs):\n",
    "    center, samples = np.inf, []\n",
    "    while True:\n",
    "        samples.append(eval_sp(params, n=2, **kwargs))\n",
    "        new_center = np.mean(samples)\n",
    "        if abs(center - new_center) < tol: return new_center, samples\n",
    "        new_center = center\n",
    "# This metric differs depending on tokenisation, so for the testing of models, a full phrase accuracy function is required\n",
    "strip_comma = lambda x: x[:-1] if len(x) > 1 else x\n",
    "def test_sp(params, min_l=0, max_l=1e9, n1=3, n2=10, prmt=None, phase=\"train\", uniform=True, mdl='gpt3', max_tokens=phrl_max):\n",
    "    d, max_l, engine_str = [], int(max_l), ','.join([str(params[k]) for k in [\"engine\", \"model\"] if k in params])\n",
    "    cp, cs, p = ((cats_test, cats_sing_test, phrases_test) if phase == \"test\" else (cats_train, cats_sing_train, phrases_train))\n",
    "    for i in range(len(cp)):\n",
    "        x, y, sqlen = [], [], []\n",
    "        cp_, cs_, p_ = cp[i], cs[i], p[i]\n",
    "        cp_cs = cp_ + cs_\n",
    "        for m in range(n1):\n",
    "            cat_ix = np.random.randint(len(cp_cs))  # uniformly sample a category title\n",
    "            sing = cat_ix >= len(cp_)\n",
    "            prompt = prmt\n",
    "            if prompt is None:\n",
    "                lprmpts = lprompts_sing if sing else lprompts\n",
    "                prompt = lprmpts[np.random.randint(len(lprmpts))]\n",
    "            phr_ix = np.random.choice(len(p_), np.random.randint(min_l, min(max_l, len(p_) - 1)), replace=False)\n",
    "            missing_ix = [i for i in range(len(p_)) if i not in phr_ix]\n",
    "            prompt += ' ' + cp_cs[cat_ix] + ': ' + ''.join([p_[j] + ', ' for j in phr_ix])\n",
    "            d.append([prompt[:-1], [p_[j] for j in missing_ix]])\n",
    "    params['n'], params[\"max_tokens\"] = n2, max_tokens\n",
    "    if mdl == 'gpt3': r = [p_req_m(d_[0], **params)[0] for d_ in d]  # Convert to strings and request predictions from OpenAI\n",
    "    else:             r = mdl[\"completions\"]([d_[0] for d_ in d], **params)\n",
    "    for i in range(len(cp)):\n",
    "        create_folder(data_dir + learning_data_dir + \"msp_samples_nb/\" + str(i))\n",
    "        ix, mdl_name = range(i * n1, (i + 1) * n1), mdl if isinstance(mdl, str) else mdl[\"name\"]\n",
    "        fn = str(time.time()) + '_' + '_'.join([str(v) for v in [params[k] for k in msps_] + [min_l, max_l]]) + '_' + mdl_name\n",
    "        save_ld((params, [d[j] for j in ix], [r[j] for j in ix], str(mdl)), \"msp_samples_nb/\" + str(i) + \"/\" + fn, compress=9)\n",
    "    r = [sum([strip_comma(''.join([r___[0] for r___ in r__]).split(',')) for r__ in r_], []) for r_ in r]\n",
    "#     r = [sum([strip_comma(''.join([r___[0] for r___ in r__]).split(','))[:max_l - min_l] for r__ in r_], []) for r_ in r]\n",
    "    if phase == \"train\":  # for validation, output the test set accuracy\n",
    "        d_test = []\n",
    "        cp, cs, p = (cats_test, cats_sing_test, phrases_test)\n",
    "        for i in range(len(cp)):\n",
    "            x, y, sqlen = [], [], []\n",
    "            cp_, cs_, p_ = cp[i], cs[i], p[i]\n",
    "            cp_cs = cp_ + cs_\n",
    "            for m in range(n1 * int((1 - test_set_frac) / test_set_frac)):\n",
    "                cat_ix = np.random.randint(len(cp_cs))  # uniformly sample a category title\n",
    "                sing = cat_ix >= len(cp_)\n",
    "                prompt = prmt\n",
    "                if prompt is None:\n",
    "                    lprmpts = lprompts_sing if sing else lprompts\n",
    "                    prompt = lprmpts[np.random.randint(len(lprmpts))]\n",
    "                phr_ix = np.random.choice(len(p_), np.random.randint(min_l, min(max_l, len(p_) - 1)), replace=False)\n",
    "                missing_ix = [i for i in range(len(p_)) if i not in phr_ix]\n",
    "                prompt += ' ' + cp_cs[cat_ix] + ': ' + ''.join([p_[j] + ', ' for j in phr_ix])\n",
    "                d_test.append([prompt[:-1], [p_[j] for j in missing_ix]])\n",
    "        if mdl == 'gpt3': r_test = [p_req_m(d_[0], **params)[0] for d_ in d_test]\n",
    "        else:             r_test = mdl[\"completions\"]([d_[0] for d_ in d_test], **params)\n",
    "        for i in range(len(cp)):\n",
    "            create_folder(data_dir + learning_data_dir + \"msp_samples_nb_test/\" + str(i))\n",
    "            ix, mdl_name = range(i * n1, (i + 1) * n1), (mdl if isinstance(mdl, str) else mdl[\"name\"]) + ',' + engine_str\n",
    "            fn = str(time.time()) + '_' + '_'.join([str(v) for v in [params[k] for k in msps_] + [min_l, max_l]]) + \\\n",
    "                '_' + mdl_name\n",
    "            save_ld((params, [d_test[j] for j in ix], [r_test[j] for j in ix], str(mdl)),\n",
    "                    \"msp_samples_nb_test/\" + str(i) + \"/\" + fn, compress=9)\n",
    "        r_test = [sum([strip_comma(''.join([r___[0] for r___ in r__]).split(',')) for r__ in r_], []) for r_ in r_test]\n",
    "#r_test = [sum([strip_comma(''.join([r___[0] for r___ in r__]).split(','))[:max_l - min_l] for r__ in r_], []) for r_ in r_test]\n",
    "        test_acc = np.mean([prob_msp(r_test[i], d_test[i][1]) for i in range(len(r_test))])\n",
    "\n",
    "    return np.mean([prob_msp(r[i], d[i][1]) for i in range(len(r))]), test_acc\n",
    "def test_sp_conv(params, tol=0.01, **kwargs):\n",
    "    center, ys, i = np.inf, [], 1\n",
    "    while True:\n",
    "        ys.append(test_sp(params, n1=1, **kwargs))\n",
    "        new_center = np.mean([s[0] for s in ys])\n",
    "        if abs(center - new_center) < tol and i >= 3:\n",
    "            print([s[1] for s in ys])\n",
    "            print([s[0] for s in ys])\n",
    "            sys_print(str((\"Test acc:\", 100*np.mean([s[1] for s in ys]), \"sd:\", np.std([100*s[1] for s in ys])))+\"\\n\", False)\n",
    "            return new_center, ys\n",
    "        center = new_center\n",
    "        i += 1\n",
    "# eval_sp(default_ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define next batch function\n",
    "def adapt_form(xs, ys, sqlens, mlen=max_len, repl_finalcomma=True):\n",
    "    xs = pt.vstack([F.pad(x, (0, max(0, mlen - len(x))), mode='constant', value=pad_token)[:mlen] for x in xs])\n",
    "    _ys = pt.vstack(ys) if ys is not None else None\n",
    "    if repl_finalcomma and (lastcomma_repl != ',') and ys is not None:\n",
    "        _ys[:, repl_token] += _ys[:, comma_token] - lid_val\n",
    "        _ys[:, comma_token] = lid_val\n",
    "    return xs, _ys, pt.tensor(sqlens, device=d)\n",
    "curr_ri = np.zeros(len(train_idx), dtype=int)\n",
    "def next_batch(sz):\n",
    "    global cats_e_train, cats_sing_e_train, phrases_e_train, curr_ri\n",
    "    return adapt_form(*gen_samples_uniform(cats_e_train, cats_sing_e_train, phrases_e_train, sz, stac=curr_ri, mlen=max_len))\n",
    "#     return adapt_form(*gen_samples(cats_e_train, cats_sing_e_train, phrases_e_train, sz, mlen=max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "GPT2 device: cuda:0\n",
      "Pretrained parameters loaded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "331"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if 'model' in locals():\n",
    "    del model\n",
    "pt.cuda.empty_cache()\n",
    "print(gc.collect())\n",
    "create_folder(\"models\")\n",
    "create_folder(\"models/pretrained\")\n",
    "create_folder(\"models/pretrained/GPT2LMHead\")\n",
    "model_ = GPT2LMHeadModel.from_pretrained(gpt2_modelkey,\n",
    "    output_hidden_states=False, output_attentions=False, \n",
    "    cache_dir=\"models/pretrained/GPT2LMHead\")\n",
    "if pt.cuda.device_count() > 1:\n",
    "    device_map = {0: [0, 1, 2],\n",
    "                  1: [3, 4, 5, 6, 7, 8],\n",
    "                  2: [9, 10, 11, 12, 13, 14],\n",
    "                  3: [15, 16, 17, 18, 19, 20],\n",
    "                  4: [21, 22, 23, 24, 25, 26, 27],\n",
    "                  5: [28, 29, 30, 31, 32, 33, 34],\n",
    "                  6: [35, 36, 37, 38, 39, 40, 41],\n",
    "                  7: [42, 43, 44, 45, 46, 47],\n",
    "                 }\n",
    "    model_.parallelize(device_map)\n",
    "else:\n",
    "    model_ = model_.to(d)\n",
    "print(\"GPT2 device:\", model_.device)\n",
    "model_.resize_token_embeddings(N_tokens)\n",
    "pad_token = model_.config.pad_token_id = model_.config.eos_token_id\n",
    "pad_token = pt.tensor(pad_token, device=d)\n",
    "repl_token = pt.tensor(tokenizer.encode(lastcomma_repl)[0], device=d) if lastcomma_repl != 'EOS' else pad_token\n",
    "n_embd = pt.tensor(model_.config.n_embd, device=d)\n",
    "# model = nn.parallel.DistributedDataParallel(model_, device_ids=[d])\n",
    "# model = nn.DataParallel model_, device_ids=list(range(pt.cuda.device_count()))) if dev != \"cpu\" else model_\n",
    "model = model_\n",
    "mname_fn = gpt2_modelkey\n",
    "print(\"Pretrained parameters loaded\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "llayer = None #nn.Linear(n_embd, N_tokens, bias=False).to(d)#.cpu()\n",
    "# nn.init.xavier_uniform_(llayer.weight)\n",
    "# llayer = nn.DataParallel(llayer, device_ids=list(range(pt.cuda.device_count()))) if dev != \"cpu\" else llayer\n",
    "# llayer = nn.parallel.DistributedDataParallel(llayer, device_ids=list(range(pt.cuda.device_count()))).to(d)\n",
    "# softmax = nn.Softmax()\n",
    "bcewl_loss = nn.BCEWithLogitsLoss()#.to(d)#.cpu()\n",
    "# bcewl_loss = nn.DataParallel(bcewl_loss, device_ids=list(range(pt.cuda.device_count()))).to(d) if dev != \"cpu\" else bcewl_loss\n",
    "# bcewl_loss = nn.parallel.DistributedDataParallel(bcewl_loss, device_ids=list(range(pt.cuda.device_count()))).to(d)\n",
    "# nll_loss = nn.NLLLoss()\n",
    "# kl_loss = nn.KLDivLoss()\n",
    "optimizer = pt.optim.AdamW(model.parameters(), lr=learning_rate, eps=adam_epsilon)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=n_sched_warmup, num_training_steps=N_train_batches)\n",
    "def sequence_mask(lengths, maxlen=None, dtype=pt.int):\n",
    "    if maxlen is None:\n",
    "        maxlen = lengths.max()\n",
    "    row_vector = pt.arange(0, maxlen, 1, device=d)\n",
    "    matrix = pt.unsqueeze(lengths, dim=-1)\n",
    "    mask = row_vector < matrix\n",
    "\n",
    "    mask = mask.type(dtype)\n",
    "    return mask\n",
    "def train_step():\n",
    "    global model, llayer, bcewl_loss, optimizer, bsz, scheduler\n",
    "    x_batch, y_batch, sqlens_batch = next_batch(bsz)\n",
    "\n",
    "    model.zero_grad()\n",
    "    mask = sequence_mask(sqlens_batch, max_len)\n",
    "    outputs = model(x_batch.long(), attention_mask=mask)  # Get logits\n",
    "    logits = outputs[0][[pt.arange(x_batch.shape[0]), sqlens_batch - 1]]\n",
    "\n",
    "#     logsofts = pt.log(softmax(logits))\n",
    "    loss = bcewl_loss(logits, y_batch.float())\n",
    "    loss = loss.mean()\n",
    "    correct = pt.mean((y_batch[pt.arange(batch_size), pt.argmax(logits, axis=1)] > (lid_val + 1e-10)).float())\n",
    "    loss_, correct_ = loss.detach().cpu().numpy(), correct.detach().cpu().numpy()\n",
    "    \n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    return loss_, correct_\n",
    "\n",
    "def inference(x, sqlens, past=None, return_states=False, seq_maxlen=max_len, add=0):\n",
    "    global model, llayer\n",
    "\n",
    "    multitoken = x.shape[1] > 1\n",
    "    mask = sequence_mask(sqlens, seq_maxlen) if (multitoken or add != 0) else None\n",
    "    if add != 0: mask = pt.cat([mask, pt.ones(mask.shape[0], add).to(d)], dim=1)  # Append mask entry for new stream token\n",
    "    outputs = model(x.long(), attention_mask=mask, use_cache=None if not return_states else True, past_key_values=past)\n",
    "    logits = outputs[0][[pt.arange(x.shape[0]), sqlens - 1]] if multitoken else outputs[0].squeeze(1)\n",
    "\n",
    "    return (logits, outputs[1]) if return_states else logits  # Optionally return the past states needed to restore the stream\n",
    "def eval_test(x, y, sqlens):\n",
    "    global bcewl_loss\n",
    "\n",
    "    with pt.no_grad():\n",
    "        logits = inference(x, sqlens)\n",
    "        loss = bcewl_loss(logits, y.float())\n",
    "        loss = loss.mean()\n",
    "        correct = pt.mean((y[pt.arange(x.shape[0]), pt.argmax(logits, axis=1)] > (lid_val + 1e-10)).float())\n",
    "        loss_, correct_ = loss.detach().cpu().numpy(), correct.detach().cpu().numpy()\n",
    "    return loss_, correct_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_i = -1  # Batch 0 is the first iteration, where testing occurs without any training\n",
    "best_acc, best_loss = 0, np.inf\n",
    "best_acc_idx = -1\n",
    "out_str = ''\n",
    "create_folder(\"models\")\n",
    "create_folder(\"model_logs\")\n",
    "create_folder(\"models/\" + model_name)\n",
    "graphs_folder = \"graphs\"\n",
    "create_folder(graphs_folder)\n",
    "train_loss, train_accuracy, test_loss, test_accuracy = [], [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_training(verbose=True):\n",
    "    global model, batch_i, best_acc, best_loss, best_acc_idx, train_loss, train_accuracy, test_loss, test_accuracy\n",
    "    \n",
    "    model.train()\n",
    "    iter_loss, iter_accuracy, b_no_inp = [], [], 0\n",
    "    while batch_i < N_train_batches:\n",
    "        batch_i += 1\n",
    "        if batch_i > 0:\n",
    "            gc.collect()\n",
    "            if dev != \"cpu\": pt.cuda.empty_cache()\n",
    "            b_loss, b_accuracy = train_step()\n",
    "            mname_fn = model_name\n",
    "            if verbose:\n",
    "                sys_print('\\rLoss, accuracy: ' + str(np.mean(b_loss)) + ', ' + str(np.mean(b_accuracy)) + \\\n",
    "                          ' @ batch '+ str(batch_i) + ' (' + str(batch_i * batch_size) + ' samples) complete.                  ')\n",
    "            iter_loss.append(b_loss)\n",
    "            iter_accuracy.append(b_accuracy)\n",
    "\n",
    "        if batch_i % log_period_batches == 0:  # Test on test set\n",
    "            model.eval()\n",
    "            loss, accuracy = [], []\n",
    "            out_str = '\\n'\n",
    "            for i in range(N_test):\n",
    "                test_X, test_Y, test_Sqlens = adapt_form(test_xs[i], test_ys[i], test_sqlens[i], repl_finalcomma=batch_i > 0)\n",
    "                feed_batches = [range(len(test_X))[i * bsz:(i + 1) * bsz] for i in range((len(test_X) // bsz) + \\\n",
    "                                                                                        (1 if (len(test_X) % bsz) != 0 else 0))]\n",
    "                if dev != \"cpu\": pt.cuda.empty_cache()\n",
    "                ls, cs = zip(*[eval_test(test_X[inds], test_Y[inds], test_Sqlens[inds]) for inds in feed_batches])\n",
    "                loss.append(np.mean(ls))\n",
    "                accuracy.append(np.mean(cs))\n",
    "                out_str += test_cats[i] + ': ' + str(loss[-1]) + ', ' + str(accuracy[-1]) + '\\n'\n",
    "            \n",
    "            test_l, test_a = np.mean(loss), np.mean(accuracy)\n",
    "            test_loss.append(test_l)\n",
    "            test_accuracy.append(test_a)\n",
    "            if batch_i == 0:\n",
    "                iter_loss, iter_accuracy = [test_l], [test_a]\n",
    "            train_l, train_a = np.mean(iter_loss), np.mean(iter_accuracy)\n",
    "            train_loss.append(train_l)\n",
    "            train_accuracy.append(train_a)\n",
    "            iter_loss, iter_accuracy = [], []\n",
    "            \n",
    "            val_a = 0\n",
    "            if test_a > best_acc:      # Save best accuracy model\n",
    "                best_acc = test_a\n",
    "                best_loss = test_l\n",
    "                best_acc_idx = batch_i // log_period_batches\n",
    "                pt.save({\"model\": model.state_dict(),\n",
    "#                          \"llayer\": llayer.state_dict(),\n",
    "#                          \"softrmax\": softrmax.state_dict(),\n",
    "                         \"bcewl_loss\": bcewl_loss.state_dict(),\n",
    "#                          \"nll_loss\": nll_loss.state_dict(),\n",
    "#                          \"kl_loss\": kl_loss.state_dict(),\n",
    "                         \"optimizer\": optimizer.state_dict(),\n",
    "                         \"scheduler\": scheduler.state_dict(),\n",
    "                         }, \"./models/\" + model_name + '/' + model_name)\n",
    "                b_no_inp = 0\n",
    "            else:\n",
    "                b_no_inp += log_period_batches\n",
    "                \n",
    "            if verbose:\n",
    "                clear_output()\n",
    "                print(out_str + \"Batch\", batch_i, ':', train_a, test_a, \"loss:\", train_l, test_l, \\\n",
    "                      \"Best:\", best_acc, best_loss, 'idx:', best_acc_idx)\n",
    "                fig = plt.figure()\n",
    "                fig.set_size_inches(16, 5)\n",
    "                g = fig.add_subplot(1,2,1)\n",
    "                g.grid()\n",
    "                g.plot(train_accuracy, label='train acc')\n",
    "                g.plot(test_accuracy, label='test acc')\n",
    "                g.legend(loc='lower right')\n",
    "#                 g.axhline(y=0.714, ls='--', color='grey')\n",
    "\n",
    "                g = fig.add_subplot(1,2,2)\n",
    "                g.grid()\n",
    "                g.plot(train_loss, label='train loss')\n",
    "                plt.yscale(\"log\")\n",
    "                g.plot(test_loss, label='test loss')\n",
    "                plt.yscale(\"log\")\n",
    "                g.legend(loc='upper right')\n",
    "\n",
    "                save_ld((train_accuracy, test_accuracy, train_loss, test_loss),\n",
    "                        \"model_logs/\" + model_name + '_log_latest', pad=False)\n",
    "                plt.savefig(graphs_folder + '/' + model_name + \"_curve_latest\" + '.pdf', format='pdf')\n",
    "                plt.show()\n",
    "\n",
    "            model.train()\n",
    "    return best_acc, best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "chemical elements: 0.00025517723, 0.415\n",
      "dramatic and literature elements: 0.0002438265, 0.53\n",
      "vehicles referred to as crafts: 0.0002460668, 0.46\n",
      "music: 0.00025458867, 0.485\n",
      "scientific cycles: 0.0002582115, 0.465\n",
      "machine learning algorithms: 0.00028182744, 0.385\n",
      "glassware: 0.00025489595, 0.485\n",
      "windings: 0.0002432588, 0.525\n",
      "Batch 230 : 0.3 0.46875 loss: 0.00032879168 0.00025473163 Best: 0.68937504 0.001019297 idx: 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss, accuracy: 0.00027112218, 0.0 @ batch 240 (480 samples) complete.                  "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-5ff61b84ca1e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0miterate_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-20-722920898fdb>\u001b[0m in \u001b[0;36miterate_training\u001b[1;34m(verbose)\u001b[0m\n\u001b[0;32m     24\u001b[0m                 \u001b[0mfeed_batches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mbsz\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mbsz\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mbsz\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m                                                                                         \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbsz\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mdev\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"cpu\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m                 \u001b[0mls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0meval_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minds\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_Y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minds\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_Sqlens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minds\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0minds\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfeed_batches\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m                 \u001b[0maccuracy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-722920898fdb>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     24\u001b[0m                 \u001b[0mfeed_batches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mbsz\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mbsz\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mbsz\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m                                                                                         \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbsz\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mdev\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"cpu\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m                 \u001b[0mls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0meval_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minds\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_Y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minds\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_Sqlens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minds\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0minds\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfeed_batches\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m                 \u001b[0maccuracy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-4093c0378004>\u001b[0m in \u001b[0;36meval_test\u001b[1;34m(x, y, sqlens)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msqlens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbcewl_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-4093c0378004>\u001b[0m in \u001b[0;36minference\u001b[1;34m(x, sqlens, past, return_states, seq_maxlen, add)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msequence_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msqlens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_maxlen\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmultitoken\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0madd\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0madd\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Append mask entry for new stream token\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_cache\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mreturn_states\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpast_key_values\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpast\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msqlens\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mmultitoken\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    960\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    961\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 962\u001b[1;33m             \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    963\u001b[0m         )\n\u001b[0;32m    964\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    799\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoder_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    800\u001b[0m                     \u001b[0muse_cache\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 801\u001b[1;33m                     \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    802\u001b[0m                 )\n\u001b[0;32m    803\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    327\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattn_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m         \u001b[1;31m# residual connection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 329\u001b[1;33m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattn_output\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mresidual\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(gc.collect())\n",
    "iterate_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load best model (checkpoint)\n",
    "pt.cuda.empty_cache()\n",
    "gc.collect()\n",
    "cpoint = pt.load(\"./models/\" + model_name + '/' + model_name)\n",
    "# cpoint = pt.load(\"./models/\" + 'ernst_one - 2k ts' + \"/\" + model_name)\n",
    "# cpoint = pt.load(\"./models/\" + 'ernst_one - 3k ts' + \"/\" + model_name)  # ~3000 training samples observed has current optimum\n",
    "# cpoint = pt.load(\"./models/\" + 'ernst_one - 30k ts' + \"/\" + model_name)\n",
    "model.load_state_dict(cpoint['model'])\n",
    "bcewl_loss.load_state_dict(cpoint['bcewl_loss'])\n",
    "optimizer.load_state_dict(cpoint['optimizer'])\n",
    "scheduler.load_state_dict(cpoint['scheduler'])\n",
    "# llayer.load_state_dict(cpoint['llayer'])\n",
    "mname_fn = model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_top_p_temperature_filtering(logits, tcounts=None, filter_value=-float('Inf'),\n",
    "                                      top_k=0, top_p=0.0, temperature=1.0, frequency_penalty=0.0, presence_penalty=0.0):\n",
    "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
    "        Args:\n",
    "            logits: logits distribution shape (vocabulary size)\n",
    "            top_k >0: keep only top k tokens with highest probability (top-k filtering).\n",
    "            top_p >0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
    "                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
    "    \"\"\"\n",
    "    assert logits.dim() == 1  # batch size 1 for now - could be updated for more but the code would be less clear\n",
    "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
    "    if tcounts is not None: logits -= (tcounts * frequency_penalty) + ((tcounts > 0) * presence_penalty)\n",
    "    logits /= temperature\n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the last token of the top-k\n",
    "        indices_to_remove = logits < pt.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p > 0.0:\n",
    "        sorted_logits, sorted_indices = pt.sort(logits, descending=True)\n",
    "        cumulative_probs = pt.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "        \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "padder = int(pad_token.detach().cpu().numpy())\n",
    "# stop_tokens_e = [tokenizer.encode(t)[0] for t in [\"<|endoftext|>\"]]\n",
    "def gprobs(s, past=None, return_sts=False, tcounts=None, add=0, **kwargs):  # Inference and sampling for tokens\n",
    "    global model\n",
    "    xs, mlen = None, None\n",
    "    if isinstance(s, tuple):    # s either list of token tensors or tuple of preformatted 2d tensors\n",
    "        xs, _, sqlen = s\n",
    "        mlen = max(sqlen)\n",
    "    else:\n",
    "        sqlen = [len(s_) for s_ in s]\n",
    "        mlen = max(sqlen)\n",
    "        xs, _, sqlen = adapt_form([pt.tensor(s_).to(d) for s_ in s], None, sqlen, mlen=mlen)\n",
    "    model.eval()\n",
    "    y_hat = inference(xs, sqlen, seq_maxlen=mlen, add=add, past=past, return_states=return_sts)\n",
    "    if return_sts: y_hat, states = y_hat\n",
    "    y_hat = pt.vstack([F.softmax(top_k_top_p_temperature_filtering(y_hat[i], tcounts[i] if tcounts is not None else None,\n",
    "                                                                   **kwargs), dim=0) for i in range(len(xs))])\n",
    "    return (y_hat, states) if return_sts else y_hat\n",
    "def append_next_token(sent, olen=None, top_k=-1, top_p=0.9, temperature=1.0):  # Interface for field testing\n",
    "    print(\"k =\", top_k, \", p =\", top_p, \", temp =\", temperature)\n",
    "    tokens = tokenizer.encode(sent)\n",
    "    ou = tokens.copy()\n",
    "    tcounts = pt.zeros(N_tokens, dtype=int).to(d)\n",
    "    for token in tokens: tcounts[token] += 1\n",
    "    probs = gprobs([tokens], top_k=top_k, top_p=top_p, temperature=temperature, tcounts=[tcounts])[0]\n",
    "    token = pt.multinomial(probs, 1).detach().cpu().numpy()[0]\n",
    "    ou += [token]\n",
    "    prev_len = len(sent) if olen is None else olen\n",
    "    sent_new = tokenizer.decode(ou)\n",
    "    print(sent[:prev_len] + 'âž¡' + sent_new[prev_len:])\n",
    "    return sent_new\n",
    "def gen_probs(s, **kwargs):  # Adapter for strings\n",
    "    inp = [tokenizer.encode(s_) for s_ in s]\n",
    "    return gprobs(inp, **kwargs)\n",
    "# bszinf = 256\n",
    "bszinf = bsz * 16\n",
    "def shift_s(st, sql):\n",
    "    r = pt.ones(st.shape).to(d)\n",
    "    for k in range(r.shape[0]):\n",
    "        r[k, :, -sql[k]:] = st[k, :, :sql[k]]\n",
    "    return r\n",
    "def gen_completions(s, n=1, max_tokens=8, best_of=1, **kwargs):  # Completion generator equivalent to OpenAI's for GPT3\n",
    "    n_bats, best_of = int(np.ceil(len(s) / (bsz // 2))), int(round(best_of))\n",
    "    if n == 1 and best_of != 1: n, best_of = best_of, n\n",
    "    if best_of == 1 and n != 1: best_of = n\n",
    "    gc.collect()\n",
    "    outputs = []\n",
    "    for i in range(n_bats):\n",
    "        s_batch = s[i * (bsz // 2):(i + 1) * (bsz // 2)]\n",
    "        tc_b = []\n",
    "        for s_ in s_batch:\n",
    "            tc = pt.zeros(N_tokens, dtype=int).to(d)\n",
    "            for t in tokenizer.encode(s_): tc[t] += 1\n",
    "            tc_b.append(tc)\n",
    "        p, sts = gen_probs(s_batch, return_sts=True, tcounts=tc_b, **kwargs)\n",
    "        sql_b = [len(tokenizer.encode(s_)) for s_ in s_batch]\n",
    "        mlen = max(sql_b)\n",
    "        sql_b = pt.tensor(sql_b).to(d)\n",
    "        tokens = pt.multinomial(p, n, replacement=True) \n",
    "        outs, avg_logprobs = [], [] # first use the (as yet undiverged) token distribution to generate n tokens for each sample\n",
    "        for j in range(n):\n",
    "            tks = tokens[:, j]\n",
    "            gc.collect()\n",
    "            pt.cuda.empty_cache()\n",
    "            tc_b_itr = [t.clone() for t in tc_b]\n",
    "            for j in range(len(tc_b_itr)): tc_b_itr[j][tks[j]] += 1\n",
    "            p, st = gprobs((pt.unsqueeze(tks, -1), None, sql_b), past=sts, return_sts=True, tcounts=tc_b_itr, add=1, **kwargs)\n",
    "            su, ls = pt.log(p[pt.arange(p.shape[0]), tks]), pt.ones(p.shape[0]).to(d)\n",
    "            out = [tks]\n",
    "            for token_i in range(max_tokens - 1):\n",
    "                t = pt.multinomial(p, 1).to(d)[:, 0]\n",
    "                out.append(t)\n",
    "                for j in range(len(tc_b_itr)): tc_b_itr[j][t[j]] += 1\n",
    "                cont = t != pad_token\n",
    "                ls += cont.int()\n",
    "                su += cont * pt.log(p[pt.arange(p.shape[0]), t])\n",
    "                if token_i == max_tokens - 1: break\n",
    "                p,st=gprobs((pt.unsqueeze(t,-1),None,sql_b),past=st,return_sts=True,tcounts=tc_b_itr,add=token_i+2,**kwargs)\n",
    "            outs.append(pt.vstack(out).T)\n",
    "            avg_logprobs.append(su / ls)\n",
    "        gc.collect()\n",
    "        pt.cuda.empty_cache()\n",
    "        outs = pt.stack(outs, 1)\n",
    "        avg_logprobs = pt.vstack(avg_logprobs).T\n",
    "        s1 = outs.shape[0]\n",
    "        idx = pt.argsort(avg_logprobs, axis=1)[:, :best_of].repeat_interleave(max_tokens, 1).reshape(s1, best_of, max_tokens)\n",
    "        outs = pt.gather(outs, 1, idx)\n",
    "        outputs += [[[(tokenizer.decode([x_]),) for x_ in x] for x in o] for o in outs.cpu().detach().numpy()]\n",
    "#     pr([[''.join([x_[0] for x_ in x]) for x in o] for o in outputs])\n",
    "    return outputs\n",
    "mdl = {\"completions\": gen_completions, \"probabilities\": gprobs, \"name\": mname_fn + ',' + gpt2_modelkey, \"mstr\": str(model)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | freque... | presen... | temper... |   top_p   |\n",
      "-------------------------------------------------------------------------\n",
      "('Test accuracy:', 0.0125)\n",
      "('Test accuracy:', 0.0)\n",
      "('Test accuracy:', 0.0125)\n",
      "|  1        |  0.0      |  1.911    |  0.7123   |  0.5203   |  0.9362   |\n",
      "('Test accuracy:', 0.0)\n",
      "('Test accuracy:', 0.0)\n",
      "('Test accuracy:', 0.0)\n",
      "|  2        |  0.0      |  1.551    |  0.9312   |  0.5706   |  0.3043   |\n",
      "('Test accuracy:', 0.0)\n",
      "('Test accuracy:', 0.0)\n",
      "('Test accuracy:', 0.0)\n",
      "|  3        |  0.001333 |  1.828    |  1.601    |  0.8436   |  0.9063   |\n",
      "('Test accuracy:', 0.0)\n",
      "('Test accuracy:', 0.0125)\n",
      "('Test accuracy:', 0.0)\n",
      "|  4        |  0.002667 |  1.504    |  0.04204  |  0.9047   |  0.8733   |\n",
      "('Test accuracy:', 0.2880907388479067)\n",
      "('Test accuracy:', 0.20542283803153366)\n",
      "('Test accuracy:', 0.20992411387631976)\n",
      "|  5        |  0.1772   |  0.1451   |  0.5229   |  0.8151   |  0.4791   |\n",
      "('Test accuracy:', 0.0125)\n",
      "('Test accuracy:', 0.0)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\bayes_opt\\target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[1;34m(self, params)\u001b[0m\n\u001b[0;32m    190\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 191\u001b[1;33m             \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0m_hashable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    192\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: (1.046839752240178, 1.9836875783091816, 0.8438445112992012, 0.6157739222087157)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-b64411426d79>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtest_sp_conv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_l\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_l\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_l\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_l\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.005\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphase\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"train\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muniform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmdl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmdl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0moptimizers_gpt2xl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBayesianOptimization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfun\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpbounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbounds_gptxl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0moptimizers_gpt2xl\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_points\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[0mresults_gpt2xl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizers_gpt2xl\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\bayes_opt\\bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[1;34m(self, init_points, n_iter, acq, kappa, xi, **gp_params)\u001b[0m\n\u001b[0;32m    172\u001b[0m                 \u001b[0miteration\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_probe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlazy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOPTMIZATION_END\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\bayes_opt\\bayesian_optimization.py\u001b[0m in \u001b[0;36mprobe\u001b[1;34m(self, params, lazy)\u001b[0m\n\u001b[0;32m    110\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOPTMIZATION_STEP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\bayes_opt\\target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[1;34m(self, params)\u001b[0m\n\u001b[0;32m    192\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m             \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 194\u001b[1;33m             \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    195\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-24-b64411426d79>\u001b[0m in \u001b[0;36mfun\u001b[1;34m(temperature, top_p, presence_penalty, frequency_penalty)\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlocals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m#         ps[\"best_of\"] = 5.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtest_sp_conv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_l\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_l\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_l\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_l\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.005\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphase\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"train\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muniform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmdl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmdl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[0moptimizers_gpt2xl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBayesianOptimization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfun\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpbounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbounds_gptxl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0moptimizers_gpt2xl\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_points\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-3bcd4d942ff1>\u001b[0m in \u001b[0;36mtest_sp_conv\u001b[1;34m(params, tol, **kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[0mcenter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[0msamples\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_sp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m         \u001b[0mnew_center\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcenter\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mnew_center\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mtol\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0mnew_center\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-3bcd4d942ff1>\u001b[0m in \u001b[0;36mtest_sp\u001b[1;34m(params, min_l, max_l, n1, n2, prmt, phase, uniform, mdl, max_tokens)\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'n'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"max_tokens\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_tokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmdl\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'gpt3'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mp_req_m\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# Convert to strings and request predictions from OpenAI\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m             \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmdl\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"completions\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0md_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[0mcreate_folder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlearning_data_dir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"msp_samples_nb/\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-df662fd3fb69>\u001b[0m in \u001b[0;36mgen_completions\u001b[1;34m(s, n, max_tokens, best_of, **kwargs)\u001b[0m\n\u001b[0;32m     76\u001b[0m                 \u001b[0msu\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mcont\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mtoken_i\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mmax_tokens\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m                 \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mst\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgprobs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msql_b\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpast\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mst\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreturn_sts\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtcounts\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtc_b_itr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtoken_i\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m             \u001b[0mouts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[0mavg_logprobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msu\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-df662fd3fb69>\u001b[0m in \u001b[0;36mgprobs\u001b[1;34m(s, past, return_sts, tcounts, add, **kwargs)\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mxs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msqlen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapt_form\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msqlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0my_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msqlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_maxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpast\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpast\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_sts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreturn_sts\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my_hat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_hat\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     y_hat = pt.vstack([F.softmax(top_k_top_p_temperature_filtering(y_hat[i], tcounts[i] if tcounts is not None else None,\n",
      "\u001b[1;32m<ipython-input-18-4093c0378004>\u001b[0m in \u001b[0;36minference\u001b[1;34m(x, sqlens, past, return_states, seq_maxlen, add)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msequence_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msqlens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_maxlen\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmultitoken\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0madd\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0madd\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Append mask entry for new stream token\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_cache\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mreturn_states\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpast_key_values\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpast\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msqlens\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mmultitoken\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    960\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    961\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 962\u001b[1;33m             \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    963\u001b[0m         )\n\u001b[0;32m    964\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    799\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoder_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    800\u001b[0m                     \u001b[0muse_cache\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 801\u001b[1;33m                     \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    802\u001b[0m                 )\n\u001b[0;32m    803\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    322\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m             \u001b[0muse_cache\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m         )\n\u001b[0;32m    326\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattn_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# output_attn: a, present, (attentions)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    241\u001b[0m             \u001b[0mattention_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder_attention_mask\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 243\u001b[1;33m             \u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_attn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    244\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m         \u001b[0mquery\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_split_heads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36msplit\u001b[1;34m(self, split_size, dim)\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplit_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplit_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplit_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplit_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# evaluate fine-tuned model\n",
    "lgroups_ft = [[4, 5]]\n",
    "bounds_gptxl = {\n",
    "  \"temperature\": [0.01, 1.0],  # a temp of 0 selects the top completion every time due to the infinitely larger logit\n",
    "  \"top_p\": [0.01, 1.0],        # same with this but more obvious\n",
    "#   \"top_k\": [1, 100],        # fix this to be a log of the input value so that lower values are sampled with high resolution?\n",
    "  \"presence_penalty\": [0.01, 2.0],   # both presence and frequency penalty have optimal values\n",
    "  \"frequency_penalty\": [0.01, 2.0],  # this can also go down to -2.0, probably not useful for our case...\n",
    "#   \"best_of\": [0.51, 5.49], # to do\n",
    "}\n",
    "optimizers_gpt2xl, results_gpt2xl = [], []\n",
    "for lgroup in lgroups_ft:\n",
    "    min_l, max_l = min(lgroup) - 1, max(lgroup)\n",
    "    def fun(temperature, top_p, presence_penalty, frequency_penalty):\n",
    "        global min_l, max_l\n",
    "        ps = locals()\n",
    "        ps[\"best_of\"] = 5.0\n",
    "        return test_sp_conv(ps, min_l=min_l, max_l=max_l, tol=0.005, phase=\"train\", uniform=True, mdl=mdl, max_tokens=8)[0]\n",
    "    optimizers_gpt2xl.append(BayesianOptimization(f=fun, pbounds=bounds_gptxl, verbose=1000))\n",
    "    optimizers_gpt2xl[-1].maximize(init_points=8, n_iter=10)\n",
    "    results_gpt2xl.append(optimizers_gpt2xl[-1].max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | freque... | presen... | temper... |   top_p   |\n",
      "-------------------------------------------------------------------------\n",
      "[0.0, 0.0, 0.0]\n",
      "[0.0, 0.008, 0.016]\n",
      "('Test acc:', 0.0, 'sd:', 0.0)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'samples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\bayes_opt\\target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[1;34m(self, params)\u001b[0m\n\u001b[0;32m    190\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 191\u001b[1;33m             \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0m_hashable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    192\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: (1.5493618294567606, 0.6856368231600667, 0.8140626073016369, 0.334907673903822)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-565f3789037d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtest_sp_conv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_l\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_l\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_l\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_l\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.005\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphase\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"train\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muniform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmdl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmdl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0moptimizers_gpt2xl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBayesianOptimization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfun\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpbounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbounds_gptxl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0moptimizers_gpt2xl\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_points\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[0mresults_gpt2xl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizers_gpt2xl\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\bayes_opt\\bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[1;34m(self, init_points, n_iter, acq, kappa, xi, **gp_params)\u001b[0m\n\u001b[0;32m    172\u001b[0m                 \u001b[0miteration\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_probe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlazy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOPTMIZATION_END\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\bayes_opt\\bayesian_optimization.py\u001b[0m in \u001b[0;36mprobe\u001b[1;34m(self, params, lazy)\u001b[0m\n\u001b[0;32m    110\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOPTMIZATION_STEP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\bayes_opt\\target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[1;34m(self, params)\u001b[0m\n\u001b[0;32m    192\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m             \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 194\u001b[1;33m             \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    195\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-24-565f3789037d>\u001b[0m in \u001b[0;36mfun\u001b[1;34m(temperature, top_p, presence_penalty, frequency_penalty)\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlocals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"best_of\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtest_sp_conv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_l\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_l\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_l\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_l\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.005\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphase\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"train\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muniform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmdl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmdl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[0moptimizers_gpt2xl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBayesianOptimization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfun\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpbounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbounds_gptxl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0moptimizers_gpt2xl\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_points\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-34778eb31bdc>\u001b[0m in \u001b[0;36mtest_sp_conv\u001b[1;34m(params, tol, **kwargs)\u001b[0m\n\u001b[0;32m     99\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mys\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m             \u001b[0msys_print\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Test acc:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mys\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"sd:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mys\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnew_center\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m         \u001b[0mcenter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_center\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[0mi\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'samples' is not defined"
     ]
    }
   ],
   "source": [
    "# evaluate fine-tuned model\n",
    "lgroups_ft = [[4, 5]]\n",
    "bounds_gptxl = {\n",
    "  \"temperature\": [0.01, 1.0],  # a temp of 0 selects the top completion every time due to the infinitely larger logit\n",
    "  \"top_p\": [0.01, 1.0],        # same with this but more obvious\n",
    "#   \"top_k\": [1, 100],        # fix this to be a log of the input value so that lower values are sampled with high resolution?\n",
    "  \"presence_penalty\": [0.01, 2.0],   # both presence and frequency penalty have optimal values\n",
    "  \"frequency_penalty\": [0.01, 2.0],  # this can also go down to -2.0, probably not useful for our case...\n",
    "#   \"best_of\": [0.51, 5.49], # to do\n",
    "}\n",
    "optimizers_gpt2xl, results_gpt2xl = [], []\n",
    "for lgroup in lgroups_ft:\n",
    "    min_l, max_l = min(lgroup) - 1, max(lgroup)\n",
    "    def fun(temperature, top_p, presence_penalty, frequency_penalty):\n",
    "        global min_l, max_l\n",
    "        ps = locals()\n",
    "        ps[\"best_of\"] = 5.0\n",
    "        return test_sp_conv(ps, min_l=min_l, max_l=max_l, tol=0.005, phase=\"train\", uniform=True, mdl=mdl, max_tokens=8)[0]\n",
    "    optimizers_gpt2xl.append(BayesianOptimization(f=fun, pbounds=bounds_gptxl, verbose=1000))\n",
    "    optimizers_gpt2xl[-1].maximize(init_points=8, n_iter=10)\n",
    "    results_gpt2xl.append(optimizers_gpt2xl[-1].max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # evaluate un-fine-tuned model\n",
    "# lgroups_ft = [[4, 5]]\n",
    "# bounds_gptsm = {\n",
    "#   \"temperature\": [0.01, 1.0],  # a temp of 0 selects the top completion every time due to the infinitely larger logit\n",
    "#   \"top_p\": [0.01, 1.0],        # same with this but more obvious\n",
    "# #   \"top_k\": [1, 100],        # fix this to be a log of the input value so that lower values are sampled with high resolution?\n",
    "#   \"presence_penalty\": [0.01, 2.0],   # both presence and frequency penalty have optimal values\n",
    "#   \"frequency_penalty\": [0.01, 2.0],  # this can also go down to -2.0, probably not useful for our case...\n",
    "# #   \"best_of\": [0.51, 5.49], # to do\n",
    "# }\n",
    "# optimizers_gpt2sm, results_gpt2sm = [], []\n",
    "# for lgroup in lgroups_ft:\n",
    "#     min_l, max_l = min(lgroup) - 1, max(lgroup)\n",
    "#     def fun(temperature, top_p, presence_penalty, frequency_penalty):\n",
    "#         global min_l, max_l\n",
    "#         ps = locals()\n",
    "#         ps[\"best_of\"] = 5.0\n",
    "#         return test_sp_conv(ps, min_l=min_l, max_l=max_l, tol=0.005, phase=\"train\", uniform=True, mdl=mdl, max_tokens=8)[0]\n",
    "#     optimizers_gpt2sm.append(BayesianOptimization(f=fun, pbounds=bounds_gptsm, verbose=1000))\n",
    "#     optimizers_gpt2sm[-1].maximize(init_points=8, n_iter=10)\n",
    "#     results_gpt2sm.append(optimizers_gpt2sm[-1].max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test accuracy for the top performing (training accuracy 0.2026) sampling parameters for gpt2-small\n",
    "# np.mean([0.3353320494864612,0.3277777777777778,0.21805555555555556,0.3402514152514152,0.28348214285714285,0.13194444444444445])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With about half an hour of fine-tuning using 7 training samples on an NVidia 980 Ti (showing improvements):\n",
    "# Initial input words on first line, those eventually found by repeatedly querying model + recursively adding words on 2nd & 3rd\n",
    "input_sentence = (\"pace, silence, diversion, attention, plot twist, cliffhanger, surprise, fate, fourth wall, monologue\" + \\\n",
    "\"reinterpretation, harmony, character progression, reading circle\")\n",
    "# \"monolith, elevator effect, time loop, survival, desert resort, town watchman\")\n",
    "# \"monolith, allegro, soundtrack, chord, classical, opera\")\n",
    "input_sentence = input_sentence.split(', ')\n",
    "np.random.shuffle(input_sentence)\n",
    "input_sentence = \"A list of types of element of drama and writing: \"+', '.join(input_sentence[:10])+', '\n",
    "olen = len(input_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_sentence = append_next_token(input_sentence, top_k=25, top_p=0.6, temperature=15.0, olen=olen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # With about half an hour of fine-tuning using 7 training samples on an NVidia 980 Ti (showing improvements):\n",
    "# # Initial input words on first line, those eventually found by repeatedly querying model + recursively adding words on 2nd & 3rd\n",
    "# input_sentence = (\"pace, silence, diversion, attention, plot twist, cliffhanger, surprise, fate, fourth wall, monologue\" + \\\n",
    "# \"reinterpretation, harmony, character progression, reading circle\")\n",
    "# # \"monolith, elevator effect, time loop, survival, desert resort, town watchman\")\n",
    "# # \"monolith, allegro, soundtrack, chord, classical, opera\")\n",
    "# input_sentence = input_sentence.split(', ')\n",
    "# np.random.shuffle(input_sentence)\n",
    "# input_sentence = \"A list of types of element of drama and writing: \"+', '.join(input_sentence[:10])+', '\n",
    "# olen = len(input_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# input_sentence = append_next_token(input_sentence, top_k=25, top_p=0.6, temperature=15.0, olen=olen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # With about half an hour of fine-tuning using 7 training samples on an NVidia 980 Ti (showing improvements):\n",
    "# # Initial input words on first line, those eventually found by repeatedly querying model + recursively adding words on 2nd & 3rd\n",
    "# input_sentence = (\"coffee, water, tea, coke, lemonade, milkshake, \" + \\\n",
    "# \"watermelon juice, cherry juice, blackcurrant mixture, kava, orangeade, lemon juice, cherryade, cranberry juice\")\n",
    "# input_sentence = input_sentence.split(', ')\n",
    "# np.random.shuffle(input_sentence)\n",
    "# input_sentence = \"A list of types of element of drama and writing: \"+', '.join(input_sentence[:10])+', '\n",
    "# olen = len(input_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# input_sentence = append_next_token(input_sentence, top_k=10, top_p=0.8, temperature=5.0, olen=olen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # With about half an hour of fine-tuning using 7 training samples on an NVidia 980 Ti (showing improvements):\n",
    "# # Initial input words on first line, those eventually found by repeatedly querying model + recursively adding words on 2nd & 3rd\n",
    "# input_sentence = (\"pace, silence, diversion, attention, plot twist, cliffhanger, surprise, fate, fourth wall, monologue\" + \\\n",
    "# \"\")\n",
    "# input_sentence = input_sentence.split(', ')\n",
    "# np.random.shuffle(input_sentence)\n",
    "# input_sentence = \"A list of types of element of drama and writing: \"+', '.join(input_sentence[:10])+', ' # Randomly generate shuffled drinks sublist\n",
    "# olen = len(input_sentence)\n",
    "# input_sentence = append_next_token(input_sentence, top_k=5, top_p=0.9, temperature=5.0, olen=olen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With about half an hour of fine-tuning using 7 training samples on an NVidia 980 Ti (showing improvements):\n",
    "# Initial input words on first line, those eventually found by repeatedly querying model + recursively adding words on 2nd & 3rd\n",
    "# input_sentence = (\"coffee, water, tea, coke, lemonade, milkshake, \" + \\\n",
    "# \"milk, soda\")\n",
    "# input_sentence = input_sentence.split(', ')\n",
    "# np.random.shuffle(input_sentence)\n",
    "# input_sentence = \"A list of types of drink: \"+', '.join(input_sentence[:10])+', ' # Randomly generate shuffled drinks sublist\n",
    "# olen = len(input_sentence)\n",
    "# input_sentence = append_next_token(input_sentence, top_k=25, top_p=0.7, temperature=2.0, olen=olen)\n",
    "# input_sentence = append_next_token(input_sentence, top_k=5, top_p=0.9, temperature=5.0, olen=olen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # With about half an hour of fine-tuning using 7 training samples on an NVidia 980 Ti (showing improvements):\n",
    "# # Initial input words on first line, those eventually found by repeatedly querying model + recursively adding words on 2nd & 3rd\n",
    "# input_sentence = (\"coffee, water, tea, coke, lemonade, milkshake\" + \\\n",
    "# \", liquor, wine, juice, beer, milk, soft drink, whiskey, vodka, spirits, soda, ice water, ice cold beer, cider, yoghurt, soda pop, rum, chocolate milk, hot cocoa, alcohol\")\n",
    "# input_sentence = input_sentence.split(', ')\n",
    "# np.random.shuffle(input_sentence)\n",
    "# input_sentence = \"A list of types of drink: \"+', '.join(input_sentence[:10])+', ' # Randomly generate shuffled drinks sublist\n",
    "# olen = len(input_sentence)\n",
    "# input_sentence = append_next_token(input_sentence, top_k=25, top_p=0.7, temperature=2.0, olen=olen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # With about half an hour of fine-tuning using 7 training samples on an NVidia 980 Ti (showing improvements):\n",
    "# # Initial input words on first line, those eventually found by repeatedly querying model + recursively adding words on 2nd & 3rd\n",
    "# input_sentence = (\"coffee, water, tea, coke, lemonade, milkshake\" + \\\n",
    "# \", liquor, wine, juice, beer, milk, soft drink, whiskey, vodka, spirits, soda, ice water, ice cold beer, cider\" + \\\n",
    "# \", yoghurt, soda pop, rum, chocolate milk, hot cocoa, alcohol\")\n",
    "# input_sentence = input_sentence.split(', ')\n",
    "# np.random.shuffle(input_sentence)\n",
    "# input_sentence = \"A list of types of drink: \"+', '.join(input_sentence[:10])+', ' # Randomly generate shuffled drinks sublist\n",
    "# olen = len(input_sentence)\n",
    "# input_sentence = append_next_token(input_sentence, top_k=25, top_p=0.7, temperature=2.0, olen=olen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changelog 16/04/2021 5pm: Using log_period=1 (max_len=96) reliably finds improvement, need more data and larger gpt2 (medium+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With about half an hour of fine-tuning using 7 training samples on an NVidia 980 Ti (showing improvements):\n",
    "# Initial input words on first line, those eventually found by repeatedly querying model + recursively adding words on 2nd & 3rd\n",
    "# input_sentence = (\"coffee, water, tea, coke, lemonade, milkshake,\" + \\\n",
    "# \" milk, vodka, beer, ice water, soda, lassi, juice, alcohol, whiskey\")\n",
    "# input_sentence = input_sentence.split(', ')\n",
    "# np.random.shuffle(input_sentence)\n",
    "# input_sentence = \"A list of types of drink: \"+', '.join(input_sentence[:10])+', ' # Randomly generate shuffled drinks sublist\n",
    "# olen = len(input_sentence)\n",
    "# input_sentence = append_next_token(input_sentence, top_k=25, top_p=0.9, temperature=2.0, olen=olen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With about half an hour of fine-tuning using 7 training samples on an NVidia 980 Ti (showing improvements):\n",
    "# Initial input words on first line, those eventually found by repeatedly querying model + recursively adding words on 2nd & 3rd\n",
    "# input_sentence = (\"coffee, water, tea, coke, lemonade, milkshake,\" + \\\n",
    "# \" wine, soda, alcohol, beer, liquor, apple cider, whiskey, milk, bourbon, vodka, cider, lemon juice\")\n",
    "# input_sentence = input_sentence.split(', ')\n",
    "# np.random.shuffle(input_sentence)\n",
    "# input_sentence = \"A list of types of drink: \"+', '.join(input_sentence[:6])+', ' # Randomly generate shuffled drinks sublist\n",
    "# olen = len(input_sentence)\n",
    "# input_sentence = append_next_token(input_sentence, top_k=30, top_p=0.7, temperature=3.0, olen=olen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With about half an hour of fine-tuning using 7 training samples on an NVidia 980 Ti (showing improvements):\n",
    "# Initial input words on first line, those eventually found by repeatedly querying model + recursively adding words on 2nd & 3rd\n",
    "# input_sentence = (\"coffee, water, tea, coke, lemonade, milkshake,\" + \\\n",
    "# \" beer, milk, juice, wine, spirits, alcohol, soda, whiskey, brandy, apple juice, liquor, ice tea, watermelon juice, vodka,\" + \\\n",
    "# \" lemon tea, apple cider, ale, lager, fruit tea, lime cider, cocktail, mocha, red wine, apple soda\")\n",
    "# input_sentence = input_sentence.split(', ')\n",
    "# np.random.shuffle(input_sentence)\n",
    "# input_sentence = \"A list of types of drink: \"+', '.join(input_sentence[:10])+', ' # Randomly generate shuffled drinks sublist\n",
    "# olen = len(input_sentence)\n",
    "# input_sentence = append_next_token(input_sentence, top_k=30, top_p=0.7, temperature=2.0, olen=olen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changelog 16/04/2021 5am: lr=1e-5, max_len=80, log_period_batches=5 increased accuracy by 8%. Need to add a bunch more data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With about half an hour of fine-tuning using 5 training samples on an NVidia 980 Ti (showing improvements):\n",
    "# Initial input words on first line, those eventually found by repeatedly querying model + recursively adding words on 2nd & 3rd\n",
    "# input_sentence = (\"coffee, water, tea, coke, lemonade, milkshake, \" + \\\n",
    "# \"soda, milk, juice, wine, vodka, gin, lime juice, beer, hot chocolate, cider, whiskey, fruit juice, cocktail, liquor, \" + \\\n",
    "# \"spirits, watermelon juice, martini, rum, chocolate milk, orangeade\")\n",
    "# input_sentence = input_sentence.split(', ')\n",
    "# np.random.shuffle(input_sentence)\n",
    "# input_sentence = \"A list of types of drink: \"+', '.join(input_sentence[:10])+', ' # Randomly generate shuffled drinks sublist\n",
    "# olen = len(input_sentence)\n",
    "# input_sentence = append_next_token(input_sentence, top_k=25, top_p=0.9, temperature=1.89, olen=olen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without fine tuning (regular GPT-2):\n",
    "# Initial input words on first line, those eventually found by repeatedly querying model on 2nd\n",
    "# input_sentence = (\"coffee, water, tea, coke, lemonade, milkshake, \" + \\\n",
    "# \"milk, juice, fruit juice, soda, wine, beer, hot chocolate, chocolate milk, alcohol, cider, ice tea, liquor, spirit\")\n",
    "# input_sentence.split(', ')\n",
    "# np.random.shuffle(input_sentence)\n",
    "# input_sentence = \"A list of types of drink: \"+', '.join(input_sentence[:10])+', ' # Randomly generate shuffled drinks sublist\n",
    "# olen = len(input_sentence)\n",
    "# input_sentence = append_next_token(input_sentence, top_k=25, top_p=0.75, temperature=1.89, olen=olen) # k = 25 also used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training for too long (in this case ~6:30 hours) overfits\n",
    "# input_sentence = (\"coffee, water, tea, coke, lemonade, milkshake, \" + \\\n",
    "# \"beer, juice, soda, liquor, wine, tequila, spirits, alcohol, cocktail, martini, whiskey, rum, vodka\")\n",
    "# input_sentence = input_sentence.split(', ')\n",
    "# np.random.shuffle(input_sentence)\n",
    "# input_sentence = \"A list of types of drink: \"+', '.join(input_sentence[:10])+', ' # Randomly generate shuffled drinks sublist\n",
    "# olen = len(input_sentence)\n",
    "# input_sentence = append_next_token(input_sentence, top_k=25, top_p=0.9, temperature=1.89, olen=olen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changelog 15/04/2021 11am: Found learning rate 1e-7, max_listlen 15, min_nw 0.7, max_nw 0.9, lidstone_e 0.01 ACTUALLY WORKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No fine tuning (testing gpt2) (old append function):\n",
    "# input_sentence = \"A list of types of drink: coffee, water, tea, coke, lemonade, milkshake\"\n",
    "# input_sentence = append_next_token(input_sentence, top_k=25, top_p=0.75, temperature=1.89)\n",
    "# A list of types of drink: juice, tea, cider, lemonade, milk, beer, hot chocolate,âž¡ ice cold milk. Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working (reproducible) examples using various non-fine-tuned models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT3 (via AI Dungeon) (Randomness = 2.0, model = Dragon):\n",
    "# sentence = \"A list of ML algorithms: inverse reinforcement learning, ELMo, decision tree, LDA, \"\n",
    "# expected_completion = \"MLP, MLL, MMM. You can't believe you're actually using these things!\"\n",
    "\n",
    "# sentence = \"A list of animals seen in the wild: wolffish, woodlouse, sheep, zebra, yak, \"\n",
    "# expected_completion = \"goat, fox, dog, rat. You're guessing that a lot of other animals have been seen as well; maybe even all the animals on your list except for wolf and rat?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT2 (via Write with Transformer) (Top-p = 0.67, temperature = 1.89, max time = 1.9):\n",
    "# sentence = \"A list of round fruits: peach, apricot, lime, plum, blackberry, cantaloupe, nectarine, pitaya, persimmon, \"\n",
    "# expected_completion = \"mango, papaya and raspberry, as also many\"\n",
    "\n",
    "# sentence = \"A list of chemical elements: hydrogen, carbon, oxygen, nitrogen, gold, \"\n",
    "# expected_completion = \"silver, aluminum, potassium and phosphorus; atomic number.\"\n",
    "\n",
    "# sentence = \"A list of microbes found on earth: bacteria, virus, prokaryote, amoeba, \"\n",
    "# expected_completion = \"archaea, algae, nematode, euk\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This result shows why we need not redistribute the mass when evaluating gpt3 accuracy\n",
    "# response = openai.Completion.create(**{**default_params,\n",
    "#   \"prompt\": \"A list of cyclic phenomena: day and night, induction coil, self-oscillation, tornado, runaway greenhouse effect,\",\n",
    "#   \"temperature\": 1.5,\n",
    "#   \"top_p\": 1.0,\n",
    "#   \"n\": 5,\n",
    "#   \"best_of\": 20,\n",
    "#   \"max_tokens\": 7,\n",
    "#   \"stop\": [\",\", \"\\n\"],\n",
    "# })\n",
    "# for choice in response[\"choices\"]:\n",
    "#     d = {}\n",
    "#     tokens = choice[\"logprobs\"][\"tokens\"]\n",
    "#     t_i = -1\n",
    "#     for t in tokens:\n",
    "#         t_i += 1\n",
    "#         r = [(np.e**v, k) for (k, v) in choice[\"logprobs\"][\"top_logprobs\"][t_i].items()]\n",
    "#         r.sort(reverse=True)\n",
    "#         print(sum([v for (v, k) in r]))\n",
    "#         rd = dict([(k, v) for (v, k) in r])\n",
    "#         r = [k for (v, k) in r]\n",
    "#         d[t] = (rd, r, np.e**choice[\"logprobs\"][\"token_logprobs\"][t_i])\n",
    "#     print('|'.join([' '.join((s.replace(\"\\n\", \"âŽ\"),'%.2f' % (d[s][2] * 100),\n",
    "#                               str(d[s][1].index(s) + 1) if s in d[s][1] else \"<100\")) for s in tokens]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
