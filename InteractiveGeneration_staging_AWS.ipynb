{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A fine-tuned language model for pictionary word list completion (topic/category phrase + example words -> list of 30 examples). For example, one may complete the list\n",
    "\n",
    "\"A list of round fruits: peach, apricot, lime, plum,\"\n",
    "\n",
    "with\n",
    "\n",
    "\"mango, cherry, pineapple, strawberry, pumpkin, watermelon, orange, pomegranate, melon, apple, pear, grapefruit, papaya, lemon, kiwi, passionfruit, blueberry, raspberry, blackberry, cantaloupe, nectarine, pitaya, persimmon, durian, guava, jackfruit, avocado, lychee, soursop, guarana, mangosteen, blackcurrant, cranberry\"\n",
    "\n",
    "using this model. These words should be compatible with pictionary/skribbl.io; i.e., \"sketchable\" within a few minutes, and easily recognisable. Scroll to the end for more examples, or see the file `data/examples.txt`. Sketchability parameter estimation examples can also be found in `data/data.tsv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import openai\n",
    "# with open('../../openai-api-org.txt', 'r') as f: openai.organization = f.read()\n",
    "# with open('../../openai-api-key.txt', 'r') as f: openai.api_key      = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1019 02:36:06.077298 56772 modeling_bert.py:226] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
      "I1019 02:36:06.089267 56772 modeling_xlnet.py:339] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\alfew\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Complete.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import time\n",
    "import glob\n",
    "import string\n",
    "import jsonlines\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as pt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "import matplotlib.pyplot as plt\n",
    "from bayes_opt import BayesianOptimization\n",
    "from IPython.utils import io\n",
    "from IPython.display import clear_output\n",
    "from transformers import GPT2ForSequenceClassification, GPT2LMHeadModel, GPTNeoForCausalLM, ReformerModelWithLMHead, \\\n",
    "                         get_linear_schedule_with_warmup, GPT2TokenizerFast, AutoTokenizer, XLNetLMHeadModel\n",
    "from pytorch_transformers import GPT2Tokenizer\n",
    "from Learning import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                  ###   Options   ###\n",
    "model_name = \"ernst_one\"\n",
    "# model_name = \"unfinetuned\"\n",
    "modelkey = \"xlnet-base-cased\"                   # Pretrained model to start from\n",
    "# modelkey = \"gpt2-xl\"\n",
    "modelclass = \"XLNetLMHeadModel\"\n",
    "# modelclass = \"GPT2LMHeadModel\"\n",
    "val_frac, test_frac = 0.25, 0.25    # Fraction of samples to keep as separate validation/test set (word lists)\n",
    "TsN = 200                           # Number of randomly generated prompts for each sample when validating model\n",
    "log_period_batches = 10             # Batches per iteration\n",
    "# learning_rate = 5e-7              # Adam learning rate (default is 5e-5, sentiment classification example had 2e-5)\n",
    "learning_rate = 2e-5\n",
    "# learning_rate = 4e-6\n",
    "adam_epsilon = 1e-8                 # Adam epsilon (default is 1e-8)\n",
    "n_sched_warmup = 0                  # Linear scheduler for optimizer number of warmup steps\n",
    "# batch_size = bsz = 64               # Samples per batch\n",
    "# batch_size = bsz = 32\n",
    "batch_size = bsz = 4\n",
    "# N_train_batches = int(1e7 / bsz)  # Total number of batches to show model\n",
    "N_train_batches = 600\n",
    "n_unfreeze = \"all\"                  # Number of model layers to fine tune, in addition to the last output linear layer\n",
    "# max_len = 1024                    # Max n. tokens applied prior to rng_train (number of phrases range)\n",
    "max_len = 32\n",
    "train_phrase_log_pctile = 0.0       # Phrase generation probability percentile excluded from training dataset\n",
    "lidstone_eps = 0.01                 # Smoothing epsilon for possible words/subwords which are not in the missing list words set\n",
    "lastcomma_repl = ',' # 'EOS', ','   # Token optionally used to replace the final comma that ends the generated phrase\n",
    "use_correct_nouns = True            # Whether to use only correct singular or plural form of category nouns for the given prompt\n",
    "swap_noun = False                   # Whether to swap plural and singular nouns in prompt\n",
    "# rng_train = [0, 512]              # Range of prompt list lengths (number of phrases) to generate for training data\n",
    "rng_train = [3, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = \"cuda\" if pt.cuda.is_available() else \"cpu\"  # Setup torch device(s)\n",
    "d = device = pt.device(dev)\n",
    "# world_size = 1\n",
    "# rank = 0\n",
    "# def setup(rank, world_size): \n",
    "#     os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
    "#     os.environ['MASTER_PORT'] = find_free_port()\n",
    "#     dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)  # initialize the process group\n",
    "# def cleanup():\n",
    "#     dist.destroy_process_group()\n",
    "# # mp.spawn(setup, args=(rank, world_size), nprocs=world_size)\n",
    "# setup(rank, world_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cats, cats_sing, phrases = Listset().load()  # Import word lists dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([79, 649, 20, 866, 10164, 60, 18907, 19, 6090, 23, 19, 25571, 23, 19],\n",
       " [79, 649, 20, 866, 10164, 60, 18907, 19],\n",
       " [6090, 23, 19, 25571, 23, 19],\n",
       " [32, 1351, 286, 2835, 15921, 25, 22514, 11, 48389, 11, 279, 4127, 11],\n",
       " [32, 1351, 286, 2835, 15921, 25, 22514, 11],\n",
       " [273, 6231, 11, 279, 4127, 11])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DerivTokenizer():  # Wrap the tokenizer so that we don't produce the special tokens (NLG case)\n",
    "    def __init__(self, tokenizer): self.tokenizer = tokenizer\n",
    "    def __len__(self): return self.tokenizer.__len__()\n",
    "    def encode(self, *args, **kwargs): return self.tokenizer.encode(*args, add_special_tokens=False, **kwargs)\n",
    "    def decode(self, *args, **kwargs): return self.tokenizer.decode(*args, **kwargs)\n",
    "gpt3_tokenizer = DerivTokenizer(GPT2TokenizerFast.from_pretrained((\"gpt2-large\")))\n",
    "with io.capture_output() as captured:\n",
    "    tokenizer = DerivTokenizer(AutoTokenizer.from_pretrained(modelkey.replace(\"-xl\", \"-large\")))\n",
    "tokenizer.encode(\"A list of round fruits: apples, oranges, pears,\"), \\\n",
    "    tokenizer.encode(\"A list of round fruits: apples,\"), tokenizer.encode(\"oranges, pears,\"), \\\n",
    "  gpt3_tokenizer.encode(\"A list of round fruits: apples, oranges, pears,\"), \\\n",
    "    gpt3_tokenizer.encode(\"A list of round fruits: apples,\"), \\\n",
    "    gpt3_tokenizer.encode(\"oranges, pears,\") # Note: gpt-3 tokenizes words differently if they are at the start of the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lprompts_sing = [p for p in lprompts if ((\"types of\" in p) ^ swap_noun)]                 # construct dataset\n",
    "enc_prompts = lambda tknzr, prmts: [pt.tensor(tknzr.encode(p)).to(d) for p in prmts]\n",
    "enc_listset = lambda tknzr, Xs: [[[pt.tensor(tknzr.encode(p + suffix)).to(d) for p in ps] for ps in X] for (X, suffix) in Xs]\n",
    "lprompts_encoded, lprompts_encoded3 = enc_prompts(tokenizer, lprompts), enc_prompts(gpt3_tokenizer, lprompts)\n",
    "lprompts_sing_encoded, lprompts_sing_encoded3 =enc_prompts(tokenizer, lprompts_sing),enc_prompts(gpt3_tokenizer, lprompts_sing)\n",
    "Xs = (cats, ': '), (cats_sing, ': '), (phrases, ', ')\n",
    "(cats_e,cats_sing_e,phrases_e), (cats_e3,cats_sing_e3,phrases_e3) = enc_listset(tokenizer, Xs), enc_listset(gpt3_tokenizer, Xs)\n",
    "comma_token = pt.tensor(tokenizer.encode(\"a,\")[1], device=d)\n",
    "N_tokens = len(tokenizer)\n",
    "N_wordlists = len(cats)\n",
    "lidstone_eps = pt.tensor(lidstone_eps, device=d) if not isinstance(lidstone_eps, pt.Tensor) else lidstone_eps\n",
    "lidstone_value = lidstone_eps / N_tokens\n",
    "y_zero = (lidstone_value).repeat(N_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_phrases = list(set(sum(phrases, [])))\n",
    "vowels = 'aeiuo'\n",
    "# phrase_logs = get_s_avglogs([('An' if p[0].lower() in vowels else 'A') + ' ' + p for p in all_phrases])\n",
    "phrase_logs = ()# todo: function to get log probability of phrase (n-gram) occuring in the pretraining dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "use_max_plog = train_phrase_log_pctile > 0.0\n",
    "if use_max_plog:\n",
    "    max_phrase_log = np.percentile(phrase_logs, 100 * (1.0 - train_phrase_log_pctile))\n",
    "    phrase_incls = [p > max_phrase_log for p in phrase_logs]\n",
    "    phrase_incl = dict(zip(all_phrases, phrase_incls))\n",
    "    all_phrases_enc = sum([[tuple(p_.cpu().detach().numpy().tolist()) for p_ in p] for p in phrases_e], [])\n",
    "    enc_phrase_log_incl = dict(zip(list(set(all_phrases_enc)), phrase_incls))\n",
    "    plt.hist(phrase_logs, bins=250)\n",
    "    plt.axvline(x=max_phrase_log, color='red')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a fixed test set and save to disk. This function defines the next list token prediction problem\n",
    "def gen_truncated_list(prmt, p, ra=True, rng=rng_train, mlen=max_len):  # prmt = prompt tokens, p = list phrase/word tokens\n",
    "    tkzs, sent, tkix, wordix = [], [], 0, -1   # rng is the inclusive range of list lengths to generate (number of phrases)\n",
    "    min_nw, max_nw, min_nt, max_nt = rng[0], int(rng[1]), 0, int(mlen) - len(prmt)\n",
    "    incl_words = np.random.choice(len(p), min(len(p), max_nw), replace=False)\n",
    "    if use_max_plog:\n",
    "        p_ra = [i for i in range(len(p)) if enc_phrase_log_incl[tuple(p[i].cpu().detach().numpy().tolist())]]\n",
    "        p_incl = [i for i in range(len(p)) if i not in p_ra]\n",
    "        p_ra_chosen = [i for i in incl_words if i in p_ra]\n",
    "        min_nw -= len(p_ra_chosen)\n",
    "        incl_words = [i for i in incl_words if i in p_incl]\n",
    "    if len(incl_words) == 0:#rare when train_phrase_log_pctile is low enough and min_nw is high enough (temporary optimisation)\n",
    "        return gen_truncated_list(prmt, p, ra=ra, rng=rng, mlen=mlen)\n",
    "    for phz_i in incl_words:\n",
    "        phz_enc, wordix = p[phz_i], wordix + 1\n",
    "        tkzs.append((tkix, phz_enc)), sent.append(phz_enc)\n",
    "        tkix += len(phz_enc)\n",
    "        if wordix < min_nw: min_nt = tkix\n",
    "        if tkix >= max_nt:\n",
    "            tkix = max_nt\n",
    "            break\n",
    "    if min_nt - 1 >= tkix:  # rare when max_len is large enough (0.5x max possible total list length) (temporary optimisation)\n",
    "        return gen_truncated_list(prmt, p, ra=ra, rng=rng, mlen=mlen)\n",
    "    sent = pt.hstack(sent)[:max_nt]\n",
    "    # If we don't want to include outliers in target\n",
    "    missing_w = [p[i] for i in range(len(p)) if (i not in incl_words) and (True if (ra or not use_max_plog) else (i in p_incl))]\n",
    "    trunc_ix = np.random.randint(max(min_nt - 1, 0), tkix)\n",
    "    trunc_n = min([(trunc_ix - ix) for (ix, enc) in tkzs if ix <= trunc_ix])  # N. end phrase tokens\n",
    "    missing_w += [enc for (ix, enc) in tkzs if ix >= (trunc_ix - trunc_n)]\n",
    "    missing_matches = missing_w\n",
    "    if trunc_n > 0:\n",
    "        phr_start = trunc_ix - trunc_n\n",
    "        partial_phr = sent[phr_start:trunc_ix]\n",
    "        missing_matches = [enc for enc in missing_w if len(enc) >= trunc_n and all(enc[:trunc_n] == partial_phr)]\n",
    "    next_tokens = [enc[trunc_n] for enc in missing_matches]\n",
    "    norm = len(next_tokens) * (1.0 + lidstone_eps)\n",
    "    tunit, y_ = pt.tensor(1 / norm, device=d), y_zero.clone()\n",
    "    for token in next_tokens: y_[token] += tunit\n",
    "    return pt.hstack([prmt, sent[:trunc_ix]]), y_\n",
    "def stac_sample(stac, n):   # Create batches by random permutations (maximise diversity and uniformity) (shuffling done later)\n",
    "    r, mode, total = [], tuple(np.unique(stac)), stac.shape[0]\n",
    "    while n > 0:\n",
    "        if mode == (0,) or mode == (1,) or mode == (-1,):\n",
    "            if n >= total:\n",
    "                new = sum([list(range(total)) for _ in range(n // total)], [])\n",
    "                r += new\n",
    "                n -= len(new)\n",
    "            else:\n",
    "                new = np.random.choice(total, n, replace=False)\n",
    "                stac[new] = (mode[0] + 1) if mode != (1,) else -1\n",
    "                r += new.tolist()\n",
    "                n = 0\n",
    "        else:\n",
    "            old_val, new_val = mode[0] if mode != (-1, 1) else 1, mode[1] if mode != (-1, 1) else -1\n",
    "            old_i = np.nonzero(stac == old_val)[0]\n",
    "            if n >= old_i.shape[0]:\n",
    "                stac[old_i] = new_val\n",
    "                r += old_i.tolist()\n",
    "                n -= old_i.shape[0]\n",
    "            else:\n",
    "                new = np.random.choice(old_i, n, replace=False)\n",
    "                stac[new] = new_val\n",
    "                r += new.tolist()\n",
    "                n = 0\n",
    "        mode = tuple(np.unique(stac))\n",
    "    return r\n",
    "def gen_listname(cp, cp_cs, prompt, prmt, tknzr=tokenizer):\n",
    "    cat_ix = np.random.randint(len(cp_cs))            # First uniformly sample a category title\n",
    "    sing = use_correct_nouns and (cat_ix >= len(cp))  # Singular vs plural prompt prefix\n",
    "    if prompt is None:                                # Uniformly sample a list beginning phrase (\"A list of...\") if not given\n",
    "        lprmpts = ((lprompts_sing_encoded if sing else lprompts_encoded) if tknzr is tokenizer else \\\n",
    "                   (lprompts_sing_encoded3 if sing else lprompts_encoded3)) if tknzr is not None else \\\n",
    "                   (lprompts_sing if sing else lprompts)\n",
    "        prmt = lprmpts[np.random.randint(len(lprmpts))]\n",
    "    return cp_cs[cat_ix], prmt if prompt is None else prompt\n",
    "def gen_listnames_uniform(xcp, xcs, xp, n, prompt=None, tknzr=tokenizer, verbose=False, stac=None):\n",
    "    prmts, cats, ps, j, prmt = [], [], [], 0, None\n",
    "    if prompt is not None and tknzr is not None: prmt = pt.tensor(tknzr.encode(prompt), device=d)\n",
    "    stac_, stac = stac_sample(stac, n) if (stac is not None) else None, stac is not None\n",
    "    if stac: np.random.shuffle(stac_)\n",
    "    for i in (range(len(xcp)) if not stac else stac_):\n",
    "        prmts_, cats_ = [], []\n",
    "        cp, cs, p = xcp[i], xcs[i], xp[i]\n",
    "        cp_cs = cp + cs\n",
    "        for m in range(n if not stac else 1):\n",
    "            cat, prmt = gen_listname(cp, cp_cs, prompt, prmt, tknzr=tknzr)\n",
    "            prmts_.append(prmt), cats_.append(cat)\n",
    "            j += 1\n",
    "            if verbose and j % 100 == 0: sys_print(\"\\rGenerating list names, done: \" + str(j))\n",
    "        ps.append(p), prmts.append(prmts_), cats.append(cats_)\n",
    "    if verbose: sys_print(\"\\rGenerating list names, done: \" + str(j) + \", finished!\\n\")\n",
    "    return prmts, cats, ps, stac\n",
    "def gen_samples_uniform(xcp, xcs, xp, n,              # Weight testing samples (word lists) exactly uniformly\n",
    "                        ra=True, rng=rng_train, prompt=None, tknzr=tokenizer, verbose=False, inds=False, stac=None, mlen=1e9):\n",
    "    xs, ys, sqlens, j, prmt = [], [], [], 0, None\n",
    "    prmts, cats, ps, stac = gen_listnames_uniform(xcp, xcs, xp, n, prompt=prompt, tknzr=tknzr, verbose=verbose, stac=stac)\n",
    "    for i in range(len(prmts)):\n",
    "        x, y, sqlen, p = [], [], [], ps[i]\n",
    "        for k in range(len(prmts[i])):\n",
    "            x_, y_ = gen_truncated_list(pt.hstack([prmts[i][k], cats[i][k]]), p, ra=ra, rng=rng, mlen=mlen)\n",
    "            x.append(x_), y.append(y_), sqlen.append(len(x_))\n",
    "            j += 1\n",
    "            if verbose and j % 100 == 0: sys_print(\"\\rGenerating list elements, done: \" + str(j))\n",
    "        xs.append(x), ys.append(y), sqlens.append(sqlen)\n",
    "    if inds or stac: xs, ys, sqlens = sum(xs, []), sum(ys, []), sum(sqlens, [])\n",
    "    if verbose: sys_print(\"\\rGenerating list elements, done: \" + str(j) + \", finished!\\n\")\n",
    "    return (xs, ys, sqlens, np.arange(len(xcp)).repeat(n)) if inds else (xs, ys, sqlens)\n",
    "def gen_samples(xcp, xcs, xp, n,\n",
    "                ra=True, rng=rng_train, prompt=None, tknzr=tokenizer, inds=False, mlen=1e9):\n",
    "    xs, ys, sqlens, j, prmt = [], [], [], 0, None   \n",
    "    if prompt is not None and tknzr is not None: prmt = pt.tensor(tknzr.encode(prompt), device=d)\n",
    "    xs, ys, sqlens, j = [], [], [], 0\n",
    "    n_sets, indices = len(xcp), []\n",
    "    for m in range(n):  # Maximise per-batch training diversity by randomly sampling the word lists\n",
    "        i = np.random.randint(n_sets)\n",
    "        cp, cs, p = xcp[i], xcs[i], xp[i]\n",
    "        cp_cs = cp + cs\n",
    "        cat_ix, prmt = gen_listname(cp, cp_cs, prompt, prmt)\n",
    "        x_, y_ = gen_truncated_list(pt.hstack([prmt, cp_cs[cat_ix]]), p, ra=ra, rng=rng, mlen=mlen)\n",
    "        xs.append(x_), ys.append(y_), sqlens.append(len(x_)), indices.append(i)\n",
    "    return (xs, ys, sqlens, np.asarray(indices)) if inds else (xs, ys, sqlens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2  8 21  4  7  5 10 25] [14 11  1 19 24 31 18 32]\n",
      "Train\n",
      " ['round fruits', 'microorganisms', 'outback experiences', 'buildings', 'holed pasta', 'rod shaped pasta', 'sounds of a building', 'biological examples of math in nature', 'non-biological examples of math in nature', 'handcrafts', 'communication media', 'storage media', 'scientific principles behind showers', 'scientific principles behind rain showers', 'spacecraft types', 'real spacecrafts', 'interpersonal tokens of trust'] \n",
      "Validation\n",
      " ['construction sounds', 'hats', 'wild animals', 'woodland ecoregions', 'winds', 'physical tokens that confer trust', 'timbers', 'digital tokens that confer trust'] \n",
      "Test\n",
      " ['chemical elements', 'dramatic and literature elements', 'vehicles referred to as crafts', 'music', 'scientific cycles', 'machine learning algorithms', 'glassware', 'windings']\n",
      "Generating list names, done: 1600, finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python36\\lib\\site-packages\\ipykernel_launcher.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating list elements, done: 1600, finished!\n",
      "Generating list names, done: 1600, finished!\n",
      "Generating list elements, done: 1600, finished!\n"
     ]
    }
   ],
   "source": [
    "N_test, N_val = int(test_frac * N_wordlists), int(val_frac * N_wordlists)\n",
    "N_train = N_wordlists - N_test\n",
    "# test_idx = np.random.choice(N_train, N_test, replace=False)\n",
    "test_idx = np.asarray([ 2,  8, 21,  4,  7,  5, 10, 25])\n",
    "save_ld(test_idx, \"test_idx\")\n",
    "# test_idx = load_ld(\"test_idx\")\n",
    "trval_idx = np.asarray([i for i in range(N_wordlists) if i not in test_idx])\n",
    "# val_idx = np.random.choice(trval_idx, N_val, replace=False)\n",
    "val_idx = np.asarray([14, 11,  1, 19, 24, 31, 18, 32])\n",
    "save_ld(val_idx, \"val_idx\")\n",
    "# val_idx = load_ld(\"val_idx\")\n",
    "train_idx = np.asarray([i for i in trval_idx if i not in val_idx])\n",
    "print(test_idx, val_idx)\n",
    "print(*sum([[c+'\\n',[cats[i][0] for i in ix]]for(c,ix)in[[\"Train\",train_idx],[\"\\nValidation\",val_idx],[\"\\nTest\",test_idx]]],[]))\n",
    "index_listset, listset_iXs = lambda inds, Xs: [[X[i] for i in inds] for X in Xs],(\"trval_idx\",\"train_idx\",\"val_idx\",\"test_idx\")\n",
    "phase_listsets = {None: {k[:-4]: index_listset(globals()[k], (cats, cats_sing, phrases)) for k in listset_iXs},\n",
    "             \"default\": {k[:-4]: index_listset(globals()[k], (cats_e, cats_sing_e, phrases_e)) for k in listset_iXs},\n",
    "                \"gpt3\": {k[:-4]: index_listset(globals()[k], (cats_e3, cats_sing_e3, phrases_e3)) for k in listset_iXs}}\n",
    "cats_e_test, cats_sing_e_test, phrases_e_test = phase_listsets[\"default\"][\"test\"]\n",
    "cats_e_val, cats_sing_e_val, phrases_e_val = phase_listsets[\"default\"][\"val\"]\n",
    "val_cats = [cats[i][0] for i in val_idx]\n",
    "test_xs,test_ys,test_sqlens= gen_samples_uniform(cats_e_test, cats_sing_e_test, phrases_e_test, TsN, mlen=max_len, verbose=True)\n",
    "val_xs, val_ys, val_sqlens = gen_samples_uniform(cats_e_val, cats_sing_e_val, phrases_e_val, TsN, mlen=max_len, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write function that takes a prompt, existing list and sampling params and returns gpt3's next token probs\n",
    "default_msp = {\n",
    "  \"best_of\": 1,\n",
    "}\n",
    "default_sp = {\n",
    "  \"temperature\": 1.0,\n",
    "  \"top_p\": 1.0,\n",
    "#   \"top_k\": -1.0,                 # todo: add code to apply this to gpt3 output (top100), max k ~=90, min = 2?\n",
    "  \"presence_penalty\": 0.0,\n",
    "  \"frequency_penalty\": 0.0,\n",
    "}\n",
    "default_params = {\n",
    "  \"engine\": \"davinci\",\n",
    "  \"model\": None,\n",
    "  \"max_tokens\": 1,\n",
    "  \"temperature\": 1.0,\n",
    "  \"top_p\": 1.0,\n",
    "#   \"top_k\": -1.0,\n",
    "  \"presence_penalty\": 0.0,\n",
    "  \"frequency_penalty\": 0.0,\n",
    "  \"n\": 1,\n",
    "  \"stream\": False,\n",
    "  \"logprobs\": 100,\n",
    "#       \"logit_bias\": {\"50256\": -100},\n",
    "  \"stop\": [\",\", \"\\n\"],\n",
    "}\n",
    "# Define and test the OpenAI API next token probability request (response-token-efficient streaming version)\n",
    "def format_gpt3_probs(choice, tokenize):\n",
    "    res, r = [], sorted([(np.e**v, k) for (k, v) in choice[\"logprobs\"][\"top_logprobs\"][0].items()])[::-1]\n",
    "    for i in range(len(r)):\n",
    "        k = gpt3_tokenizer.encode(r[i][1])\n",
    "        if len(k) == 1: res.append((r[i][0], k if tokenize else r[i][1]))\n",
    "    return res\n",
    "def p_req(s, tokenize=False, **kwargs):\n",
    "    use_stream = \"max_tokens\" in kwargs and kwargs[\"max_tokens\"] != 1\n",
    "    kwargs[\"prompt\"], kwargs[\"stream\"] = s, use_stream\n",
    "    with io.capture_output() as captured:\n",
    "        response, result = openai.Completion.create(**{**default_params, **kwargs}), []\n",
    "    return [(np.e**resp[\"choices\"][0][\"logprobs\"][\"token_logprobs\"][0], resp[\"choices\"][0][\"logprobs\"][\"tokens\"][0],\n",
    "             format_gpt3_probs(resp[\"choices\"][0], tokenize)) for resp in (response if use_stream else [response])]\n",
    "# todo: version to handle multiple choices for phrase level evaluation (response-token-expensive)\n",
    "def p_req_m(s, tokenize=False, **kwargs):\n",
    "    if \"max_tokens\" not in kwargs: kwargs[\"max_tokens\"] = 8\n",
    "    if \"n\" not in kwargs: kwargs[\"n\"] = 5\n",
    "    if \"best_of\" in kwargs:\n",
    "        kwargs[\"best_of\"] = int(round(kwargs[\"best_of\"]))\n",
    "        if kwargs[\"n\"] != 1: kwargs[\"best_of\"], kwargs[\"n\"] = kwargs[\"n\"], kwargs[\"best_of\"]\n",
    "    kwargs[\"prompt\"] = s\n",
    "    with io.capture_output() as captured:\n",
    "        response, tokens, probs = openai.Completion.create(**{**default_params, **kwargs}), [], []\n",
    "    for choice in response[\"choices\"]:\n",
    "        tks = [np.e**v for v in choice[\"logprobs\"][\"token_logprobs\"]]\n",
    "        tks = [(choice[\"logprobs\"][\"tokens\"][i], tks[i]) for i in range(len(tks))]\n",
    "        tokens.append(tks), probs.append(format_gpt3_probs(choice, tokenize))\n",
    "    return tokens, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# a = p_req(\"A list of cyclic phenomena: day and night, induction coil, self-oscillation, tornado, runaway greenhouse effect,\")\n",
    "# b = p_req_m(\"A list of cyclic phenomena: day and night, induction coil, self-oscillation, tornado, runaway greenhouse effect,\")\n",
    "# print(sum([a_[0] for a_ in a[0][2]]))\n",
    "# print(','.join([''.join([b__[0] for b__ in b_]) for b_ in b[0]] + [''.join([a_[1] for a_ in a])]).replace('\\n', '⏎'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that takes a completion distribution (top 100) and target next token distribution (multinomial) and computes the\n",
    "# probability that the completion produces a desired output token\n",
    "def prob_corr(pred_p, target_p):\n",
    "    r = 0\n",
    "    if isinstance(pred_p, list):\n",
    "        for (p, token) in pred_p:\n",
    "            if target_p[token] > (lidstone_value + 1e-10): r += p\n",
    "    else:\n",
    "        r = np.sum(target_p[np.nonzero(pred_p > (lidstone_value + 1e-10))[0]])\n",
    "    return r\n",
    "# directly computes the similarity between target and predicted token distributions\n",
    "def score_corr(pred_p, target_p, distance=\"cross-entropy\", redistribute_mass=False, include_negatives=False):  \n",
    "    r = 0\n",
    "    if isinstance(pred_p, list) and not redistribute_mass:\n",
    "        for (p, token) in pred_p:\n",
    "            targ = target_p[token]\n",
    "            if targ > (lidstone_value + 1e-10) or include_negatives:\n",
    "                if   distance == \"unnormalized\":  r -= p * targ\n",
    "                elif distance == \"cross-entropy\": r -= p * np.log(targ)\n",
    "                elif distance == \"kl-divergence\": r += p * np.log(p / targ)\n",
    "                elif distance == \"bhattacharyya\": r += np.sqrt(p * targ)\n",
    "        if distance == \"bhattacharyya\": r = -np.log(r)\n",
    "    else:\n",
    "        p = pred_p\n",
    "        if isinstance(pred_p, list):\n",
    "            p_ = np.asarray([p for (p, _) in pred_p])\n",
    "            ts = np.asarray([t for (_, t) in pred_p])\n",
    "            unaccounted_mass = 1.0 - sum(p_)\n",
    "            n_missing_tokens = N_tokens - len(pred_p)\n",
    "            p = np.repeat(unaccounted_mass / n_missing_tokens, N_tokens)\n",
    "            p[ts] = p_\n",
    "        if not include_negatives:\n",
    "            pos = np.nonzero(target_p > (lidstone_value + 1e-10))[0]\n",
    "            p, target_p = p[pos], target_p[pos]\n",
    "        if   distance == \"unnormalized\":  r = -np.sum(p * targ)\n",
    "        elif distance == \"cross-entropy\": r = -np.sum(p * np.log(target_p))\n",
    "        elif distance == \"kl-divergence\": r =  np.sum(p * np.log(p / target_p))\n",
    "        elif distance == \"bhattacharyya\": r = -np.log(np.sum(np.sqrt(p * target_p)))\n",
    "    return -r\n",
    "# probability that a completion phrase is a desired missing list entry\n",
    "def prob_msp(outs, missing):\n",
    "    correct = 0\n",
    "    missing = set([phrase.lower() for phrase in missing])\n",
    "    for i in range(len(outs)):\n",
    "        out = outs[i].strip().lower()\n",
    "        if out not in missing and (out[:4] == 'the '): out = out[4:]\n",
    "        if out in missing: correct += 1\n",
    "    return correct / len(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   (   'filati cu lu pirtuso,',\n",
      "        array([   17, 10291,  4807,    17,  3525,    17,  2311,    17,  9261,\n",
      "        7077,   155,    19], dtype=int64),\n",
      "        12)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens = sum([[len(p) for p in p_ if len(p) < 1000] for p_ in phrases_e], [])  # Print longest list elements in dataset, get max\n",
    "phrs = sum([[p for p in p_ if len(p) < 1000] for p_ in phrases_e], [])\n",
    "m = np.max(lens)\n",
    "inds = [i for i in range(len(phrs)) if lens[i] == m]\n",
    "ree = [(tokenizer.decode(phrs[i].cpu().detach().numpy()), phrs[i].cpu().detach().numpy(), lens[i]) for i in inds]\n",
    "phrl_max = len(ree[0][1]) - 1\n",
    "pr(ree)\n",
    "phrl_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that takes sampling parameters, then generates n random incomplete list prompts (of length l), obtains completion\n",
    "# distributions (top 100 tokens or full multinomial) and evaluates the average score across the n prompts. n = 20 by default\n",
    "# All samples generated are stored fully for later training of sample-dependent sampling parameter (mixture) distribution\n",
    "sps_ = [\"top_p\", \"temperature\", \"presence_penalty\", \"frequency_penalty\"]  # sampling params                          #top_k\n",
    "msps_= sps_#[\"best_of\"] + sps_                                                 # meta sampling params\n",
    "create_folder(data_dir + learning_data_dir)\n",
    "create_folder(data_dir + learning_data_dir + \"sp_samples\")\n",
    "create_folder(data_dir + learning_data_dir + \"sp_samples_test\")\n",
    "create_folder(data_dir + learning_data_dir + \"msp_samples_nb\")\n",
    "create_folder(data_dir + learning_data_dir + \"msp_samples_nb_test\")\n",
    "phaseIx = lambda phase: globals()[phase + '_idx']\n",
    "def save_modeloutput(idx, dname, pnames, params, r, min_l, max_l, mdl, xs=None, ys=None, sqlens=None, inds=None, d=None):\n",
    "    engine_str = ','.join([str(params[k]) for k in [\"engine\", \"model\"] if k in params])\n",
    "    for i_raw in range(len(idx)):\n",
    "        i = idx[i_raw]\n",
    "        create_folder(data_dir + learning_data_dir + dname + \"/\" + str(i))\n",
    "        ix, mdl_name = np.nonzero(inds == i_raw)[0], mdl if isinstance(mdl, str) else mdl[\"name\"]\n",
    "        fn = str(time.time()) + '_' + '_'.join([str(v) for v in [params[k] for k in pnames] + [min_l, max_l]]) + '_' + mdl_name\n",
    "        input_data = [d[j] for j in ix] if d else [xs[ix], ys[ix], sqlens[ix]]\n",
    "        save_ld((params, input_data, ix, [r[j] for j in ix], str(mdl)), dname + \"/\" + str(i) + \"/\" + fn, compress=9)\n",
    "def eval_sp(params, min_l=0, max_l=1e9, n=20, prmt=None, phase=\"train\", uniform=True, mdl='gpt3'):\n",
    "    res, max_l = [], int(max_l), \n",
    "    tknzr = gpt3_tokenizer if mdl == \"gpt3\" else tokenizer\n",
    "    xcp, xcs, xp = phase_listsets[\"gpt3\" if mdl == \"gpt3\" else \"default\"][phase]\n",
    "    xs, ys, sqlens, inds = gen_samples_uniform(xcp, xcs, xp, n, prompt=prmt, tknzr=tknzr, inds=True, rng=[min_l, max_l]) \\\n",
    "           if uniform else gen_samples        (xcp, xcs, xp, n, prompt=prmt, tknzr=tknzr, inds=True, rng=[min_l, max_l])\n",
    "    if mdl == 'gpt3': r = [p_req(gpt3_tokenizer.decode(x_.detach().cpu().numpy()), **params) for x_ in xs] \n",
    "    else:             r = mdl[\"probabilities\"](xs, ys, sqlens, **params)\n",
    "    save_modeloutput(phaseIx[phase][np.unique(inds)], \"sp_samples\", sps_, params, r, min_l, max_l, mdl, xs, ys, sqlens, inds)\n",
    "    return np.mean([score_corr(r[i], ys[i]) for i in range(len(r))])\n",
    "def eval_sp_conv(params, tol=0.01, **kwargs):\n",
    "    center, samples = np.inf, []\n",
    "    while True:\n",
    "        samples.append(eval_sp(params, n=2, **kwargs))\n",
    "        new_center = np.mean(samples)\n",
    "        if abs(center - new_center) < tol: return new_center, samples\n",
    "        new_center = center\n",
    "# This metric differs depending on tokenisation, so for the testing of models, a full phrase accuracy function is required\n",
    "def gen_phraselevel_samples_uniform(phase, min_l, max_l, n, prmt, ra=False):\n",
    "    xcp, xcs, xp = phase_listsets[None][phase]\n",
    "    d, (prmts, cats, ps, _) = [], gen_listnames_uniform(xcp, xcs, xp, n, prompt=prmt, tknzr=None)\n",
    "    for i in range(len(xcp)):\n",
    "        x, y, sqlen, p = [], [], [], ps[i]\n",
    "        for m in range(n):\n",
    "            prompt, cat = prmts[i][m], cats[i][m]\n",
    "            phr_ix, phr_incl = [], []\n",
    "            while len(phr_ix) < 1:\n",
    "                phr_ix = np.random.choice(len(p), np.random.randint(min_l, min(max_l, len(p) - 1)), replace=False)\n",
    "                if use_max_plog:\n",
    "                    phr_ra = [j for j in range(len(p)) if phrase_log_incl[p[j]]]\n",
    "                    phr_incl = [j for j in range(len(p)) if j not in phr_ra]\n",
    "                    phr_ix = [i for i in phr_ix if i in phr_incl]\n",
    "            missing_ix= [k for k in range(len(p)) if (k not in phr_ix)and(True if (ra or not use_max_plog) else k in phr_incl)]\n",
    "            prompt = (prompt + ' ' + cat + ': ' + ''.join([p[j] + ', ' for j in phr_incl]))[:-1]\n",
    "            d.append([prompt, [p[j] for j in missing_ix]])\n",
    "    return d, np.arange(len(xcp)).repeat(n)\n",
    "strip_the = lambda x: x[4:] if x.lower()[:4] == 'the ' else x\n",
    "strip_tl = lambda x: strip_the(x.strip().lower())\n",
    "strip_comma = lambda x: [x_.strip() for x_ in (x[:-1] if len(x) > 1 else x)]\n",
    "strip_lower = lambda x: [strip_tl(x_) for x_ in (x[:-1] if len(x) > 1 else x)]\n",
    "def ensemble_two_models_results(m2ensemble_frac, r, r2):\n",
    "    bof = len(r[0])\n",
    "    n_replace = int(bof * m2ensemble_frac)\n",
    "    r_new = [[strip_comma(''.join([r___[0] for r___ in r__]).split(',')) for r__ in r_] for r_ in r]\n",
    "    for i in range(len(r)):\n",
    "        cur_pool = set(sum([strip_lower(''.join([r__[0] for r__ in r_]).split(',')) for r_ in r[i][:bof - n_replace]], []))\n",
    "        new_pool = sum([strip_comma(''.join([r__[0] for r__ in r_]).split(',')) for r_ in r2[i]], [])\n",
    "        for j in range(n_replace):\n",
    "            n_phrases = len(r[i][j])\n",
    "            add = []\n",
    "            while len(add) < n_phrases and len(new_pool) > 0:\n",
    "                new_phr = new_pool[0]\n",
    "                stripped_new_phr = strip_tl(new_phr)\n",
    "                if stripped_new_phr not in cur_pool:\n",
    "                    add.append(new_phr), cur_pool.add(stripped_new_phr)\n",
    "                new_pool = new_pool[1:]\n",
    "            if len(add) > 0:\n",
    "                r_new[i][j + bof - n_replace] = add\n",
    "    return [sum(r_, []) for r_ in r_new]\n",
    "def test_sp(params, min_l=0, max_l=1e9, n1=3, n2=10, prmt=None, phase=\"train\", uniform=True, max_tokens=phrl_max,\n",
    "            mdl='gpt3', mdl2=None, d=None, d_test=None, inds=None, inds_test=None, m2ensemble_frac=0.4, return_test_acc=True):\n",
    "    max_l, test_acc, dn = int(max_l), None, \"msp_samples_nb\"\n",
    "    if d is None: d, inds = gen_phraselevel_samples_uniform(phase, min_l, max_l, n1, prmt)\n",
    "    params = {**default_sp, **params, **{'n': n2, 'max_tokens': max_tokens}} #**default_msp,\n",
    "    if mdl == 'gpt3': r = [p_req_m(d_[0], **params)[0] for d_ in d]  # Request predictions from OpenAI\n",
    "    else:             r = mdl[\"completions\"]([d_[0] for d_ in d], **params)\n",
    "    save_modeloutput(phaseIx(phase), dn, msps_, params, r, min_l, max_l, mdl, d=d, inds=inds)\n",
    "    if mdl2 is not None:\n",
    "        if mdl2 == 'gpt3': r2 = [p_req_m(d_[0], **params)[0] for d_ in d]# Ensemble with 2nd model according to m2ensemble_frac\n",
    "        else:              r2 = mdl2[\"completions\"]([d_[0] for d_ in d], **params)# This loads precomputed outputs so no resave\n",
    "        r = ensemble_two_models_results(m2ensemble_frac, r, r2)\n",
    "    else:\n",
    "        r = [sum([strip_comma(''.join([r___[0] for r___ in r__]).split(',')) for r__ in r_], []) for r_ in r]\n",
    "    #     r = [sum([strip_comma(''.join([r___[0] for r___ in r__]).split(','))[:max_l - min_l] for r__ in r_], []) for r_ in r]\n",
    "    acc = np.mean([prob_msp(r[i], d[i][1]) for i in range(len(r))])\n",
    "\n",
    "    if phase != \"test\":  # if not testing finalised parameters, also output the test set accuracy\n",
    "        listset_fracs = {\"train\": 1 - (val_frac + test_frac), \"trval\": 1 - test_frac, \"val\": val_frac, \"test\": test_frac}\n",
    "        n1_ = n1 * int(listset_fracs[phase] / test_frac)  # Use approximately enough samples to converge\n",
    "        if d_test is None: d_test, inds_test = gen_phraselevel_samples_uniform(\"test\", min_l, max_l, n1_, prmt)\n",
    "        if mdl == 'gpt3': r_test = [p_req_m(d_[0], **params)[0] for d_ in d_test]\n",
    "        else:             r_test = mdl[\"completions\"]([d_[0] for d_ in d_test], **params)\n",
    "        save_modeloutput(phaseIx(\"test\"), dn+\"_test\", msps_, params, r_test, min_l, max_l, mdl, d=d_test, inds=inds_test)\n",
    "        if mdl2 is not None:\n",
    "            if mdl2 == 'gpt3': r2_test = [p_req_m(d_[0], **params)[0] for d_ in d_test]\n",
    "            else:              r2_test = mdl2[\"completions\"]([d_[0] for d_ in d_test], **params)\n",
    "            r_test = ensemble_two_models_results(m2ensemble_frac, r_test, r2_test)\n",
    "        else:\n",
    "            r_test = [sum([strip_comma(''.join([r___[0] for r___ in r__]).split(',')) for r__ in r_], []) for r_ in r_test]\n",
    "#r_test =[sum([strip_comma(''.join([r___[0] for r___ in r__]).split(','))[:max_l - min_l] for r__ in r_], []) for r_ in r_test]\n",
    "        samps = np.asarray([prob_msp(r_test[i], d_test[i][1]) for i in range(len(r_test))])\n",
    "        test_sd = np.std(100*samps)\n",
    "        test_acc = np.mean(samps)\n",
    "        if not return_test_acc:\n",
    "            print(\"Test acc:\", 100*test_acc, \"sd:\", test_sd)\n",
    "\n",
    "    return (acc, test_acc) if return_test_acc else acc\n",
    "def test_sp_conv(params, tol=0.01, **kwargs):\n",
    "    center, ys, i, steps_nochange = np.inf, [], 1, 0\n",
    "    while True:\n",
    "        ys.append(test_sp(params, n1=1, **kwargs))\n",
    "        new_center = np.mean([s[0] for s in ys])\n",
    "        if abs(center - new_center) < tol: steps_nochange += 1\n",
    "        else:                              steps_nochange = 0\n",
    "        if steps_nochange >= 3 and i >= 5:\n",
    "            if ys[0][1] is not None: print([s[1] for s in ys])\n",
    "            print([s[0] for s in ys])\n",
    "            j = 1 if ys[0][1] is not None else 0\n",
    "            sys_print(str((\"Test acc:\", 100*np.mean([s[j] for s in ys]), \"sd:\", np.std([100*s[j] for s in ys])))+\"\\n\", False)\n",
    "            return new_center, ys\n",
    "        center = new_center\n",
    "        i += 1\n",
    "# eval_sp(default_ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that uses bayesian optimisation or similar method to quickly find the maxima of the above function (find best params)\n",
    "def evaluate_gpt3_msp_lumped(temperature, top_p, presence_penalty, frequency_penalty):\n",
    "    ps = locals()\n",
    "    ps[\"best_of\"] = 5.0\n",
    "    return test_sp_conv(ps, min_l=3, max_l=30, tol=0.02, phase=\"train\", uniform=True, mdl='gpt3')[0]\n",
    "bounds = {\n",
    "  \"temperature\": [0.05, 1.0],  # a temp of 0 selects the top completion every time due to the infinitely larger logit\n",
    "  \"top_p\": [0.05, 0.9],        # same with this but more obvious\n",
    "#   \"top_k\": [1, 100],        # fix this to be a log of the input value so that lower values are sampled with high resolution?\n",
    "  \"presence_penalty\": [0.1, 2.0],   # both presence and frequency penalty have optimal values\n",
    "  \"frequency_penalty\": [0.1, 2.0],  # this can also go down to -2.0, probably not useful for our case...\n",
    "#   \"best_of\": [0.51, 5.49], # to do\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "Model device: cuda:0\n",
      "Pretrained parameters loaded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "427"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if 'model' in locals():  # Load pretrained weights\n",
    "    del model\n",
    "print(gc.collect()), pt.cuda.empty_cache()\n",
    "create_folder(\"models\")\n",
    "create_folder(\"models/pretrained\")\n",
    "create_folder(\"models/pretrained/\" + modelkey)\n",
    "model_ = locals()[modelclass].from_pretrained(modelkey, output_hidden_states=False, output_attentions=False, \n",
    "    cache_dir=\"models/pretrained/\" + modelkey)\n",
    "if pt.cuda.device_count() > 1 and \"gpt\" in modelkey:\n",
    "    device_map = {0: [0, 1, 2],\n",
    "                  1: [3, 4, 5, 6, 7, 8],\n",
    "                  2: [9, 10, 11, 12, 13, 14],\n",
    "                  3: [15, 16, 17, 18, 19, 20],\n",
    "                  4: [21, 22, 23, 24, 25, 26, 27],\n",
    "                  5: [28, 29, 30, 31, 32, 33, 34],\n",
    "                  6: [35, 36, 37, 38, 39, 40, 41],\n",
    "                  7: [42, 43, 44, 45, 46, 47],\n",
    "                 }\n",
    "    model_.parallelize(device_map)\n",
    "else:\n",
    "    model_ = model_.to(d)\n",
    "print(\"Model device:\", model_.device)\n",
    "if \"gpt2\" in modelkey:\n",
    "    model_.resize_token_embeddings(N_tokens)\n",
    "    pad_token = model_.config.pad_token_id = model_.config.eos_token_id\n",
    "    pad_token = pt.tensor(pad_token, device=d)\n",
    "    repl_token = pt.tensor(tokenizer.encode(lastcomma_repl)[0], device=d) if lastcomma_repl != 'EOS' else pad_token\n",
    "    n_embd = pt.tensor(model_.config.n_embd, device=d)\n",
    "else:\n",
    "    pad_token = model_.config.pad_token_id\n",
    "    pad_token = pt.tensor(pad_token, device=d)\n",
    "    mask_token = pt.tensor(tokenizer.encode(\"<mask>\")).to(d)\n",
    "# model = nn.parallel.DistributedDataParallel(model_, device_ids=[d])\n",
    "# model = nn.DataParallel model_, device_ids=list(range(pt.cuda.device_count()))) if dev != \"cpu\" else model_\n",
    "model = model_\n",
    "mname_fn = modelkey\n",
    "multimask_arch = \"xlnet\" in modelkey\n",
    "print(\"Pretrained parameters loaded\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine tuning the last 12 of 12 model layers plus the output linear layer\n"
     ]
    }
   ],
   "source": [
    "max_layer = max([int(name.split('transformer.layer.')[1].split('.')[0]) for (name, _) in model.named_parameters() \\\n",
    "                 if \"transformer.layer.\" in name])\n",
    "if n_unfreeze == \"all\": n_unfreeze = max_layer + 1\n",
    "if n_unfreeze < 0: n_unfreeze = max_layer + n_unfreeze\n",
    "print(\"Fine tuning the last \" + str(n_unfreeze) + \" of \" + str(max_layer + 1) + \" model layers plus the output linear layer\")\n",
    "for (name, v) in model.named_parameters():\n",
    "    if name == \"transformer.mask_emb\":              continue      # torch.Size([1, 1, 768])\n",
    "    if name == \"transformer.word_embedding.weight\": continue      # torch.Size([32000, 768])\n",
    "    if name == \"lm_loss.bias\":                      continue      # torch.Size([32000])\n",
    "    else:\n",
    "        layer_n = int(name.split('transformer.layer.')[1].split('.')[0])\n",
    "        if layer_n < n_unfreeze: v.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define next batch function\n",
    "curr_ri = np.zeros(len(train_idx), dtype=int)\n",
    "def next_batch(sz):\n",
    "    global phase_listsets, curr_ri\n",
    "    cats_, cats_sing_, phrases_ = phase_listsets[\"default\"][\"train\"]\n",
    "    return adapt_form(*gen_samples_uniform(cats_, cats_sing_, phrases_, sz, ra=False, stac=curr_ri, mlen=max_len))\n",
    "#     return adapt_form(*gen_samples(cats_, cats_sing_, phrases_, sz, ra=False, mlen=max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also create a gpt3 prompt-completion-based regression model to predict values/densities of sampling parameters (might work if\n",
    "# it's possible to find a humanlike transliteration of the problem statement that gpt3 can bootstrap on to find the params, or\n",
    "# to find some (possibly entirely textual) representation of a params-correlating multidimensional metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entirely textual representation attempts:\n",
    "# for example: Some round fruits: apple, orange, pear. As a percentage of all round fruits, this list represents <blank>\n",
    "# with: Some round fruits: apple, orange, pear. Does this list use a relatively strict definition of round fruits? <blank>\n",
    "# and Some round fruits: apple, orange, pear. To what degree is this a representative list? <blank>  davinci-instruct priming\n",
    "# etc... in some sense it reformulates it as a multi sentiment classification that's able to predict optimal PLM sampling params\n",
    "# From early experiments this may be possible however the noise is difficult to deal without some heavy improvements on priming\n",
    "# Alternatively, we could use the top-p numerical token instead of <blank> and find a priming prompt that outputs these best\n",
    "# This way we can \"fine-tune\" GPT3-davinci (which is much better than curie but has no fine tuning API) by \"priming recursion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "llayer = None #nn.Linear(n_embd, N_tokens, bias=False).to(d)#.cpu()                  # Model definition\n",
    "# nn.init.xavier_uniform_(llayer.weight)\n",
    "# llayer = nn.DataParallel(llayer, device_ids=list(range(pt.cuda.device_count()))) if dev != \"cpu\" else llayer\n",
    "# llayer = nn.parallel.DistributedDataParallel(llayer, device_ids=list(range(pt.cuda.device_count()))).to(d)\n",
    "# softmax = nn.Softmax()\n",
    "bcewl_loss = nn.BCEWithLogitsLoss()#.to(d)#.cpu()\n",
    "# bcewl_loss = nn.DataParallel(bcewl_loss, device_ids=list(range(pt.cuda.device_count()))).to(d) if dev != \"cpu\" else bcewl_loss\n",
    "# bcewl_loss = nn.parallel.DistributedDataParallel(bcewl_loss, device_ids=list(range(pt.cuda.device_count()))).to(d)\n",
    "# nll_loss = nn.NLLLoss()\n",
    "# kl_loss = nn.KLDivLoss()\n",
    "optimizer = pt.optim.AdamW(model.parameters(), lr=learning_rate, eps=adam_epsilon)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=n_sched_warmup, num_training_steps=N_train_batches)\n",
    "def sequence_mask(lengths, maxlen=None, dtype=pt.int):\n",
    "    if maxlen is None:\n",
    "        maxlen = lengths.max()\n",
    "    row_vector = pt.arange(0, maxlen, 1, device=d)\n",
    "    matrix = pt.unsqueeze(lengths, dim=-1)\n",
    "    mask = row_vector < matrix\n",
    "\n",
    "    mask = mask.type(dtype)\n",
    "    return mask\n",
    "def multimask_model(model, x=None, sqlens=None, past=None, seq_maxlen=None, add=None, many_inp=None, return_states=None, **kw):\n",
    "    x = pt.cat([x, pad_token.repeat((x.shape[0], 1))], axis=1)\n",
    "    if add == 0: \n",
    "        x[pt.arange(x.shape[0]).to(d), sqlens] = mask_token\n",
    "    else:\n",
    "        x[:, -1] = mask_token\n",
    "    perm_mask = pt.ones(x.shape[0], x.shape[1], x.shape[1], dtype=pt.float).to(d)  # causal mask\n",
    "    perm_mask = pt.triu(perm_mask, diagonal=0)\n",
    "#     perm_mask = pt.zeros((x.shape[0], (seq_maxlen if add == 0 else 0) + add + 1, seq_maxlen + add + 1), dtype=pt.float).to(d)\n",
    "    for i in range(sqlens.shape[0]):\n",
    "        l = sqlens[i] + add\n",
    "        perm_mask[i, :, -(x.shape[1] - l):seq_maxlen + 2] = 1.0  # Previous tokens don't see last token or padding tokens\n",
    "        if add > 0:\n",
    "            perm_mask[i, :, seq_maxlen + 1] = 0.0\n",
    "            perm_mask[i, :, -1] = 1.0\n",
    "#     if add > 0:\n",
    "#         perm_mask = pt.zeros((x.shape[0], 2, 2), dtype=pt.float).to(d)\n",
    "#         perm_mask[:, :, -1] = 1.0\n",
    "#         perm_mask = pt.ones((x.shape[0], 1, 1), dtype=pt.float).to(d)\n",
    "    attn_mask = None\n",
    "#     if (past is not None) and add > 0:\n",
    "# #         attn_mask = sequence_mask(sqlens, seq_maxlen)  # The equivalent attention_mask (debug)\n",
    "# #         attn_mask = pt.cat([attn_mask, pt.ones(attn_mask.shape[0], add).to(d)], dim=1)\n",
    "# #         attn_mask = pt.cat([attn_mask, pt.zeros(attn_mask.shape[0], 1).to(d)], dim=1)\n",
    "#         past_ = []\n",
    "#         for i in range(len(past)):\n",
    "#             past_.append(past[i][:-1])  # Don't use the hidden states of the mask token from last iteration\n",
    "#         past = tuple(past_)\n",
    "    target_mapping = pt.zeros((x.shape[0], 1, seq_maxlen + add + 1), dtype=pt.float).to(d)\n",
    "    if add == 0:\n",
    "        target_mapping[pt.arange(x.shape[0]).to(d), 0, sqlens] = 1.0\n",
    "    else:\n",
    "        target_mapping[:, 0, -1] = 1.0\n",
    "        target_mapping = target_mapping[:, :, -2:]\n",
    "#     print(x.shape, perm_mask.shape, attn_mask.shape if attn_mask is not None else None, \\\n",
    "#           target_mapping.shape, (len(past), past[0].shape) if past is not None else None)\n",
    "#     print(tokenizer.decode(x[0].cpu().detach().numpy()))\n",
    "#     print(perm_mask, target_mapping)\n",
    "    res = model(x, perm_mask=perm_mask if attn_mask is None else None, target_mapping=target_mapping,\n",
    "                   mems=past, attention_mask=attn_mask, use_mems=None if (past is None and not return_states) else True)\n",
    "#     print(res[0].shape)\n",
    "    return res\n",
    "# next_token_logits = outputs[0]  # Output has shape [target_mapping.size(0), target_mapping.size(1), config.vocab_size]\n",
    "def singlemask_model(model, x=None, sqlens=None, past=None, return_states=None, seq_maxlen=None, add=None, many_inp=None, **kw):\n",
    "    mask = sequence_mask(sqlens, seq_maxlen) if (many_inp or add != 0) else None\n",
    "    if add > 0: mask = pt.cat([mask, pt.ones(mask.shape[0], add).to(d)], dim=1)  # Append mask entry for new stream token\n",
    "    return model(x.long(), attention_mask=mask, use_cache=None if not return_states else True, past_key_values=past)\n",
    "def inference(x, sqlens, past=None, return_states=False, seq_maxlen=max_len, add=0, return_fulloutput=False):\n",
    "    global model\n",
    "    many_inp = x.shape[1] > 1\n",
    "    outputs = multimask_model(model, **locals()) if multimask_arch else singlemask_model(model, **locals())\n",
    "    if return_fulloutput: return outputs\n",
    "    logits = outputs[0].squeeze(1) if multimask_arch else \\\n",
    "            (outputs[0][[pt.arange(x.shape[0]), sqlens - 1]] if many_inp else outputs[0].squeeze(1))\n",
    "#     logits = outputs[0][[pt.arange(x.shape[0]), sqlens - 1]] if many_inp else outputs[0].squeeze(1)\n",
    "\n",
    "    return (logits, outputs[1]) if return_states else logits  # Optionally return the past states needed to restore the stream\n",
    "def adapt_form(xs, ys, sqlens, mlen=max_len, repl_finalcomma=True):\n",
    "    xs = pt.vstack([F.pad(x, (0, max(0, mlen - len(x))), mode='constant', value=pad_token)[:mlen] for x in xs])\n",
    "    _ys = pt.vstack(ys) if ys is not None else None\n",
    "    if repl_finalcomma and (lastcomma_repl != ',') and ys is not None:\n",
    "        _ys[:, repl_token] += _ys[:, comma_token] - lidstone_value\n",
    "        _ys[:, comma_token] = lidstone_value\n",
    "    return xs, _ys, pt.tensor(sqlens, device=d)\n",
    "def train_step():\n",
    "    global model, llayer, bcewl_loss, optimizer, bsz, scheduler\n",
    "    x_batch, y_batch, sqlens_batch = next_batch(bsz)\n",
    "\n",
    "    model.zero_grad()\n",
    "    mask = sequence_mask(sqlens_batch, max_len)\n",
    "    outputs = model(x_batch.long(), attention_mask=mask)  # Get logits\n",
    "    logits = outputs[0][[pt.arange(x_batch.shape[0]), sqlens_batch - 1]]\n",
    "\n",
    "#     logsofts = pt.log(softmax(logits))\n",
    "    loss = bcewl_loss(logits, y_batch.float())\n",
    "    loss = loss.mean()\n",
    "    correct = pt.mean((y_batch[pt.arange(batch_size), pt.argmax(logits, axis=1)] > (lidstone_value + 1e-10)).float())\n",
    "    loss_, correct_ = loss.detach().cpu().numpy(), correct.detach().cpu().numpy()\n",
    "    \n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    return loss_, correct_\n",
    "def eval_test(x, y, sqlens):\n",
    "    global bcewl_loss\n",
    "\n",
    "    with pt.no_grad():\n",
    "        logits = inference(x, sqlens)\n",
    "        loss = bcewl_loss(logits, y.float())\n",
    "        loss = loss.mean()\n",
    "        correct = pt.mean((y[pt.arange(x.shape[0]), pt.argmax(logits, axis=1)] > (lidstone_value + 1e-10)).float())\n",
    "        loss_, correct_ = loss.detach().cpu().numpy(), correct.detach().cpu().numpy()\n",
    "    return loss_, correct_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_i = -1  # Batch 0 is the first iteration, where validation occurs without any training\n",
    "best_acc, best_loss = 0, np.inf\n",
    "best_acc_idx = -1\n",
    "out_str = ''\n",
    "create_folder(\"models\")\n",
    "create_folder(\"model_logs\")\n",
    "create_folder(\"models/\" + model_name)\n",
    "graphs_folder = \"graphs\"\n",
    "create_folder(graphs_folder)\n",
    "train_loss, train_accuracy, val_loss, val_accuracy = [], [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_training(verbose=True):  # Training loop function\n",
    "    global model, batch_i, best_acc, best_loss, best_acc_idx, train_loss, train_accuracy, val_loss, val_accuracy\n",
    "    \n",
    "    model.train()\n",
    "    iter_loss, iter_accuracy, b_no_inp = [], [], 0\n",
    "    while batch_i < N_train_batches:\n",
    "        batch_i += 1\n",
    "        if batch_i > 0:\n",
    "            gc.collect()\n",
    "            if dev != \"cpu\": pt.cuda.empty_cache()\n",
    "            b_loss, b_accuracy = train_step()\n",
    "            mname_fn = model_name\n",
    "            if verbose:\n",
    "                sys_print('\\rLoss, accuracy: ' + str(np.mean(b_loss)) + ', ' + str(np.mean(b_accuracy)) + \\\n",
    "                          ' @ batch '+ str(batch_i) + ' (' + str(batch_i * batch_size) + ' samples) complete.                  ')\n",
    "            iter_loss.append(b_loss), iter_accuracy.append(b_accuracy)\n",
    "\n",
    "        if batch_i % log_period_batches == 0:  # Test on val set\n",
    "            model.eval()\n",
    "            loss, accuracy = [], []\n",
    "            out_str = '\\n'\n",
    "            for i in range(N_val):\n",
    "                val_X, val_Y, val_Sqlens = adapt_form(val_xs[i], val_ys[i], val_sqlens[i], repl_finalcomma=batch_i > 0)\n",
    "                feed_batches = [range(len(val_X))[i * bsz:(i + 1) * bsz] for i in range((len(val_X) // bsz) + \\\n",
    "                                                                                        (1 if (len(val_X) % bsz) != 0 else 0))]\n",
    "                if dev != \"cpu\": pt.cuda.empty_cache()\n",
    "                ls, cs = zip(*[eval_test(val_X[inds], val_Y[inds], val_Sqlens[inds]) for inds in feed_batches])\n",
    "                loss.append(np.mean(ls)), accuracy.append(np.mean(cs))\n",
    "                out_str += val_cats[i] + ': ' + str(loss[-1]) + ', ' + str(accuracy[-1]) + '\\n'\n",
    "            \n",
    "            val_l, val_a = np.mean(loss), np.mean(accuracy)\n",
    "            val_loss.append(val_l), val_accuracy.append(val_a)\n",
    "            if batch_i == 0: iter_loss, iter_accuracy = [val_l], [val_a]\n",
    "            train_l, train_a = np.mean(iter_loss), np.mean(iter_accuracy)\n",
    "            train_loss.append(train_l), train_accuracy.append(train_a)\n",
    "            iter_loss, iter_accuracy = [], []\n",
    "\n",
    "#             if ((val_a > best_acc and val_a >= train_a) or \\\n",
    "            if ((val_a > best_acc) or \\\n",
    "              (batch_i // log_period_batches) == 1) and batch_i > 0:      # Save best accuracy model\n",
    "                best_acc = val_a\n",
    "                best_loss = val_l\n",
    "                best_acc_idx = batch_i // log_period_batches\n",
    "                pt.save({\"model\": model.state_dict(),\n",
    "#                          \"llayer\": llayer.state_dict(),\n",
    "#                          \"softmax\": softrmax.state_dict(),\n",
    "                         \"bcewl_loss\": bcewl_loss.state_dict(),\n",
    "#                          \"nll_loss\": nll_loss.state_dict(),\n",
    "#                          \"kl_loss\": kl_loss.state_dict(),\n",
    "                         \"optimizer\": optimizer.state_dict(),\n",
    "                         \"scheduler\": scheduler.state_dict(),\n",
    "                         }, \"./models/\" + model_name + '/' + model_name)\n",
    "                b_no_inp = 0\n",
    "            else:\n",
    "                b_no_inp += log_period_batches\n",
    "                \n",
    "            if verbose:\n",
    "                clear_output()\n",
    "                print(out_str + \"Batch\", batch_i, ':', train_a, val_a, \"loss:\", train_l, val_l, \\\n",
    "                      \"Best:\", best_acc, best_loss, 'idx:', best_acc_idx)\n",
    "                fig = plt.figure()\n",
    "                fig.set_size_inches(16, 5)\n",
    "                g = fig.add_subplot(1,2,1)\n",
    "                g.grid()\n",
    "                g.plot(train_accuracy, label='train acc')\n",
    "                g.plot(val_accuracy, label='val acc')\n",
    "                g.legend(loc='lower right')\n",
    "#                 g.axhline(y=0.714, ls='--', color='grey')\n",
    "\n",
    "                g = fig.add_subplot(1,2,2)\n",
    "                g.grid()\n",
    "                g.plot(train_loss, label='train loss')\n",
    "                plt.yscale(\"log\")\n",
    "                g.plot(val_loss, label='val loss')\n",
    "                plt.yscale(\"log\")\n",
    "                g.legend(loc='upper right')\n",
    "\n",
    "                save_ld((train_accuracy, val_accuracy, train_loss, val_loss),\n",
    "                        \"model_logs/\" + model_name + '_log_latest', pad=False)\n",
    "                plt.savefig(graphs_folder + '/' + model_name + \"_curve_latest\" + '.pdf', format='pdf')\n",
    "                plt.show()\n",
    "\n",
    "            model.train()\n",
    "    return best_acc, best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "construction sounds: 1.5462981, 0.703125\n",
      "hats: 5.6392183, 0.59765625\n",
      "wild animals: 3.69217, 0.71875\n",
      "woodland ecoregions: 1.7893951, 0.6484375\n",
      "winds: 1.4743235, 0.55859375\n",
      "physical tokens that confer trust: 4.3564568, 0.56640625\n",
      "timbers: 2.9912105, 0.5859375\n",
      "digital tokens that confer trust: 3.4626179, 0.63671875\n",
      "Batch 600 : 0.165625 0.6269531 loss: 0.16254742 3.1189613 Best: 0.65185547 4.4667377 idx: 14\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(0.65185547, 4.4667377)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(gc.collect())  # Train ernst model (enumeration representation neural subtext transfer)\n",
    "iterate_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pt.cuda.empty_cache(), gc.collect()  # Load best model (checkpoint)\n",
    "cpoint = pt.load(\"./models/\" + model_name + '/' + model_name)\n",
    "model.load_state_dict(cpoint['model'])\n",
    "bcewl_loss.load_state_dict(cpoint['bcewl_loss'])\n",
    "optimizer.load_state_dict(cpoint['optimizer'])\n",
    "scheduler.load_state_dict(cpoint['scheduler'])\n",
    "# llayer.load_state_dict(cpoint['llayer'])\n",
    "mname_fn = model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_filtering(logits, tcounts=None, filter_value=-float('Inf'),  # Function to tune the output token distribution\n",
    "                  top_k=0, top_p=0.0, temperature=1.0, frequency_penalty=0.0, presence_penalty=0.0):\n",
    "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
    "        Args:\n",
    "            logits: logits distribution shape (vocabulary size)\n",
    "            top_k >0: keep only top k tokens with highest probability (top-k filtering).\n",
    "            top_p >0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
    "                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
    "    \"\"\"\n",
    "    assert logits.dim() == 1  # batch size 1 for now - could be updated for more but the code would be less clear\n",
    "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
    "    if tcounts is not None: logits -= (tcounts * frequency_penalty) + ((tcounts > 0) * presence_penalty)\n",
    "    logits /= temperature\n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the last token of the top-k\n",
    "        indices_to_remove = logits < pt.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p > 0.0:\n",
    "        sorted_logits, sorted_indices = pt.sort(logits, descending=True)\n",
    "        cumulative_probs = pt.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "        \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gprobs(s, past=None, return_sts=False, tcounts=None, add=0, **kwargs):  # Inference and sampling for tokens\n",
    "    global model\n",
    "    xs, mlen = None, None\n",
    "    if isinstance(s, tuple):    # s either list of token tensors or tuple of preformatted 2d tensors\n",
    "        xs, _, sqlen = s\n",
    "        mlen = max(sqlen)\n",
    "    else:\n",
    "        sqlen = [len(s_) for s_ in s]\n",
    "        mlen = max(sqlen)\n",
    "        xs, _, sqlen = adapt_form([pt.tensor(s_).to(d) for s_ in s], None, sqlen, mlen=mlen)\n",
    "    gc.collect(), pt.cuda.empty_cache()\n",
    "    model.eval()\n",
    "    y_hat = inference(xs, sqlen, seq_maxlen=mlen, add=add, past=past, return_states=return_sts)\n",
    "    if return_sts: y_hat, states = y_hat\n",
    "    y_hat = pt.vstack([F.softmax(top_filtering(y_hat[i], tcounts[i] if tcounts is not None else None,\n",
    "                                               **kwargs), dim=0) for i in range(len(xs))])\n",
    "    return (y_hat, states) if return_sts else y_hat\n",
    "def append_next_token(sent, olen=None, top_k=-1, top_p=0.9, temperature=1.0):  # Interface for field testing\n",
    "    print(\"k =\", top_k, \", p =\", top_p, \", temp =\", temperature)\n",
    "    tokens = tokenizer.encode(sent)\n",
    "    ou = tokens.copy()\n",
    "    tcounts = pt.zeros(N_tokens, dtype=int).to(d)\n",
    "    for token in tokens: tcounts[token] += 1\n",
    "    probs = gprobs([tokens], top_k=top_k, top_p=top_p, temperature=temperature, tcounts=[tcounts])[0]\n",
    "    token = pt.multinomial(probs, 1).detach().cpu().numpy()[0]\n",
    "    ou += [token]\n",
    "    prev_len = len(sent) if olen is None else olen\n",
    "    sent_new = tokenizer.decode(ou)\n",
    "    print(sent[:prev_len] + '➡' + sent_new[prev_len:])\n",
    "    return sent_new\n",
    "def gen_probs(s, **kwargs):  # Adapter for strings\n",
    "    inp = [tokenizer.encode(s_) for s_ in s]\n",
    "    return gprobs(inp, **kwargs)\n",
    "def gen_completions(s, n=1, max_tokens=phrl_max, best_of=1, **kwargs):  # Completion generator equivalent to OpenAI's for GPT3\n",
    "    gpu_multiplier = 1/(bsz if \"xlnet\" in modelkey else 4)\n",
    "    n_bats, best_of, outputs = int(np.ceil(len(s) / int(bsz * gpu_multiplier))), int(round(best_of)), []\n",
    "    if n == 1 and best_of != 1: n, best_of = best_of, n\n",
    "    if best_of == 1 and n != 1: best_of = n\n",
    "    gc.collect()\n",
    "    for i in range(n_bats):\n",
    "        s_batch, tc_b = s[i * int(bsz * gpu_multiplier):(i + 1) * int(bsz * gpu_multiplier)], []\n",
    "        print(s_batch)\n",
    "        for s_ in s_batch:\n",
    "            tc = pt.zeros(N_tokens, dtype=int).to(d)\n",
    "            for t in tokenizer.encode(s_): tc[t] += 1\n",
    "            tc_b.append(tc)\n",
    "        p, sts = gen_probs(s_batch, return_sts=True, tcounts=tc_b, **kwargs)\n",
    "        sql_b = [len(tokenizer.encode(s_)) for s_ in s_batch]\n",
    "        mlen = max(sql_b)\n",
    "        sql_b = pt.tensor(sql_b).to(d)\n",
    "        # First use the (as yet undiverged) token distribution (multinomial) to generate n tokens for each sample\n",
    "        tokens = pt.multinomial(p, n, replacement=True) \n",
    "        outs, avg_logprobs = [], []\n",
    "        for j in range(n):\n",
    "            tks, tc_b_itr = tokens[:, j], [tc_.clone() for tc_ in tc_b]\n",
    "            for j in range(len(tc_b_itr)): tc_b_itr[j][tks[j]] += 1\n",
    "            gc.collect(), pt.cuda.empty_cache()\n",
    "            if multimask_arch:\n",
    "                s_b = [tokenizer.decode(np.hstack([tokenizer.encode(s_batch[i]),\n",
    "                                                   tks[[i]].cpu().detach().numpy()])) for i in range(len(s_batch))]\n",
    "                print(s_b)\n",
    "                p = gen_probs(s_b, tcounts=tc_b_itr, **kwargs)\n",
    "            else:\n",
    "                p, st = gprobs((pt.unsqueeze(tks,-1),None,sql_b), past=sts, return_sts=True, tcounts=tc_b_itr, add=1, **kwargs)\n",
    "            su, ls, out = pt.log(p[pt.arange(p.shape[0]), tks]), pt.ones(p.shape[0]).to(d), [tks.cpu().detach()]\n",
    "            for token_i in range(max_tokens - 1):\n",
    "                t = pt.multinomial(p, 1).to(d)[:, 0]\n",
    "                out.append(t.cpu().detach())\n",
    "                for j in range(len(tc_b_itr)): tc_b_itr[j][t[j]] += 1\n",
    "                cont = t != pad_token\n",
    "                ls += cont.int()\n",
    "                su += cont * pt.log(p[pt.arange(p.shape[0]), t])\n",
    "                if token_i == max_tokens - 1: break\n",
    "                if multimask_arch:\n",
    "                    s_b = [tokenizer.decode(np.hstack([tokenizer.encode(s_b[i]),\n",
    "                                                       t[[i]].cpu().detach().numpy()])) for i in range(len(s_b))]\n",
    "                    print(s_b)\n",
    "                    p = gen_probs(s_b, tcounts=tc_b_itr, **kwargs)\n",
    "                else:\n",
    "                    p,st=gprobs((pt.unsqueeze(t,-1),None,sql_b),past=st,return_sts=True,tcounts=tc_b_itr,add=token_i+2,**kwargs)\n",
    "            outs.append(pt.vstack(out).T), avg_logprobs.append(su / ls)\n",
    "#             return None\n",
    "        gc.collect(), pt.cuda.empty_cache()\n",
    "        outs = pt.stack(outs, 1)\n",
    "        avg_logprobs = pt.vstack(avg_logprobs).T\n",
    "        s1 = outs.shape[0]\n",
    "        idx = pt.argsort(avg_logprobs, axis=1)[:, :best_of].repeat_interleave(max_tokens, 1).reshape(s1, best_of, max_tokens)\n",
    "        outs = pt.gather(outs, 1, idx)\n",
    "        outputs += [[[(tokenizer.decode([x_]),) for x_ in x] for x in o] for o in outs.numpy()]\n",
    "#         outputs += [[x for x in o] for o in outs.cpu().detach().numpy()]\n",
    "    pr([[' '.join([x_[0] for x_ in x]) for x in o] for o in outputs])\n",
    "#     pr([[tokenizer.decode(x) for x in o] for o in outputs])\n",
    "    return outputs\n",
    "mdl = {\"completions\": gen_completions, \"probabilities\": gprobs, \"name\": mname_fn + ',' + modelkey, \"mstr\": str(model)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gprobs(s, past=None, return_sts=False, tcounts=None, add=0, **kwargs):  # Inference and sampling for tokens\n",
    "    global model\n",
    "    xs, mlen = None, None\n",
    "    if isinstance(s, tuple):    # s either list of token tensors or tuple of preformatted 2d tensors\n",
    "        xs, _, sqlen = s\n",
    "        mlen = max(sqlen)\n",
    "    else:\n",
    "        sqlen = [len(s_) for s_ in s]\n",
    "        mlen = max(sqlen)\n",
    "        xs, _, sqlen = adapt_form([pt.tensor(s_).to(d) for s_ in s], None, sqlen, mlen=mlen)\n",
    "    gc.collect(), pt.cuda.empty_cache()\n",
    "    model.eval()\n",
    "    y_hat = inference(xs, sqlen, seq_maxlen=mlen, add=add, past=past, return_states=return_sts)\n",
    "    if return_sts: y_hat, states = y_hat\n",
    "    y_hat = pt.vstack([F.softmax(top_filtering(y_hat[i], tcounts[i] if tcounts is not None else None,\n",
    "                                               **kwargs), dim=0) for i in range(len(xs))])\n",
    "    return (y_hat, states) if return_sts else y_hat\n",
    "def append_next_token(sent, olen=None, top_k=-1, top_p=0.9, temperature=1.0):  # Interface for field testing\n",
    "    print(\"k =\", top_k, \", p =\", top_p, \", temp =\", temperature)\n",
    "    tokens = tokenizer.encode(sent)\n",
    "    ou = tokens.copy()\n",
    "    tcounts = pt.zeros(N_tokens, dtype=int).to(d)\n",
    "    for token in tokens: tcounts[token] += 1\n",
    "    probs = gprobs([tokens], top_k=top_k, top_p=top_p, temperature=temperature, tcounts=[tcounts])[0]\n",
    "    token = pt.multinomial(probs, 1).detach().cpu().numpy()[0]\n",
    "    ou += [token]\n",
    "    prev_len = len(sent) if olen is None else olen\n",
    "    sent_new = tokenizer.decode(ou)\n",
    "    print(sent[:prev_len] + '➡' + sent_new[prev_len:])\n",
    "    return sent_new\n",
    "def gen_probs(s, **kwargs):  # Adapter for strings\n",
    "    inp = [tokenizer.encode(s_) for s_ in s]\n",
    "    return gprobs(inp, **kwargs)\n",
    "def gen_completions(s, n=1, max_tokens=phrl_max, best_of=1, **kwargs):  # Completion generator equivalent to OpenAI's for GPT3\n",
    "    gpu_multiplier = 1/(bsz if \"xlnet\" in modelkey else 4)\n",
    "    n_bats, best_of, outputs = int(np.ceil(len(s) / int(bsz * gpu_multiplier))), int(round(best_of)), []\n",
    "    if n == 1 and best_of != 1: n, best_of = best_of, n\n",
    "    if best_of == 1 and n != 1: best_of = n\n",
    "    gc.collect()\n",
    "    for i in range(n_bats):\n",
    "        s_batch, tc_b = s[i * int(bsz * gpu_multiplier):(i + 1) * int(bsz * gpu_multiplier)], []\n",
    "        print(s_batch)\n",
    "        for s_ in s_batch:\n",
    "            tc = pt.zeros(N_tokens, dtype=int).to(d)\n",
    "            for t in tokenizer.encode(s_): tc[t] += 1\n",
    "            tc_b.append(tc)\n",
    "        p, sts = gen_probs(s_batch, return_sts=True, tcounts=tc_b, **kwargs)\n",
    "        sql_b = [len(tokenizer.encode(s_)) for s_ in s_batch]\n",
    "        mlen = max(sql_b)\n",
    "        sql_b = pt.tensor(sql_b).to(d)\n",
    "        # First use the (as yet undiverged) token distribution (multinomial) to generate n tokens for each sample\n",
    "        tokens = pt.multinomial(p, n, replacement=True) \n",
    "        outs, avg_logprobs = [], []\n",
    "        for j in range(n):\n",
    "            tks, tc_b_itr = tokens[:, j], [tc_.clone() for tc_ in tc_b]\n",
    "            for j in range(len(tc_b_itr)): tc_b_itr[j][tks[j]] += 1\n",
    "            gc.collect(), pt.cuda.empty_cache()\n",
    "            if multimask_arch:\n",
    "                s_b = [tokenizer.decode(np.hstack([tokenizer.encode(s_batch[i]),\n",
    "                                                   tks[[i]].cpu().detach().numpy()])) for i in range(len(s_batch))]\n",
    "                print(s_b)\n",
    "                p = gen_probs(s_b, tcounts=tc_b_itr, **kwargs)\n",
    "            else:\n",
    "                p, st = gprobs((pt.unsqueeze(tks,-1),None,sql_b), past=sts, return_sts=True, tcounts=tc_b_itr, add=1, **kwargs)\n",
    "            su, ls, out = pt.log(p[pt.arange(p.shape[0]), tks]), pt.ones(p.shape[0]).to(d), [tks]\n",
    "            for token_i in range(max_tokens - 1):\n",
    "                t = pt.multinomial(p, 1).to(d)[:, 0]\n",
    "                out.append(t)\n",
    "                for j in range(len(tc_b_itr)): tc_b_itr[j][t[j]] += 1\n",
    "                cont = t != pad_token\n",
    "                ls += cont.int()\n",
    "                su += cont * pt.log(p[pt.arange(p.shape[0]), t])\n",
    "                if token_i == max_tokens - 1: break\n",
    "                if multimask_arch:\n",
    "                    s_b = [tokenizer.decode(np.hstack([tokenizer.encode(s_b[i]),\n",
    "                                                       t[[i]].cpu().detach().numpy()])) for i in range(len(s_b))]\n",
    "                    print(s_b)\n",
    "                    p = gen_probs(s_b, tcounts=tc_b_itr, **kwargs)\n",
    "                else:\n",
    "                    p,st=gprobs((pt.unsqueeze(t,-1),None,sql_b),past=st,return_sts=True,tcounts=tc_b_itr,add=token_i+2,**kwargs)\n",
    "            outs.append(pt.vstack(out).T), avg_logprobs.append(su / ls)\n",
    "#             return None\n",
    "        gc.collect(), pt.cuda.empty_cache()\n",
    "        outs = pt.stack(outs, 1)\n",
    "        avg_logprobs = pt.vstack(avg_logprobs).T\n",
    "        s1 = outs.shape[0]\n",
    "        idx = pt.argsort(avg_logprobs, axis=1)[:, :best_of].repeat_interleave(max_tokens, 1).reshape(s1, best_of, max_tokens)\n",
    "        outs = pt.gather(outs, 1, idx)\n",
    "        outputs += [[[(tokenizer.decode([x_]),) for x_ in x] for x in o] for o in outs.cpu().detach().numpy()]\n",
    "#         outputs += [[x for x in o] for o in outs.cpu().detach().numpy()]\n",
    "    pr([[' '.join([x_[0] for x_ in x]) for x in o] for o in outputs])\n",
    "#     pr([[tokenizer.decode(x) for x in o] for o in outputs])\n",
    "    return outputs\n",
    "mdl = {\"completions\": gen_completions, \"probabilities\": gprobs, \"name\": mname_fn + ',' + modelkey, \"mstr\": str(model)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | freque... | presen... | temper... |   top_p   |\n",
      "-------------------------------------------------------------------------\n",
      "{   'best_of': 1.0,\n",
      "    'frequency_penalty': 0.04738827217229063,\n",
      "    'presence_penalty': 0.0744755794115483,\n",
      "    'temperature': 1.3101102471365533,\n",
      "    'top_p': 0.9903237363946299}\n",
      "['A list of construction sound:']\n",
      "['A list of construction sound::']\n",
      "['A list of construction sound:::']\n",
      "['A list of construction sound::::']\n",
      "['A list of construction sound:::::']\n",
      "['A list of construction sound::::::']\n",
      "['A list of construction sound::']\n",
      "['A list of construction sound:::']\n",
      "['A list of construction sound::::']\n",
      "['A list of construction sound:::::']\n",
      "['A list of construction sound::::: and']\n",
      "['A list of construction sound::']\n",
      "['A list of construction sound:::']\n",
      "['A list of construction sound::::']\n",
      "['A list of construction sound:::::']\n",
      "['A list of construction sound::::::']\n",
      "['A list of construction sound::']\n",
      "['A list of construction sound:::']\n",
      "['A list of construction sound::::']\n",
      "['A list of construction sound:::::']\n",
      "['A list of construction sound::::::']\n",
      "['A list of construction sound::']\n",
      "['A list of construction sound:: The']\n",
      "['A list of construction sound:: The<eop>']\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 6.00 GiB total capacity; 1.87 GiB already allocated; 0 bytes free; 2.02 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\bayes_opt\\target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[1;34m(self, params)\u001b[0m\n\u001b[0;32m    190\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 191\u001b[1;33m             \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0m_hashable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    192\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: (0.04738827217229063, 0.0744755794115483, 1.3101102471365533, 0.9903237363946299)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-d10e99096fc8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0moptimizers_modelf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBayesianOptimization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfun\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpbounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbounds_modelf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m#     optimizers_modelf[-1].probe(params={\"temperature\": 1.0, \"top_p\": 1.0, \"presence_penalty\": 0.0})\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0moptimizers_modelf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_points\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0mresults_modelf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizers_modelf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\bayes_opt\\bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[1;34m(self, init_points, n_iter, acq, kappa, xi, **gp_params)\u001b[0m\n\u001b[0;32m    172\u001b[0m                 \u001b[0miteration\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_probe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlazy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOPTMIZATION_END\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\bayes_opt\\bayesian_optimization.py\u001b[0m in \u001b[0;36mprobe\u001b[1;34m(self, params, lazy)\u001b[0m\n\u001b[0;32m    110\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOPTMIZATION_STEP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\bayes_opt\\target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[1;34m(self, params)\u001b[0m\n\u001b[0;32m    192\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m             \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 194\u001b[1;33m             \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    195\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-27-d10e99096fc8>\u001b[0m in \u001b[0;36mfun\u001b[1;34m(temperature, top_p, presence_penalty, frequency_penalty)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"best_of\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mpr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtest_sp_conv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_l\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_l\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_l\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_l\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.005\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphase\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"val\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muniform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmdl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmdl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprmt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"A list of\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[0moptimizers_modelf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBayesianOptimization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfun\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpbounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbounds_modelf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m#     optimizers_modelf[-1].probe(params={\"temperature\": 1.0, \"top_p\": 1.0, \"presence_penalty\": 0.0})\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-2887af08840a>\u001b[0m in \u001b[0;36mtest_sp_conv\u001b[1;34m(params, tol, **kwargs)\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[0mcenter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_nochange\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m         \u001b[0mys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_sp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m         \u001b[0mnew_center\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mys\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcenter\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mnew_center\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0msteps_nochange\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-2887af08840a>\u001b[0m in \u001b[0;36mtest_sp\u001b[1;34m(params, min_l, max_l, n1, n2, prmt, phase, uniform, max_tokens, mdl, mdl2, d, d_test, inds, inds_test, m2ensemble_frac, return_test_acc)\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mdefault_sp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'n'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mn2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'max_tokens'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmax_tokens\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m}\u001b[0m \u001b[1;31m#**default_msp,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmdl\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'gpt3'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mp_req_m\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# Request predictions from OpenAI\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m             \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmdl\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"completions\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0md_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m     \u001b[0msave_modeloutput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphaseIx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsps_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_l\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_l\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmdl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmdl2\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-8c5d9e992a5f>\u001b[0m in \u001b[0;36mgen_completions\u001b[1;34m(s, n, max_tokens, best_of, **kwargs)\u001b[0m\n\u001b[0;32m     76\u001b[0m                                                        t[[i]].cpu().detach().numpy()])) for i in range(len(s_b))]\n\u001b[0;32m     77\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms_b\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m                     \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_probs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms_b\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtcounts\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtc_b_itr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m                     \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mst\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgprobs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msql_b\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpast\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mst\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreturn_sts\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtcounts\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtc_b_itr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtoken_i\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-8c5d9e992a5f>\u001b[0m in \u001b[0;36mgen_probs\u001b[1;34m(s, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgen_probs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Adapter for strings\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0minp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgprobs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgen_completions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mphrl_max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_of\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Completion generator equivalent to OpenAI's for GPT3\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0mgpu_multiplier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbsz\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;34m\"xlnet\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodelkey\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-8c5d9e992a5f>\u001b[0m in \u001b[0;36mgprobs\u001b[1;34m(s, past, return_sts, tcounts, add, **kwargs)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0my_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msqlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_maxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpast\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpast\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_sts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreturn_sts\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my_hat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_hat\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     y_hat = pt.vstack([F.softmax(top_filtering(y_hat[i], tcounts[i] if tcounts is not None else None,\n",
      "\u001b[1;32m<ipython-input-21-4f956b10a533>\u001b[0m in \u001b[0;36minference\u001b[1;34m(x, sqlens, past, return_states, seq_maxlen, add, return_fulloutput)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[1;32mglobal\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[0mmany_inp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmultimask_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mlocals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mmultimask_arch\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msinglemask_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mlocals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreturn_fulloutput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mmultimask_arch\u001b[0m \u001b[1;32melse\u001b[0m             \u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msqlens\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mmany_inp\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-4f956b10a533>\u001b[0m in \u001b[0;36mmultimask_model\u001b[1;34m(model, x, sqlens, past, seq_maxlen, add, many_inp, return_states, **kw)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;31m#     print(perm_mask, target_mapping)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     res = model(x, perm_mask=perm_mask if attn_mask is None else None, target_mapping=target_mapping,\n\u001b[1;32m---> 61\u001b[1;33m                    mems=past, attention_mask=attn_mask, use_mems=None if (past is None and not return_states) else True)\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;31m#     print(res[0].shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\transformers\\models\\xlnet\\modeling_xlnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, mems, perm_mask, target_mapping, token_type_ids, input_mask, head_mask, inputs_embeds, labels, use_mems, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   1444\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1445\u001b[0m             \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1446\u001b[1;33m             \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1447\u001b[0m         )\n\u001b[0;32m   1448\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\transformers\\models\\xlnet\\modeling_xlnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, mems, perm_mask, target_mapping, token_type_ids, input_mask, head_mask, inputs_embeds, use_mems, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   1256\u001b[0m                 \u001b[0mtarget_mapping\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtarget_mapping\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1257\u001b[0m                 \u001b[0mhead_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1258\u001b[1;33m                 \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1259\u001b[0m             )\n\u001b[0;32m   1260\u001b[0m             \u001b[0moutput_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_g\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\transformers\\models\\xlnet\\modeling_xlnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, output_h, output_g, attn_mask_h, attn_mask_g, r, seg_mat, mems, target_mapping, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[0mtarget_mapping\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtarget_mapping\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    524\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 525\u001b[1;33m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    526\u001b[0m         )\n\u001b[0;32m    527\u001b[0m         \u001b[0moutput_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_g\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\transformers\\models\\xlnet\\modeling_xlnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, h, g, attn_mask_h, attn_mask_g, r, seg_mat, mems, target_mapping, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m             \u001b[1;31m# post processing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 384\u001b[1;33m             \u001b[0moutput_h\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpost_attention\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattn_vec_h\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    385\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m             \u001b[1;31m# g-stream\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\transformers\\models\\xlnet\\modeling_xlnet.py\u001b[0m in \u001b[0;36mpost_attention\u001b[1;34m(self, h, attn_vec, residual)\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[1;34m\"\"\"Post-attention processing.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m         \u001b[1;31m# post-attention projection (back to `d_model`)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m         \u001b[0mattn_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ibnd,hnd->ibh\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattn_vec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m         \u001b[0mattn_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattn_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\torch\\functional.py\u001b[0m in \u001b[0;36meinsum\u001b[1;34m(equation, *operands)\u001b[0m\n\u001b[0;32m    406\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0meinsum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0m_operands\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 408\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moperands\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    409\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 6.00 GiB total capacity; 1.87 GiB already allocated; 0 bytes free; 2.02 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "# optimise fine-tuned model sampling params\n",
    "lgroups_ft = [[4, 5]]\n",
    "bounds_modelf = {\n",
    "  \"temperature\": [0.99, 2.0],  # a temp of 0 selects the top completion every time due to the infinitely larger logit\n",
    "  \"top_p\": [0.99, 1.0],        # same with this but more obvious\n",
    "#   \"top_k\": [1, 100],        # fix this to be a log of the input value so that lower values are sampled with high resolution?\n",
    "  \"presence_penalty\": [0.0, 0.1],   # both presence and frequency penalty have optimal values\n",
    "  \"frequency_penalty\": [0.0, 0.1],  # this can also go down to -2.0, probably not useful for our case...\n",
    "#   \"best_of\": [0.51, 5.49], # to do\n",
    "}\n",
    "optimizers_modelf, results_modelf = [], []\n",
    "for lgroup in lgroups_ft:\n",
    "    min_l, max_l = min(lgroup) - 1, max(lgroup)\n",
    "    def fun(temperature, top_p, presence_penalty, frequency_penalty):\n",
    "        global min_l, max_l\n",
    "        ps = locals()\n",
    "        ps[\"best_of\"] = 1.0\n",
    "        pr(ps)\n",
    "        return test_sp_conv(ps, min_l=min_l, max_l=max_l, tol=0.005, phase=\"val\", uniform=True, mdl=mdl, n2=20, max_tokens=5, prmt=\"A list of\")[0]\n",
    "    optimizers_modelf.append(BayesianOptimization(f=fun, pbounds=bounds_modelf, verbose=1000))\n",
    "#     optimizers_modelf[-1].probe(params={\"temperature\": 1.0, \"top_p\": 1.0, \"presence_penalty\": 0.0})\n",
    "    optimizers_modelf[-1].maximize(init_points=5, n_iter=10)\n",
    "    results_modelf.append(optimizers_modelf[-1].max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLNetTokenizer\n",
    "tokenizerr = XLNetTokenizer.from_pretrained('xlnet-large-cased')\n",
    "# modell = XLNetLMHeadModel.from_pretrained('xlnet-large-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 6.00 GiB total capacity; 1.89 GiB already allocated; 0 bytes free; 2.04 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-fbaad2744a8d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mtarget_mapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.0\u001b[0m  \u001b[1;31m# Our first (and only) prediction will be the last token of the sequence (the masked token)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mperm_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mperm_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_mapping\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtarget_mapping\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mnext_token_logits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# Output has shape [target_mapping.size(0), target_mapping.size(1), config.vocab_size]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\transformers\\models\\xlnet\\modeling_xlnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, mems, perm_mask, target_mapping, token_type_ids, input_mask, head_mask, inputs_embeds, labels, use_mems, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   1444\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1445\u001b[0m             \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1446\u001b[1;33m             \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1447\u001b[0m         )\n\u001b[0;32m   1448\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\transformers\\models\\xlnet\\modeling_xlnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, mems, perm_mask, target_mapping, token_type_ids, input_mask, head_mask, inputs_embeds, use_mems, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   1256\u001b[0m                 \u001b[0mtarget_mapping\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtarget_mapping\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1257\u001b[0m                 \u001b[0mhead_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1258\u001b[1;33m                 \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1259\u001b[0m             )\n\u001b[0;32m   1260\u001b[0m             \u001b[0moutput_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_g\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\transformers\\models\\xlnet\\modeling_xlnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, output_h, output_g, attn_mask_h, attn_mask_g, r, seg_mat, mems, target_mapping, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[0mtarget_mapping\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtarget_mapping\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    524\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 525\u001b[1;33m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    526\u001b[0m         )\n\u001b[0;32m    527\u001b[0m         \u001b[0moutput_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_g\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\transformers\\models\\xlnet\\modeling_xlnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, h, g, attn_mask_h, attn_mask_g, r, seg_mat, mems, target_mapping, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m             \u001b[1;31m# post processing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 384\u001b[1;33m             \u001b[0moutput_h\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpost_attention\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattn_vec_h\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    385\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m             \u001b[1;31m# g-stream\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\transformers\\models\\xlnet\\modeling_xlnet.py\u001b[0m in \u001b[0;36mpost_attention\u001b[1;34m(self, h, attn_vec, residual)\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[1;34m\"\"\"Post-attention processing.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m         \u001b[1;31m# post-attention projection (back to `d_model`)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m         \u001b[0mattn_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ibnd,hnd->ibh\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattn_vec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m         \u001b[0mattn_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattn_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\torch\\functional.py\u001b[0m in \u001b[0;36meinsum\u001b[1;34m(equation, *operands)\u001b[0m\n\u001b[0;32m    406\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0meinsum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0m_operands\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 408\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moperands\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    409\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 6.00 GiB total capacity; 1.89 GiB already allocated; 0 bytes free; 2.04 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "# We show how to setup inputs to predict a next token using a bi-directional context.\n",
    "# input_ids = pt.tensor(tokenizerr.encode(\"Hello, my dog is very<mask>\", add_special_tokens=False)).unsqueeze(0).to(d)\n",
    "# input_ids = pt.tensor(tokenizerr.encode(\"A list of common types of wild animal:<mask>\", add_special_tokens=False)).unsqueeze(0).to(d)\n",
    "# input_ids = pt.tensor(tokenizerr.encode(\"The road was long, but not the<mask>\", add_special_tokens=False)).unsqueeze(0).to(d)\n",
    "input_ids = pt.tensor(tokenizerr.encode(\"A list of construction sounds:<mask>\", add_special_tokens=False)).unsqueeze(0).to(d)\n",
    "perm_mask = pt.zeros((1, input_ids.shape[1], input_ids.shape[1]), dtype=pt.float).to(d)\n",
    "perm_mask[:, :, -1] = 1.0  # Previous tokens don't see last token\n",
    "target_mapping = pt.zeros((1, 1, input_ids.shape[1]), dtype=pt.float).to(d) # Shape [1, 1, seq_length] => let's predict one token\n",
    "target_mapping[0, 0, -1] = 1.0  # Our first (and only) prediction will be the last token of the sequence (the masked token)\n",
    "\n",
    "outputs = model(input_ids, perm_mask=perm_mask, target_mapping=target_mapping)\n",
    "next_token_logits = outputs[0]  # Output has shape [target_mapping.size(0), target_mapping.size(1), config.vocab_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 6.00 GiB total capacity; 1.89 GiB already allocated; 0 bytes free; 2.04 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-80cba40c378b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtokenizerr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_token_logits\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 6.00 GiB total capacity; 1.89 GiB already allocated; 0 bytes free; 2.04 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "tokenizerr.decode(pt.argsort(next_token_logits[0, 0])[-50:].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-16.193918,\n",
       " -16.183165,\n",
       " -16.122074,\n",
       " -16.08592,\n",
       " -16.043228,\n",
       " -16.03282,\n",
       " -16.0242,\n",
       " -16.00353,\n",
       " -15.979343,\n",
       " -15.977925,\n",
       " -15.936374,\n",
       " -15.922286,\n",
       " -15.9158,\n",
       " -15.909621,\n",
       " -15.875632,\n",
       " -15.871074,\n",
       " -15.861634,\n",
       " -15.825805,\n",
       " -15.774126,\n",
       " -15.554242,\n",
       " -15.54916,\n",
       " -15.542339,\n",
       " -15.379066,\n",
       " -15.3337,\n",
       " -15.333503,\n",
       " -15.292143,\n",
       " -15.264986,\n",
       " -15.20713,\n",
       " -15.105776,\n",
       " -15.044917,\n",
       " -14.981879,\n",
       " -14.973442,\n",
       " -14.910143,\n",
       " -14.671865,\n",
       " -14.574007,\n",
       " -14.510053,\n",
       " -14.498207,\n",
       " -14.486406,\n",
       " -14.438374,\n",
       " -14.284411,\n",
       " -14.193142,\n",
       " -14.12361,\n",
       " -14.119947,\n",
       " -13.70911,\n",
       " -13.136736,\n",
       " -13.095249,\n",
       " -12.753677,\n",
       " -12.56003,\n",
       " -11.044312,\n",
       " -10.502604]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(pt.sort(next_token_logits[0, 0])[0][-50:].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | freque... | presen... | temper... |   top_p   |\n",
      "-------------------------------------------------------------------------\n",
      "['An exhaustive list of some types of construction sound:']\n",
      "['A list of common types of hats:']\n",
      "['A list of some animals seen in the wild:']\n",
      "['A list of types of  wood:']\n",
      "['A list of some types of winds:']\n",
      "['An exhaustive list of different types of physical tokens of trust:']\n",
      "['A list of different types of timber:']\n",
      "['An exhaustive list of well-known digital tokens that confer trust:']\n",
      "[   [' . . . . . . . . . .', ' . . . . . . . . . .'],\n",
      "    [':  Hip ly  \" \" \" \" \" \"', ':  Hip ly  \" \" \" \" \" \"'],\n",
      "    [' . . . . . . . . . .', ' . . . . . . . . . .'],\n",
      "    ['. . . . . . . . . . .', '. . . . . . . . . . .'],\n",
      "    [' . . . . . . . . . .', ' . . . . . . . . . .'],\n",
      "    [': : : . . . . . . . .', ': : : . . . . . . . .'],\n",
      "    ['. . . . . . . . . . .', '. . . . . . . . . . .'],\n",
      "    ['<eop> Any Ale Ale , , , , , , ,', '<eop> Any Ale Ale , , , , , , ,']]\n",
      "['A list of different types of chemical element:']\n",
      "['A list of types of  essential element to successful drama and storytelling:']\n",
      "['A list of some types of vehicles referred to as crafts:']\n",
      "['A list of common types of music genre:']\n",
      "['A list of different types of cyclic system:']\n",
      "['A list of some types of AI algorithm:']\n",
      "['A list of common types of thing made of glass:']\n",
      "['A long list of some types of coil winding:']\n",
      "[   [' . . . . . . . . . .', ' . . . . . . . . . .'],\n",
      "    ['a  lice ly ly ly . . . . .', 'a  lice ly ly ly . . . . .'],\n",
      "    ['here . . . . . . . . . .', 'here . . . . . . . . . .'],\n",
      "    [   ' ken ken ea ita ita ita ita Against Against Against',\n",
      "        ' ken ken ea ita ita ita ita Against Against Against'],\n",
      "    [': : : , 3 . . . . . .', ': : : , 3 . . . . . .'],\n",
      "    [   ': : leans leans An An An An An An An',\n",
      "        ': : leans leans An An An An An An An'],\n",
      "    [' . . . . . . . . . .', ' . . . . . . . . . .'],\n",
      "    [': . . . . . . . . . .', ': . . . . . . . . . .']]\n",
      "['An exhaustive list of some types of noise of building:']\n",
      "['A long list of hats:']\n",
      "['A long list of some types of wild animals:']\n",
      "['An exhaustive list of different types of woods:']\n",
      "['An exhaustive list of some types of wind:']\n",
      "['A list of some types of physical objects of trust:']\n",
      "['An exhaustive list of some types of lumber:']\n",
      "['A list of well-known types of digital token that can confer trust:']\n",
      "[   [' . . . . . . . . . .', ' . . . . . . . . . .'],\n",
      "    ['a a a a an  . . . . .', 'a a a a an  . . . . .'],\n",
      "    [' . . . . . . . . . .', ' . . . . . . . . . .'],\n",
      "    [' . . . . . . . . . .', ' . . . . . . . . . .'],\n",
      "    [': . . . . . . . . . .', ': . . . . . . . . . .'],\n",
      "    [' . . . . . . . . . .', ' . . . . . . . . . .'],\n",
      "    [': ly  . . . . . . . .', ': ly  . . . . . . . .'],\n",
      "    [' . . . . . . . . . .', ' . . . . . . . . . .']]\n",
      "['A long list of common types of periodic element:']\n",
      "['An exhaustive list of different types of element of drama and writing:']\n",
      "['An exhaustive list of common types of craft:']\n",
      "['A list of some types of classification of music:']\n",
      "['A list of well-known types of cyclic phenomena:']\n",
      "['A long list of ML algorithms:']\n",
      "['A long list of different types of glass things:']\n",
      "['A long list of some types of winding:']\n",
      "[   [' . . . . . . . . . .', ' . . . . . . . . . .'],\n",
      "    [' . . . . . . . . . .', ' . . . . . . . . . .'],\n",
      "    [   'once every an an an an an an . . .',\n",
      "        'once every an an an an an an . . .'],\n",
      "    [': : : , . . . . . . .', ': : : , . . . . . . .'],\n",
      "    [':  Piece  . . . . . . .', ':  Piece  . . . . . . .'],\n",
      "    ['<eop> A . . . . . . . . .', '<eop> A . . . . . . . . .'],\n",
      "    [' . . . . . . . . . .', ' . . . . . . . . . .'],\n",
      "    [' <unk> n n . . . . . . .', ' <unk> n n . . . . . . .']]\n",
      "['An exhaustive list of common types of construction noise:']\n",
      "['A list of types of  hat:']\n",
      "['Well-known animals seen in the wild:']\n",
      "['A list of well-known woods:']\n",
      "['A list of different winds:']\n",
      "['A list of different types of object that confer trust:']\n",
      "['A list of common types of timbers:']\n",
      "['An exhaustive list of well-known types of digital token of trust:']\n",
      "[   [' . . . . <eop> \" \" \" \" \"', ' . . . . <eop> \" \" \" \" \"'],\n",
      "    ['a an an an an an an An  . .', 'a an an an an an an An  . .'],\n",
      "    [' . . . . . . . . . .', ' . . . . . . . . . .'],\n",
      "    ['a a a a a a a a a a A', 'a a a a a a a a a a A'],\n",
      "    ['a a a a  ged ge ge e e E', 'a a a a  ged ge ge e e E'],\n",
      "    [   '. <eop> Not ed ed ed ed ed ed zed ,',\n",
      "        '. <eop> Not ed ed ed ed ed ed zed ,'],\n",
      "    ['- - \" \" \" \" \" \" \" \" \"', '- - \" \" \" \" \" \" \" \" \"'],\n",
      "    [': : . . . . . . . . .', ': : . . . . . . . . .']]\n",
      "['An exhaustive list of well-known types of periodic table element:']\n",
      "['A long list of different dramatic and literature elements:']\n",
      "['A list of different types of crafts:']\n",
      "['An exhaustive list of common types of music genre:']\n",
      "['A list of common types of scientific cycles:']\n",
      "['An exhaustive list of common ML models:']\n",
      "['A list of some types of glass object:']\n",
      "['A long list of different types of electromagnetic coil winding:']\n",
      "[   [': : , 4 5 6 7 8  . .', ': : , 4 5 6 7 8  . .'],\n",
      "    [':  . . . . . . . . .', ':  . . . . . . . . .'],\n",
      "    [':  . . . . . . . . .', ':  . . . . . . . . .'],\n",
      "    [' - - - - . . . . . .', ' - - - - . . . . . .'],\n",
      "    [' . . . . . . . . . .', ' . . . . . . . . . .'],\n",
      "    [', , I I I I I I I I I', ', , I I I I I I I I I'],\n",
      "    [':  ( )  . . . . . .', ':  ( )  . . . . . .'],\n",
      "    [': : . . . . . . . . .', ': : . . . . . . . . .']]\n",
      "['An exhaustive list of well-known types of construction noise:']\n",
      "['An exhaustive list of well-known types of hat:']\n",
      "['A long list of common types of wild animal:']\n",
      "['An exhaustive list of common types of woodland ecoregion:']\n",
      "['A long list of common winds:']\n",
      "['A long list of physical objects of trust:']\n",
      "['A long list of well-known types of wood:']\n",
      "['An exhaustive list of common digital tokens that can confer trust:']\n",
      "[   [' . . . . . . . . . .', ' . . . . . . . . . .'],\n",
      "    [   ': :  <unk> <unk> <unk> <unk> ′ ′ ′ ′',\n",
      "        ': :  <unk> <unk> <unk> <unk> ′ ′ ′ ′'],\n",
      "    [' . . . . . . . . . .', ' . . . . . . . . . .'],\n",
      "    [   ': : Base Four Three Fifty Two One Face Face ,',\n",
      "        ': : Base Four Three Fifty Two One Face Face ,'],\n",
      "    ['<eop> Any An And And And  . . . .', '<eop> Any An And And And  . . . .'],\n",
      "    [' . . . . . . . . . .', ' . . . . . . . . . .'],\n",
      "    ['like like like . . . . . . . .', 'like like like . . . . . . . .'],\n",
      "    ['. . . . . . . . . . .', '. . . . . . . . . . .']]\n",
      "['A list of different types of element of the periodic table:']\n",
      "['A long list of well-known types of element of drama and storywriting:']\n",
      "['A long list of some crafts:']\n",
      "['A long list of different types of music type:']\n",
      "['Well-known cyclic effects:']\n",
      "['A list of well-known types of artificial intelligence algorithms:']\n",
      "['A list of well-known things made of glass:']\n",
      "['An exhaustive list of different types of wind:']\n",
      "[   [':  . . . . . . . . .', ':  . . . . . . . . .'],\n",
      "    [   'each a short very short very very very very very very',\n",
      "        'each a short very short very very very very very very'],\n",
      "    ['. . . . . . . . . . .', '. . . . . . . . . . .'],\n",
      "    [' . . . . . . . . . .', ' . . . . . . . . . .'],\n",
      "    [   'best most most top most most most best best best best',\n",
      "        'best most most top most most most best best best best'],\n",
      "    [': :  strand ly ly ly . . . .', ': :  strand ly ly ly . . . .'],\n",
      "    [' . . . . . . . . . .', ' . . . . . . . . . .'],\n",
      "    [' . . . . . . . . . .', ' . . . . . . . . . .']]\n",
      "['A list of common noises of building:']\n",
      "['A list of types of  hat:']\n",
      "['A list of some types of animals found in the wild:']\n",
      "['A list of different types of wood:']\n",
      "['A list of some types of winds:']\n",
      "['An exhaustive list of different types of physical object that can confer trust:']\n",
      "['An exhaustive list of common types of lumber:']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/bayes_opt/target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: (0.20541783713361572, 1.5689387028580402, 0.2661199454778089, 0.2665727715909656)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-111-6c259ec353a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0moptimizers_modelf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBayesianOptimization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpbounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbounds_modelf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m#     optimizers_modelf[-1].probe(params={\"temperature\": 1.0, \"top_p\": 1.0, \"presence_penalty\": 0.0})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0moptimizers_modelf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_points\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mresults_modelf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizers_modelf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[0;34m(self, init_points, n_iter, acq, kappa, kappa_decay, kappa_decay_delay, xi, **gp_params)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0miteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_probe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlazy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bounds_transformer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mprobe\u001b[0;34m(self, params, lazy)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOPTIMIZATION_STEP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/bayes_opt/target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-111-6c259ec353a2>\u001b[0m in \u001b[0;36mfun\u001b[0;34m(temperature, top_p, presence_penalty, frequency_penalty)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"best_of\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtest_sp_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_l\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_l\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.005\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"val\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muniform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmdl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmdl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0moptimizers_modelf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBayesianOptimization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpbounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbounds_modelf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m#     optimizers_modelf[-1].probe(params={\"temperature\": 1.0, \"top_p\": 1.0, \"presence_penalty\": 0.0})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-19d4fbcb881f>\u001b[0m in \u001b[0;36mtest_sp_conv\u001b[0;34m(params, tol, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0mcenter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_nochange\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0mnew_center\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcenter\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnew_center\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msteps_nochange\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-19d4fbcb881f>\u001b[0m in \u001b[0;36mtest_sp\u001b[0;34m(params, min_l, max_l, n1, n2, prmt, phase, uniform, max_tokens, mdl, mdl2, d, d_test, inds, inds_test, m2ensemble_frac, return_test_acc)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdefault_sp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'n'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mn2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'max_tokens'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmax_tokens\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;31m#**default_msp,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmdl\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'gpt3'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp_req_m\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Request predictions from OpenAI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmdl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"completions\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m     \u001b[0msave_modeloutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphaseIx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsps_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmdl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmdl2\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-108-d5332a5be5f9>\u001b[0m in \u001b[0;36mgen_completions\u001b[0;34m(s, n, max_tokens, best_of, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0msu\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcont\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtoken_i\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmax_tokens\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgprobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msql_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mst\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreturn_sts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtcounts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtc_b_itr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_i\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0mouts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_logprobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msu\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-108-d5332a5be5f9>\u001b[0m in \u001b[0;36mgprobs\u001b[0;34m(s, past, return_sts, tcounts, add, **kwargs)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msqlen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapt_form\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msqlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msqlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_maxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_sts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_sts\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_hat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# optimise fine-tuned model sampling params\n",
    "lgroups_ft = [[4, 5]]\n",
    "bounds_modelf = {\n",
    "  \"temperature\": [0.0001, 1.0],  # a temp of 0 selects the top completion every time due to the infinitely larger logit\n",
    "  \"top_p\": [0.0001, 1.0],        # same with this but more obvious\n",
    "#   \"top_k\": [1, 100],        # fix this to be a log of the input value so that lower values are sampled with high resolution?\n",
    "  \"presence_penalty\": [0.0, 2.0],   # both presence and frequency penalty have optimal values\n",
    "  \"frequency_penalty\": [0.0, 2.0],  # this can also go down to -2.0, probably not useful for our case...\n",
    "#   \"best_of\": [0.51, 5.49], # to do\n",
    "}\n",
    "optimizers_modelf, results_modelf = [], []\n",
    "for lgroup in lgroups_ft:\n",
    "    min_l, max_l = min(lgroup) - 1, max(lgroup)\n",
    "    def fun(temperature, top_p, presence_penalty, frequency_penalty):\n",
    "        global min_l, max_l\n",
    "        ps = locals()\n",
    "        ps[\"best_of\"] = 1.0\n",
    "        return test_sp_conv(ps, min_l=min_l, max_l=max_l, tol=0.005, phase=\"val\", uniform=True, mdl=mdl, n2=2)[0]\n",
    "    optimizers_modelf.append(BayesianOptimization(f=fun, pbounds=bounds_modelf, verbose=1000))\n",
    "#     optimizers_modelf[-1].probe(params={\"temperature\": 1.0, \"top_p\": 1.0, \"presence_penalty\": 0.0})\n",
    "    optimizers_modelf[-1].maximize(init_points=5, n_iter=10)\n",
    "    results_modelf.append(optimizers_modelf[-1].max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizers_modelf[-1].max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # optimise fine-tuned model params with refined bounds\n",
    "# lgroups_ft = [[4, 5]]\n",
    "# bounds_gptxlf_rf = {\n",
    "#   \"temperature\": [0.0001, 0.003],  # a temp of 0 selects the top completion every time due to the infinitely larger logit\n",
    "#   \"top_p\": [0.001, 1.0],        # same with this but more obvious\n",
    "# #   \"top_k\": [1, 100],        # fix this to be a log of the input value so that lower values are sampled with high resolution?\n",
    "#   \"presence_penalty\": [0.1, 1.0],   # both presence and frequency penalty have optimal values\n",
    "# #   \"frequency_penalty\": [0.0, 1.0],  # this can also go down to -2.0, probably not useful for our case...\n",
    "# #   \"best_of\": [0.51, 5.49], # to do\n",
    "# }\n",
    "# optimizers_gpt2xlf_rf, results_gpt2xlf_rf = [], []\n",
    "# for lgroup in lgroups_ft:\n",
    "#     min_l, max_l = min(lgroup) - 1, max(lgroup)\n",
    "#     def fun(temperature, top_p, presence_penalty):\n",
    "#         global min_l, max_l\n",
    "#         ps = locals()\n",
    "#         ps[\"best_of\"] = 5.0\n",
    "#         return test_sp_conv(ps, min_l=min_l, max_l=max_l, tol=0.002, phase=\"val\", uniform=True, mdl=mdl)[0]\n",
    "#     optimizers_gpt2xlf_rf.append(BayesianOptimization(f=fun, pbounds=bounds_gptxlf_rf, verbose=1000))\n",
    "#     optimizers_gpt2xlf_rf[-1].probe(params={\"temperature\": 0.001, \"top_p\": 0.001, \"presence_penalty\": 0.4052})\n",
    "#     optimizers_gpt2xlf_rf[-1].maximize(init_points=2, n_iter=8)\n",
    "#     results_gpt2xlf_rf.append(optimizers_gpt2xlf_rf[-1].max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizers_gpt2xlf_rf[-1].probe(params={\"temperature\": 0.001, \"top_p\": 0.001, \"presence_penalty\": 0.4052})\n",
    "# optimizers_gpt2xlf_rf[-1].maximize(n_iter=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizers_gpt2xlf[-1].max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load predictions from the top performing fine-tuned model, and create the ensemble mdl object\n",
    "# optimal_params = {\"temperature\": ?, \"top_p\": ?, \"presence_penalty\": ?, \"frequency_penalty\": ?}\n",
    "# optimal_params = optimizers_gpt2xlf[-1].max[\"params\"]\n",
    "optimal_params = {**default_sp, **{'frequency_penalty': 0.0727219392109375,\n",
    "                                   'presence_penalty': 0.3910543467725913,\n",
    "                                   'temperature': 0.057551002098534226,\n",
    "                                   'top_p': 0.25933633649499327}}\n",
    "def approx_eq(x, y, tol=1e-7):\n",
    "    return abs(x - y) < tol\n",
    "def get_sp_samples(params, test=False):  # Get all datapoints which match these parameters (todo: for each length group)\n",
    "    dn = 'msp_samples_nb' + (\"_test\" if test else '') + '/'\n",
    "    dname = 'data/learning_data/' + dn\n",
    "    D, R, inds = [], [], []  # Gathered input data and model results\n",
    "    for i in range(len(cats)):\n",
    "        idx_set = train_idx if i in train_idx else (val_idx if i in val_idx else test_idx)\n",
    "        D_ = []\n",
    "        fns = glob.glob(dname + str(i) + '/*')\n",
    "        if len(fns) == 0: continue\n",
    "        fns = [fn.split('/')[-1].split('\\\\')[-1] for fn in fns][::-1]  # Most recent first\n",
    "        load_fns = []\n",
    "        for fn in fns:\n",
    "            fn_split = fn.split('_')\n",
    "            params = {sps_[i]: float(fn_split[i + 1]) for i in range(len(sps_))}\n",
    "            if (\"ernst_one\" in fn) and all([approx_eq(params[k], optimal_params[k]) for k in sps_]): load_fns.append(fn)\n",
    "        for fn in load_fns[:15]:\n",
    "            params, d, _, r, mdl_str = load_ld(dn + str(i) + '/' + fn.split('.data')[0])\n",
    "            D_ += d\n",
    "            R += r\n",
    "        D += D_\n",
    "        i_in_set = idx_set.tolist().index(i)\n",
    "        inds += [i_in_set for _ in range(len(D_))]\n",
    "    return D, R, np.asarray(inds)\n",
    "mdl2_d, mdl2_r, mdl2_inds = get_sp_samples(optimal_params)\n",
    "mdl2_d_test, mdl2_r_test, mdl2_inds_test = get_sp_samples(optimal_params, test=True)\n",
    "mdl2_d_x, mdl2_d_test_x = [d_[0] for d_ in mdl2_d], [d_[0] for d_ in mdl2_d_test]\n",
    "def precomputed_completions(d_x, **kwargs):\n",
    "    return mdl2_r if (d_x == mdl2_d_x) else (mdl2_r_test if d_x == mdl2_d_test_x else None)\n",
    "mdl2 = {\"completions\": precomputed_completions, \"name\": \"precomputed_one\", \"mstr\": \"precomputed_one_mstr\"}\n",
    "len(mdl2_d), len(mdl2_d_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate ensemble of fine-tuned and original model\n",
    "lgroups_ft = [[4, 5]]\n",
    "bounds_gptxlfe = {\n",
    "  \"temperature\": [0.0001, 1.0],  # a temp of 0 selects the top completion every time due to the infinitely larger logit\n",
    "  \"top_p\": [0.0001, 1.0],        # same with this but more obvious\n",
    "#   \"top_k\": [1, 100],        # fix this to be a log of the input value so that lower values are sampled with high resolution?\n",
    "  \"presence_penalty\": [0.0, 2.0],   # both presence and frequency penalty have optimal values\n",
    "  \"frequency_penalty\": [0.0, 2.0],  # this can also go down to -2.0, probably not useful for our case...\n",
    "#   \"best_of\": [0.51, 5.49], # to do\n",
    "}\n",
    "optimizers_gpt2xlfe, results_gpt2xlfe = [], []\n",
    "for lgroup in lgroups_ft:\n",
    "    min_l, max_l = min(lgroup) - 1, max(lgroup)\n",
    "    def fun(temperature, top_p, presence_penalty, frequency_penalty):\n",
    "        global min_l, max_l\n",
    "        ps = locals()\n",
    "        ps[\"best_of\"] = 5.0\n",
    "        return test_sp(ps, n1=1, min_l=min_l, max_l=max_l, phase=\"val\", uniform=True, mdl=mdl, mdl2=mdl2, return_test_acc=False,\n",
    "                       d=mdl2_d, d_test=mdl2_d_test, inds=mdl2_inds, inds_test=mdl2_inds_test)\n",
    "    optimizers_gpt2xlfe.append(BayesianOptimization(f=fun, pbounds=bounds_gptxlfe, verbose=1000))\n",
    "    optimizers_gpt2xlfe[-1].probe(params={\"temperature\": 1.0, \"top_p\": 1.0, \"presence_penalty\": 0.0, \"frequency_penalty\": 0.0})\n",
    "    optimizers_gpt2xlfe[-1].maximize(init_points=8, n_iter=10)\n",
    "    results_gpt2xlfe.append(optimizers_gpt2xlfe[-1].max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizers_gpt2xlfe[-1].max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizers_gpt2xlfe[-1].res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |   iter    |  target   | freque... | presen... | temper... |   top_p   |\n",
    "# -------------------------------------------------------------------------\n",
    "# [0.14608262108262107, 0.1544311013061013, 0.1774588258963259]\n",
    "# [0.5712862692862692, 0.5167355977355977, 0.5436751026751027]\n",
    "# ('Test acc:', 15.932418276168276, 'sd:', 1.326833930141234)\n",
    "# |  1        |  0.5439   |  0.7545   |  0.8837   |  1.164    |  0.1678   |\n",
    "# [0.006944444444444444, 0.008333333333333333, 0.025000000000000005]\n",
    "# [0.008, 0.0, 0.014666666666666668]\n",
    "# ('Test acc:', 1.3425925925925926, 'sd:', 0.8203724604939516)\n",
    "# |  2        |  0.007556 |  1.563    |  1.115    |  1.345    |  0.4973   |\n",
    "# [0.21354131979131977, 0.173926362988863, 0.13297558922558925, 0.25522775835275835]\n",
    "# [0.5312100122100122, 0.6170219780219779, 0.5921578421578422, 0.5759225219225219]\n",
    "# ('Test acc:', 19.39177575896326, 'sd:', 4.543568017003007)\n",
    "# |  3        |  0.5791   |  1.185    |  1.479    |  0.3418   |  0.9886   |\n",
    "# [0.35648148148148145, 0.3520833333333333, 0.3180147058823529, 0.332175925925926, 0.2954656862745098]\n",
    "# [0.6432820512820513, 0.7113333333333333, 0.7075555555555556, 0.8067179487179486, 0.738952380952381]\n",
    "# ('Test acc:', 33.08442265795207, 'sd:', 2.247834350297864)\n",
    "# |  4        |  0.7216   |  0.5497   |  0.4131   |  0.04958  |  0.5878   |\n",
    "# [0.24212986087986085, 0.20677008177008174, 0.2121680402930403, 0.24574632543382546]\n",
    "# [0.5814134754134754, 0.5753290043290044, 0.6005385725385726, 0.5997070707070707]\n",
    "# ('Test acc:', 22.67035770942021, 'sd:', 1.73869387881864)\n",
    "# |  5        |  0.5892   |  0.989    |  1.323    |  0.6668   |  0.5829   |\n",
    "# [0.006944444444444444, 0.0, 0.0]\n",
    "# [0.0, 0.0, 0.0]\n",
    "# ('Test acc:', 0.23148148148148145, 'sd:', 0.32736425054932755)\n",
    "# |  6        |  0.0      |  1.268    |  1.645    |  1.636    |  0.3853   |\n",
    "# [0.029563492063492066, 0.05555555555555556, 0.03511904761904762, 0.024537037037037038, 0.01636904761904762]\n",
    "# [0.07957142857142857, 0.13923809523809524, 0.07619047619047618, 0.15666666666666665, 0.1219047619047619]\n",
    "# ('Test acc:', 3.222883597883598, 'sd:', 1.319310339155671)\n",
    "# |  7        |  0.1147   |  1.617    |  0.524    |  0.9253   |  0.9548   |\n",
    "# [0.0, 0.0, 0.0]\n",
    "# [0.0, 0.008, 0.008]\n",
    "# ('Test acc:', 0.0, 'sd:', 0.0)\n",
    "# |  8        |  0.005333 |  1.573    |  0.1914   |  1.962    |  0.2006   |\n",
    "# [0.29166666666666663, 0.3055555555555555, 0.3819444444444444, 0.24305555555555555, 0.3506944444444444]\n",
    "# [0.6366666666666666, 0.7466666666666666, 0.7399999999999999, 0.7999999999999998, 0.73]\n",
    "# ('Test acc:', 31.458333333333332, 'sd:', 4.809247136994663)\n",
    "# |  9        |  0.7307   |  0.001    |  1.477    |  0.001    |  0.001    |\n",
    "# [0.2643718553548275, 0.2827585030710031, 0.27731273356273356, 0.2543742511573394]\n",
    "# [0.5516450836744954, 0.5830235059058588, 0.544432178932179, 0.5589042819925172]\n",
    "# ('Test acc:', 26.97043357864759, 'sd:', 1.1087671560167434)\n",
    "# |  10       |  0.5595   |  0.001    |  1.146    |  0.5046   |  1.0      |\n",
    "# [0.40625, 0.2916666666666667, 0.34722222222222215]\n",
    "# [0.55, 0.6666666666666665, 0.6066666666666667]\n",
    "# ('Test acc:', 34.83796296296296, 'sd:', 4.678560863751791)\n",
    "# |  11       |  0.6078   |  0.001    |  0.001    |  0.4932   |  0.001    |\n",
    "# [0.32638888888888884, 0.2222222222222222, 0.2916666666666667, 0.2534722222222222, 0.34722222222222215, 0.20138888888888887, 0.21527777777777776, 0.23611111111111108, 0.2847222222222222]\n",
    "# [0.74, 0.6666666666666665, 0.6866666666666665, 0.6133333333333333, 0.7266666666666666, 0.7866666666666666, 0.78, 0.6466666666666666, 0.6933333333333332]\n",
    "# ('Test acc:', 26.427469135802472, 'sd:', 4.8236109901505495)\n",
    "# |  12       |  0.7044   |  1.045    |  1.107    |  0.001    |  0.001    |\n",
    "# [0.2708333333333333, 0.2569444444444444, 0.25, 0.18055555555555555]\n",
    "# [0.62, 0.7666666666666666, 0.6266666666666666, 0.6666666666666665]\n",
    "# ('Test acc:', 23.958333333333332, 'sd:', 3.4895401462225317)\n",
    "# |  13       |  0.67     |  0.7444   |  2.0      |  0.001    |  0.001    |\n",
    "# [0.3333333333333333, 0.3263888888888889, 0.37152777777777773, 0.3020833333333333]\n",
    "# [0.6166666666666667, 0.72, 0.7941025641025641, 0.7266666666666666]\n",
    "# ('Test acc:', 33.33333333333333, 'sd:', 2.4917882108346023)\n",
    "# |  14       |  0.7144   |  0.2284   |  2.0      |  0.001    |  1.0      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate un-fine-tuned model\n",
    "lgroups_ft = [[4, 5]]\n",
    "bounds_gptxl = {\n",
    "  \"temperature\": [0.0001, 1.1],  # a temp of 0 selects the top completion every time due to the infinitely larger logit\n",
    "  \"top_p\": [0.0001, 1.0],        # same with this but more obvious\n",
    "#   \"top_k\": [1, 100],        # fix this to be a log of the input value so that lower values are sampled with high resolution?\n",
    "  \"presence_penalty\": [0.0, 2.0],   # both presence and frequency penalty have optimal values\n",
    "  \"frequency_penalty\": [0.0, 2.0],  # this can also go down to -2.0, probably not useful for our case...\n",
    "#   \"best_of\": [0.51, 5.49], # to do\n",
    "}\n",
    "optimizers_gpt2xl, results_gpt2xl = [], []\n",
    "for lgroup in lgroups_ft:\n",
    "    min_l, max_l = min(lgroup) - 1, max(lgroup)\n",
    "    def fun(temperature, top_p, presence_penalty, frequency_penalty):\n",
    "        global min_l, max_l\n",
    "        ps = locals()\n",
    "        ps[\"best_of\"] = 5.0\n",
    "        return test_sp_conv(ps, min_l=min_l, max_l=max_l, tol=0.005, phase=\"val\", uniform=True, mdl=mdl)[0]\n",
    "    optimizers_gpt2xl.append(BayesianOptimization(f=fun, pbounds=bounds_gptxl, verbose=1000))\n",
    "    optimizers_gpt2xl[-1].maximize(init_points=8, n_iter=10)\n",
    "    results_gpt2xl.append(optimizers_gpt2xl[-1].max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test accuracy for the top performing (training accuracy ) sampling parameters for gpt2-\n",
    "# \n",
    "# |   iter    |  target   | freque... | presen... | temper... |   top_p   |\n",
    "# -------------------------------------------------------------------------\n",
    "# [0.049999999999999996, 0.025000000000000005, 0.061111111111111116]\n",
    "# [0.008, 0.03333333333333333, 0.029333333333333336]\n",
    "# ('Test acc:', 4.537037037037037, 'sd:', 1.5101394842870455)\n",
    "# |  1        |  0.02356  |  0.7232   |  0.9207   |  1.792    |  0.1562   |\n",
    "# [0.2111111111111111, 0.13333333333333333, 0.23576388888888888, 0.3333333333333333]\n",
    "# [0.1965714285714286, 0.11199999999999999, 0.111, 0.12]\n",
    "# ('Test acc:', 22.838541666666668, 'sd:', 7.141744752411932)\n",
    "# |  2        |  0.1349   |  0.6561   |  0.6181   |  0.6707   |  0.1418   |\n",
    "# [0.0, 0.016666666666666666, 0.0]\n",
    "# [0.0, 0.0, 0.0]\n",
    "# ('Test acc:', 0.5555555555555556, 'sd:', 0.7856742013183862)\n",
    "# |  3        |  0.0      |  1.035    |  0.6253   |  1.287    |  0.9439   |\n",
    "# [0.5208333333333334, 0.375, 0.3854166666666667, 0.4930555555555555, 0.34027777777777773, 0.40625, 0.3194444444444444]\n",
    "# [0.32, 0.38, 0.21333333333333332, 0.28, 0.16666666666666663, 0.2, 0.2733333333333333]\n",
    "# ('Test acc:', 40.57539682539682, 'sd:', 6.965317270864225)\n",
    "# |  4        |  0.2619   |  0.4573   |  0.1734   |  0.6163   |  0.02537  |\n",
    "# [0.025000000000000005, 0.0, 0.03333333333333333]\n",
    "# [0.008, 0.024000000000000004, 0.008]\n",
    "# ('Test acc:', 1.9444444444444444, 'sd:', 1.4163943093313291)\n",
    "# |  5        |  0.01333  |  1.27     |  1.156    |  1.195    |  0.7833   |\n",
    "# [0.008333333333333333, 0.0, 0.0]\n",
    "# [0.0, 0.0, 0.0]\n",
    "# ('Test acc:', 0.2777777777777778, 'sd:', 0.3928371006591931)\n",
    "# |  6        |  0.0      |  1.297    |  1.846    |  1.938    |  0.7382   |\n",
    "# [0.013888888888888888, 0.0, 0.0]\n",
    "# [0.014666666666666668, 0.008, 0.0]\n",
    "# ('Test acc:', 0.4629629629629629, 'sd:', 0.6547285010986551)\n",
    "# |  7        |  0.007556 |  0.2306   |  1.936    |  1.592    |  0.7024   |\n",
    "# [0.2511354386354386, 0.3434765466015466, 0.30643372830872834, 0.3152858715358715]\n",
    "# [0.32786868686868686, 0.3295645530939648, 0.3073928293928294, 0.3104887334887335]\n",
    "# ('Test acc:', 30.408289627039625, 'sd:', 3.349002098569766)\n",
    "# |  8        |  0.3188   |  0.1998   |  0.08897  |  0.9428   |  0.6384   |\n",
    "# [0.4106481481481481, 0.26304563492063493, 0.31493055555555555, 0.3008207070707071, 0.37310600279350276, 0.38941300733580136]\n",
    "# [0.3358236208236208, 0.1989130869130869, 0.21833333333333332, 0.34352380952380945, 0.3320714285714286, 0.3045044955044955]\n",
    "# ('Test acc:', 34.19940093040583, 'sd:', 5.258394155542585)\n",
    "# |  9        |  0.2889   |  0.01     |  0.01     |  0.1864   |  0.669    |\n",
    "# [0.4861111111111111, 0.3020833333333333, 0.2604166666666667, 0.2916666666666667, 0.3055555555555555, 0.34722222222222215, 0.3541666666666667, 0.2708333333333333]\n",
    "# [0.3833333333333333, 0.20666666666666664, 0.3161904761904762, 0.38, 0.3466666666666666, 0.4137777777777778, 0.29333333333333333, 0.35]\n",
    "# ('Test acc:', 32.72569444444444, 'sd:', 6.743512364375678)\n",
    "# |  10       |  0.3362   |  0.01     |  0.01     |  1.212    |  0.01     |\n",
    "# [0.0, 0.0, 0.0]\n",
    "# [0.0, 0.0, 0.0]\n",
    "# ('Test acc:', 0.0, 'sd:', 0.0)\n",
    "# |  11       |  0.0      |  0.01     |  0.01     |  2.0      |  1.0      |\n",
    "# [0.5181517556517556, 0.2377946127946128, 0.4213624338624338, 0.3735119047619048]\n",
    "# [0.27763369963369966, 0.32315873015873015, 0.23523076923076924, 0.2697142857142857]\n",
    "# ('Test acc:', 38.77051767676768, 'sd:', 10.102443647288354)\n",
    "# |  12       |  0.2764   |  0.01     |  0.01     |  0.6837   |  0.1847   |\n",
    "# [0.19999999999999998, 0.23750000000000002, 0.2722222222222222, 0.375, 0.20833333333333334, 0.0625, 0.16666666666666666, 0.325]\n",
    "# [0.22, 0.08, 0.10400000000000001, 0.264, 0.12, 0.096, 0.2, 0.16]\n",
    "# ('Test acc:', 23.09027777777778, 'sd:', 9.035987186191242)\n",
    "# |  13       |  0.1555   |  0.8694   |  0.01     |  0.01     |  1.0      |\n",
    "# [0.5590277777777778, 0.3819444444444444, 0.31527777777777777, 0.3541666666666667, 0.37152777777777773, 0.375, 0.518287037037037, 0.34027777777777773, 0.4236111111111111]\n",
    "# [0.2338095238095238, 0.19422222222222224, 0.26, 0.25666666666666665, 0.3053333333333333, 0.31, 0.305, 0.36, 0.256]\n",
    "# ('Test acc:', 40.434670781893004, 'sd:', 7.765741054904079)\n",
    "# |  14       |  0.2757   |  0.01     |  0.8582   |  0.01     |  1.0      |\n",
    "# [0.35763888888888884, 0.3756944444444444, 0.33796296296296297, 0.38055555555555554, 0.2824074074074074, 0.3171296296296296, 0.43402777777777773]\n",
    "# [0.3680952380952381, 0.23466666666666666, 0.2222222222222222, 0.22, 0.32, 0.14, 0.22666666666666666]\n",
    "# ('Test acc:', 35.50595238095238, 'sd:', 4.524186608251292)\n",
    "# |  15       |  0.2474   |  0.01     |  2.0      |  0.01     |  1.0      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test accuracy for the top performing (training accuracy 0.2026) sampling parameters for gpt2-small (after the full 18 runs)\n",
    "# np.mean([0.3353320494864612,0.3277777777777778,0.21805555555555556,0.3402514152514152,0.28348214285714285,0.13194444444444445])# = 0.27280723089546616"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With about half an hour of fine-tuning using 7 training samples on an NVidia 980 Ti (showing improvements):\n",
    "# Initial input words on first line, those eventually found by repeatedly querying model + recursively adding words on 2nd & 3rd\n",
    "input_sentence = (\"pace, silence, diversion, attention, plot twist, cliffhanger, surprise, fate, fourth wall, monologue\" + \\\n",
    "\"reinterpretation, harmony, character progression, reading circle\")\n",
    "# \"monolith, elevator effect, time loop, survival, desert resort, town watchman\")\n",
    "# \"monolith, allegro, soundtrack, chord, classical, opera\")\n",
    "input_sentence = input_sentence.split(', ')\n",
    "np.random.shuffle(input_sentence)\n",
    "input_sentence = \"A list of types of element of drama and writing: \"+', '.join(input_sentence[:10])+', '\n",
    "olen = len(input_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_sentence = append_next_token(input_sentence, top_k=25, top_p=0.6, temperature=15.0, olen=olen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # With about half an hour of fine-tuning using 7 training samples on an NVidia 980 Ti (showing improvements):\n",
    "# # Initial input words on first line, those eventually found by repeatedly querying model + recursively adding words on 2nd & 3rd\n",
    "# input_sentence = (\"pace, silence, diversion, attention, plot twist, cliffhanger, surprise, fate, fourth wall, monologue\" + \\\n",
    "# \"reinterpretation, harmony, character progression, reading circle\")\n",
    "# # \"monolith, elevator effect, time loop, survival, desert resort, town watchman\")\n",
    "# # \"monolith, allegro, soundtrack, chord, classical, opera\")\n",
    "# input_sentence = input_sentence.split(', ')\n",
    "# np.random.shuffle(input_sentence)\n",
    "# input_sentence = \"A list of types of element of drama and writing: \"+', '.join(input_sentence[:10])+', '\n",
    "# olen = len(input_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# input_sentence = append_next_token(input_sentence, top_k=25, top_p=0.6, temperature=15.0, olen=olen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # With about half an hour of fine-tuning using 7 training samples on an NVidia 980 Ti (showing improvements):\n",
    "# # Initial input words on first line, those eventually found by repeatedly querying model + recursively adding words on 2nd & 3rd\n",
    "# input_sentence = (\"coffee, water, tea, coke, lemonade, milkshake, \" + \\\n",
    "# \"watermelon juice, cherry juice, blackcurrant mixture, kava, orangeade, lemon juice, cherryade, cranberry juice\")\n",
    "# input_sentence = input_sentence.split(', ')\n",
    "# np.random.shuffle(input_sentence)\n",
    "# input_sentence = \"A list of types of element of drama and writing: \"+', '.join(input_sentence[:10])+', '\n",
    "# olen = len(input_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# input_sentence = append_next_token(input_sentence, top_k=10, top_p=0.8, temperature=5.0, olen=olen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # With about half an hour of fine-tuning using 7 training samples on an NVidia 980 Ti (showing improvements):\n",
    "# # Initial input words on first line, those eventually found by repeatedly querying model + recursively adding words on 2nd & 3rd\n",
    "# input_sentence = (\"pace, silence, diversion, attention, plot twist, cliffhanger, surprise, fate, fourth wall, monologue\" + \\\n",
    "# \"\")\n",
    "# input_sentence = input_sentence.split(', ')\n",
    "# np.random.shuffle(input_sentence)\n",
    "# input_sentence = \"A list of types of element of drama and writing: \"+', '.join(input_sentence[:10])+', ' # Randomly generate shuffled drinks sublist\n",
    "# olen = len(input_sentence)\n",
    "# input_sentence = append_next_token(input_sentence, top_k=5, top_p=0.9, temperature=5.0, olen=olen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With about half an hour of fine-tuning using 7 training samples on an NVidia 980 Ti (showing improvements):\n",
    "# Initial input words on first line, those eventually found by repeatedly querying model + recursively adding words on 2nd & 3rd\n",
    "# input_sentence = (\"coffee, water, tea, coke, lemonade, milkshake, \" + \\\n",
    "# \"milk, soda\")\n",
    "# input_sentence = input_sentence.split(', ')\n",
    "# np.random.shuffle(input_sentence)\n",
    "# input_sentence = \"A list of types of drink: \"+', '.join(input_sentence[:10])+', ' # Randomly generate shuffled drinks sublist\n",
    "# olen = len(input_sentence)\n",
    "# input_sentence = append_next_token(input_sentence, top_k=25, top_p=0.7, temperature=2.0, olen=olen)\n",
    "# input_sentence = append_next_token(input_sentence, top_k=5, top_p=0.9, temperature=5.0, olen=olen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # With about half an hour of fine-tuning using 7 training samples on an NVidia 980 Ti (showing improvements):\n",
    "# # Initial input words on first line, those eventually found by repeatedly querying model + recursively adding words on 2nd & 3rd\n",
    "# input_sentence = (\"coffee, water, tea, coke, lemonade, milkshake\" + \\\n",
    "# \", liquor, wine, juice, beer, milk, soft drink, whiskey, vodka, spirits, soda, ice water, ice cold beer, cider, yoghurt, soda pop, rum, chocolate milk, hot cocoa, alcohol\")\n",
    "# input_sentence = input_sentence.split(', ')\n",
    "# np.random.shuffle(input_sentence)\n",
    "# input_sentence = \"A list of types of drink: \"+', '.join(input_sentence[:10])+', ' # Randomly generate shuffled drinks sublist\n",
    "# olen = len(input_sentence)\n",
    "# input_sentence = append_next_token(input_sentence, top_k=25, top_p=0.7, temperature=2.0, olen=olen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # With about half an hour of fine-tuning using 7 training samples on an NVidia 980 Ti (showing improvements):\n",
    "# # Initial input words on first line, those eventually found by repeatedly querying model + recursively adding words on 2nd & 3rd\n",
    "# input_sentence = (\"coffee, water, tea, coke, lemonade, milkshake\" + \\\n",
    "# \", liquor, wine, juice, beer, milk, soft drink, whiskey, vodka, spirits, soda, ice water, ice cold beer, cider\" + \\\n",
    "# \", yoghurt, soda pop, rum, chocolate milk, hot cocoa, alcohol\")\n",
    "# input_sentence = input_sentence.split(', ')\n",
    "# np.random.shuffle(input_sentence)\n",
    "# input_sentence = \"A list of types of drink: \"+', '.join(input_sentence[:10])+', ' # Randomly generate shuffled drinks sublist\n",
    "# olen = len(input_sentence)\n",
    "# input_sentence = append_next_token(input_sentence, top_k=25, top_p=0.7, temperature=2.0, olen=olen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changelog 16/04/2021 5pm: Using log_period=1 (max_len=96) reliably finds improvement, need more data and larger gpt2 (medium+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With about half an hour of fine-tuning using 7 training samples on an NVidia 980 Ti (showing improvements):\n",
    "# Initial input words on first line, those eventually found by repeatedly querying model + recursively adding words on 2nd & 3rd\n",
    "# input_sentence = (\"coffee, water, tea, coke, lemonade, milkshake,\" + \\\n",
    "# \" milk, vodka, beer, ice water, soda, lassi, juice, alcohol, whiskey\")\n",
    "# input_sentence = input_sentence.split(', ')\n",
    "# np.random.shuffle(input_sentence)\n",
    "# input_sentence = \"A list of types of drink: \"+', '.join(input_sentence[:10])+', ' # Randomly generate shuffled drinks sublist\n",
    "# olen = len(input_sentence)\n",
    "# input_sentence = append_next_token(input_sentence, top_k=25, top_p=0.9, temperature=2.0, olen=olen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With about half an hour of fine-tuning using 7 training samples on an NVidia 980 Ti (showing improvements):\n",
    "# Initial input words on first line, those eventually found by repeatedly querying model + recursively adding words on 2nd & 3rd\n",
    "# input_sentence = (\"coffee, water, tea, coke, lemonade, milkshake,\" + \\\n",
    "# \" wine, soda, alcohol, beer, liquor, apple cider, whiskey, milk, bourbon, vodka, cider, lemon juice\")\n",
    "# input_sentence = input_sentence.split(', ')\n",
    "# np.random.shuffle(input_sentence)\n",
    "# input_sentence = \"A list of types of drink: \"+', '.join(input_sentence[:6])+', ' # Randomly generate shuffled drinks sublist\n",
    "# olen = len(input_sentence)\n",
    "# input_sentence = append_next_token(input_sentence, top_k=30, top_p=0.7, temperature=3.0, olen=olen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With about half an hour of fine-tuning using 7 training samples on an NVidia 980 Ti (showing improvements):\n",
    "# Initial input words on first line, those eventually found by repeatedly querying model + recursively adding words on 2nd & 3rd\n",
    "# input_sentence = (\"coffee, water, tea, coke, lemonade, milkshake,\" + \\\n",
    "# \" beer, milk, juice, wine, spirits, alcohol, soda, whiskey, brandy, apple juice, liquor, ice tea, watermelon juice, vodka,\" + \\\n",
    "# \" lemon tea, apple cider, ale, lager, fruit tea, lime cider, cocktail, mocha, red wine, apple soda\")\n",
    "# input_sentence = input_sentence.split(', ')\n",
    "# np.random.shuffle(input_sentence)\n",
    "# input_sentence = \"A list of types of drink: \"+', '.join(input_sentence[:10])+', ' # Randomly generate shuffled drinks sublist\n",
    "# olen = len(input_sentence)\n",
    "# input_sentence = append_next_token(input_sentence, top_k=30, top_p=0.7, temperature=2.0, olen=olen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changelog 16/04/2021 5am: lr=1e-5, max_len=80, log_period_batches=5 increased accuracy by 8%. Need to add a bunch more data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With about half an hour of fine-tuning using 5 training samples on an NVidia 980 Ti (showing improvements):\n",
    "# Initial input words on first line, those eventually found by repeatedly querying model + recursively adding words on 2nd & 3rd\n",
    "# input_sentence = (\"coffee, water, tea, coke, lemonade, milkshake, \" + \\\n",
    "# \"soda, milk, juice, wine, vodka, gin, lime juice, beer, hot chocolate, cider, whiskey, fruit juice, cocktail, liquor, \" + \\\n",
    "# \"spirits, watermelon juice, martini, rum, chocolate milk, orangeade\")\n",
    "# input_sentence = input_sentence.split(', ')\n",
    "# np.random.shuffle(input_sentence)\n",
    "# input_sentence = \"A list of types of drink: \"+', '.join(input_sentence[:10])+', ' # Randomly generate shuffled drinks sublist\n",
    "# olen = len(input_sentence)\n",
    "# input_sentence = append_next_token(input_sentence, top_k=25, top_p=0.9, temperature=1.89, olen=olen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without fine tuning (regular GPT-2):\n",
    "# Initial input words on first line, those eventually found by repeatedly querying model on 2nd\n",
    "# input_sentence = (\"coffee, water, tea, coke, lemonade, milkshake, \" + \\\n",
    "# \"milk, juice, fruit juice, soda, wine, beer, hot chocolate, chocolate milk, alcohol, cider, ice tea, liquor, spirit\")\n",
    "# input_sentence.split(', ')\n",
    "# np.random.shuffle(input_sentence)\n",
    "# input_sentence = \"A list of types of drink: \"+', '.join(input_sentence[:10])+', ' # Randomly generate shuffled drinks sublist\n",
    "# olen = len(input_sentence)\n",
    "# input_sentence = append_next_token(input_sentence, top_k=25, top_p=0.75, temperature=1.89, olen=olen) # k = 25 also used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training for too long (in this case ~6:30 hours) overfits\n",
    "# input_sentence = (\"coffee, water, tea, coke, lemonade, milkshake, \" + \\\n",
    "# \"beer, juice, soda, liquor, wine, tequila, spirits, alcohol, cocktail, martini, whiskey, rum, vodka\")\n",
    "# input_sentence = input_sentence.split(', ')\n",
    "# np.random.shuffle(input_sentence)\n",
    "# input_sentence = \"A list of types of drink: \"+', '.join(input_sentence[:10])+', ' # Randomly generate shuffled drinks sublist\n",
    "# olen = len(input_sentence)\n",
    "# input_sentence = append_next_token(input_sentence, top_k=25, top_p=0.9, temperature=1.89, olen=olen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changelog 15/04/2021 11am: Found learning rate 1e-7, max_listlen 15, min_nw 0.7, max_nw 0.9, lidstone_eps 0.01 works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No fine tuning (testing gpt2) (old append function):\n",
    "# input_sentence = \"A list of types of drink: coffee, water, tea, coke, lemonade, milkshake\"\n",
    "# input_sentence = append_next_token(input_sentence, top_k=25, top_p=0.75, temperature=1.89)\n",
    "# A list of types of drink: juice, tea, cider, lemonade, milk, beer, hot chocolate,➡ ice cold milk. Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working (reproducible) examples using various non-fine-tuned models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT3 (via AI Dungeon) (Randomness = 2.0, model = Dragon):\n",
    "# sentence = \"A list of ML algorithms: inverse reinforcement learning, ELMo, decision tree, LDA, \"\n",
    "# expected_completion = \"MLP, MLL, MMM. You can't believe you're actually using these things!\"\n",
    "\n",
    "# sentence = \"A list of animals seen in the wild: wolffish, woodlouse, sheep, zebra, yak, \"\n",
    "# expected_completion = \"goat, fox, dog, rat. You're guessing that a lot of other animals have been seen as well; maybe even all the animals on your list except for wolf and rat?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT2 (via Write with Transformer) (Top-p = 0.67, temperature = 1.89, max time = 1.9):\n",
    "# sentence = \"A list of round fruits: peach, apricot, lime, plum, blackberry, cantaloupe, nectarine, pitaya, persimmon, \"\n",
    "# expected_completion = \"mango, papaya and raspberry, as also many\"\n",
    "\n",
    "# sentence = \"A list of chemical elements: hydrogen, carbon, oxygen, nitrogen, gold, \"\n",
    "# expected_completion = \"silver, aluminum, potassium and phosphorus; atomic number.\"\n",
    "\n",
    "# sentence = \"A list of microbes found on earth: bacteria, virus, prokaryote, amoeba, \"\n",
    "# expected_completion = \"archaea, algae, nematode, euk\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_s_avglogs(X, use_log_probs=True):\n",
    "#     tokens = [pt.tensor(tokenizer.encode(x)).to(d) for x in X]\n",
    "#     sqlens = [len(x) for x in tokens]\n",
    "#     n_bats = (len(sqlens) // batch_size) + 1\n",
    "#     res = []\n",
    "#     for i in range(n_bats):\n",
    "#         tokens_b, sqlens_b = tokens[i * bsz:(i + 1) * bsz], sqlens[i * bsz:(i + 1) * bsz]\n",
    "#         xs, _, sqlen = adapt_form(tokens_b, None, sqlens_b, mlen=max(sqlens_b))\n",
    "#         logits = inference(xs, sqlen, seq_maxlen=max(sqlens_b), return_fulloutput=True)[0]\n",
    "#         pt.cuda.empty_cache()\n",
    "# #         logits = pt.stack(logits, 0)\n",
    "#         for i in range(len(sqlens_b)):\n",
    "#             x_logits = [logits[i][j] for j in range(sqlens_b[i])]\n",
    "#             x_logs = ([pt.log(F.softmax(logits[i][j]))[tokens_b[i][j]].cpu().detach().numpy() for j in range(sqlens_b[i])] if \\\n",
    "#                       use_log_probs else \\\n",
    "#                       [logits[i][j][tokens_b[i][j]].cpu().detach().numpy() for j in range(sqlens_b[i])])\n",
    "#             res.append(np.mean(x_logs[1:]))  # ignore initial prefix token 'A'/'An'\n",
    "#     return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = openai.Completion.create(**{**default_params,\n",
    "#   \"prompt\": \"A list of cyclic phenomena: day and night, induction coil, self-oscillation, tornado, runaway greenhouse effect,\",\n",
    "#   \"temperature\": 1.5,\n",
    "#   \"top_p\": 1.0,\n",
    "#   \"n\": 5,\n",
    "#   \"best_of\": 20,\n",
    "#   \"max_tokens\": 7,\n",
    "#   \"stop\": [\",\", \"\\n\"],\n",
    "# })\n",
    "# for choice in response[\"choices\"]:\n",
    "#     d = {}\n",
    "#     tokens = choice[\"logprobs\"][\"tokens\"]\n",
    "#     t_i = -1\n",
    "#     for t in tokens:\n",
    "#         t_i += 1\n",
    "#         r = [(np.e**v, k) for (k, v) in choice[\"logprobs\"][\"top_logprobs\"][t_i].items()]\n",
    "#         r.sort(reverse=True)\n",
    "#         print(sum([v for (v, k) in r]))\n",
    "#         rd = dict([(k, v) for (v, k) in r])\n",
    "#         r = [k for (v, k) in r]\n",
    "#         d[t] = (rd, r, np.e**choice[\"logprobs\"][\"token_logprobs\"][t_i])\n",
    "#     print('|'.join([' '.join((s.replace(\"\\n\", \"⏎\"),'%.2f' % (d[s][2] * 100),\n",
    "#                               str(d[s][1].index(s) + 1) if s in d[s][1] else \"<100\")) for s in tokens]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
