{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A fine-tuned language model for pictionary word list completion (topic/category phrase + example words -> list of 30 examples). For example, one may complete the list\n",
    "\n",
    "\"A list of round fruits: peach, apricot, lime, plum,\"\n",
    "\n",
    "with\n",
    "\n",
    "\"mango, cherry, pineapple, strawberry, pumpkin, watermelon, orange, pomegranate, melon, apple, pear, grapefruit, papaya, lemon, kiwi, passionfruit, blueberry, raspberry, blackberry, cantaloupe, nectarine, pitaya, persimmon, durian, guava, jackfruit, avocado, lychee, soursop, guarana, mangosteen, blackcurrant, cranberry\"\n",
    "\n",
    "using this model. These words should be compatible with pictionary/skribbl.io; i.e., \"sketchable\" within a few minutes, and easily recognisable. Scroll to the end for more examples, or see the file `data/examples.txt`. Sketchability parameter estimation examples can also be found in `data/data.tsv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import openai\n",
    "# with open('../../openai-api-org.txt', 'r') as f: openai.organization = f.read()\n",
    "# with open('../../openai-api-key.txt', 'r') as f: openai.api_key      = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import time\n",
    "import string\n",
    "import jsonlines\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as pt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "import matplotlib.pyplot as plt\n",
    "from bayes_opt import BayesianOptimization\n",
    "from IPython.utils import io\n",
    "from IPython.display import clear_output\n",
    "from transformers import GPT2ForSequenceClassification, GPT2LMHeadModel, GPTNeoForCausalLM, ReformerModelWithLMHead, \\\n",
    "                         get_linear_schedule_with_warmup\n",
    "from pytorch_transformers import GPT2Tokenizer\n",
    "from transformers import GPT2TokenizerFast\n",
    "from Learning import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                  ###   Options   ###\n",
    "model_name = \"ernst_one\"\n",
    "gpt2_modelkey = \"gpt2-xl\"            # Pretrained model to start from\n",
    "# gpt2_modelkey = \"gpt2\"\n",
    "test_set_frac = 0.25                 # Fraction of samples to keep as separate test set (word lists)\n",
    "TsN = 20                            # Number of randomly generated prompts for each sample when validating model\n",
    "log_period_batches = 10            # Batches per iteration\n",
    "# learning_rate = 5e-7               # Adam learning rate (default is 5e-5, sentiment classification example had 2e-5)\n",
    "learning_rate = 7e-5\n",
    "# learning_rate = 4e-6\n",
    "adam_epsilon = 1e-8                  # Adam epsilon (default is 1e-8)\n",
    "n_sched_warmup = 0                   # Linear scheduler for optimizer number of warmup steps\n",
    "batch_size = bsz = 64                # Samples per batch\n",
    "# batch_size = bsz = 32\n",
    "# batch_size = bsz = 4\n",
    "# N_train_batches = int(1e7 / bsz)   # Total number of batches to show model\n",
    "N_train_batches = 450\n",
    "# max_len = 1024                     # Max n. tokens applied prior to *max_nw (tokens)\n",
    "max_len = 32\n",
    "lidstone_e = 0.01                    # Smoothing for possible words/subwords which are not in the missing list words set\n",
    "lastcomma_repl = ',' # 'EOS', ','    # Token optionally used to replace the final comma that ends the generated phrase\n",
    "use_correct_nouns = True             # Whether to use only correct singular or plural form of category nouns for the given prompt\n",
    "swap_noun = False                    # Whether to swap plural and singular nouns in prompt\n",
    "# rng_train = [0, 512]               # Range of prompt list lengths (number of phrases) to generate for training data\n",
    "rng_train = [3, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = \"cuda\" if pt.cuda.is_available() else \"cpu\"  # Setup torch device(s)\n",
    "d = device = pt.device(dev)\n",
    "# world_size = 1\n",
    "# rank = 0\n",
    "# def setup(rank, world_size): \n",
    "#     os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
    "#     os.environ['MASTER_PORT'] = find_free_port()\n",
    "#     dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)  # initialize the process group\n",
    "# def cleanup():\n",
    "#     dist.destroy_process_group()\n",
    "# # mp.spawn(setup, args=(rank, world_size), nprocs=world_size)\n",
    "# setup(rank, world_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cats, cats_sing, phrases = Listset().load()  # Import word lists dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([317, 1351, 286, 2835, 15921, 25, 22514, 11, 48389, 11, 279, 4127, 11],\n",
       " [317, 1351, 286, 2835, 15921, 25, 22514, 11],\n",
       " [48389, 11, 279, 4127, 11],\n",
       " [32, 1351, 286, 2835, 15921, 25, 22514, 11, 48389, 11, 279, 4127, 11],\n",
       " [32, 1351, 286, 2835, 15921, 25, 22514, 11],\n",
       " [273, 6231, 11, 279, 4127, 11])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt3_tokenizer = GPT2TokenizerFast.from_pretrained((\"gpt2-large\"), padding=True)\n",
    "with io.capture_output() as captured:\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(gpt2_modelkey.replace(\"-xl\", \"-large\"), padding=True)\n",
    "tokenizer.encode(\"A list of round fruits: apples, oranges, pears,\"), \\\n",
    "    tokenizer.encode(\"A list of round fruits: apples,\"), \\\n",
    "    tokenizer.encode(\"oranges, pears,\"), \\\n",
    "  gpt3_tokenizer.encode(\"A list of round fruits: apples, oranges, pears,\"), \\\n",
    "      gpt3_tokenizer.encode(\"A list of round fruits: apples,\"), \\\n",
    "      gpt3_tokenizer.encode(\"oranges, pears,\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lprompts_encoded = [[tokenizer.encode(prompt), \"types of\" in prompt] for prompt in lprompts]\n",
    "cats_e = [[tokenizer.encode(c + ': ') for c in cs] for cs in cats]\n",
    "cats_sing_e = [[tokenizer.encode(c + ': ') for c in cs] for cs in cats_sing]\n",
    "phrases_e = [[tokenizer.encode(p + ', ') for p in ps] for ps in phrases]\n",
    "comma_token = pt.tensor(tokenizer.encode(\",\")[0], device=d)\n",
    "lprompts_encoded3 = [[gpt3_tokenizer.encode(prompt), \"types of\" in prompt] for prompt in lprompts]\n",
    "cats_e3 = [[gpt3_tokenizer.encode(c + ': ') for c in cs] for cs in cats]\n",
    "cats_sing_e3 = [[gpt3_tokenizer.encode(c + ': ') for c in cs] for cs in cats_sing]\n",
    "phrases_e3 = [[gpt3_tokenizer.encode(p + ', ') for p in ps] for ps in phrases]\n",
    "comma_token3 = pt.tensor(gpt3_tokenizer.encode(\",\")[0], device=d)\n",
    "N_tokens = len(tokenizer)\n",
    "N_wordlists = len(cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lprompts_encoded = [[pt.tensor(prmt, device=d), pt.tensor(typesof, device=d)] for (prmt, typesof) in lprompts_encoded]\n",
    "cats_e = [[pt.tensor(c, device=d) for c in cs] for cs in cats_e]\n",
    "cats_sing_e = [[pt.tensor(c, device=d) for c in cs] for cs in cats_sing_e]\n",
    "phrases_e = [[pt.tensor(p, device=d) for p in ps] for ps in phrases_e]\n",
    "lprompts_sing_encoded = [(prmpt, typesof) for (prmpt, typesof) in lprompts_encoded if typesof ^ swap_noun]\n",
    "lprompts_sing = [tokenizer.decode(prmpt[0].detach().cpu().numpy()) for prmpt in lprompts_sing_encoded]\n",
    "lprompts_encoded3 = [[pt.tensor(prmt, device=d), pt.tensor(typesof, device=d)] for (prmt, typesof) in lprompts_encoded3]\n",
    "cats_e3 = [[pt.tensor(c, device=d) for c in cs] for cs in cats_e3]\n",
    "cats_sing_e3 = [[pt.tensor(c, device=d) for c in cs] for cs in cats_sing_e3]\n",
    "phrases_e3 = [[pt.tensor(p, device=d) for p in ps] for ps in phrases_e3]\n",
    "lprompts_sing3_encoded = [(prmpt, typesof) for (prmpt, typesof) in lprompts_encoded3 if typesof ^ swap_noun]\n",
    "lidstone_e = pt.tensor(lidstone_e, device=d)\n",
    "lid_val = lidstone_e / N_tokens\n",
    "y_zero = (lid_val).repeat(N_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a fixed test set and save to disk. This function defines the next list token prediction problem\n",
    "def gen_truncated_list(prmt, p, rng=rng_train, mlen=max_len):  # prmt = prompt tokens, p = list phrase/word tokens\n",
    "    tkzs, sent, tkix, wordix = [], [], 0, -1   # rng is the inclusive range of list lengths to generate (number of phrases)\n",
    "    min_nw, max_nw, min_nt, max_nt = rng[0], int(rng[1]), 0, int(mlen) - len(prmt)\n",
    "    incl_words = np.random.choice(len(p), min(len(p), max_nw), replace=False)\n",
    "    for phz_i in incl_words:\n",
    "        phz_enc, wordix = p[phz_i], wordix + 1\n",
    "        tkzs.append((tkix, phz_enc))\n",
    "        tkix += len(phz_enc)\n",
    "        sent.append(phz_enc)\n",
    "        if wordix < min_nw: min_nt = tkix\n",
    "        if tkix >= max_nt:\n",
    "            tkix = max_nt\n",
    "            break\n",
    "    if min_nt - 1 >= tkix:  # rare when max_len is large enough (0.5x max possible total list length) (temporary optimisation)\n",
    "        return gen_truncated_list(prmt, p, rng=rng, mlen=mlen)\n",
    "    sent = pt.hstack(sent)[:max_nt]\n",
    "    missing_w = [p[i] for i in range(len(p)) if i not in incl_words]\n",
    "    trunc_ix = np.random.randint(min_nt - 1, tkix)\n",
    "    trunc_n = min([(trunc_ix - ix) for (ix, enc) in tkzs if ix <= trunc_ix])  # N. end phrase tokens\n",
    "    missing_w += [enc for (ix, enc) in tkzs if ix >= (trunc_ix - trunc_n)]\n",
    "    missing_matches = missing_w\n",
    "    if trunc_n > 0:\n",
    "        phr_start = trunc_ix - trunc_n\n",
    "        partial_phr = sent[phr_start:trunc_ix]\n",
    "        missing_matches = [enc for enc in missing_w if len(enc) >= trunc_n and all(enc[:trunc_n] == partial_phr)]\n",
    "    next_tokens = [enc[trunc_n] for enc in missing_matches]\n",
    "    norm = len(next_tokens) * (1.0 + lidstone_e)\n",
    "    tunit, y_ = pt.tensor(1 / norm, device=d), y_zero.clone()\n",
    "    for token in next_tokens: y_[token] += tunit\n",
    "    return pt.hstack([prmt, sent[:trunc_ix]]), y_\n",
    "def stac_sample(stac, n):   # Create batches by random permutations (maximise diversity and uniformity) (shuffling done later)\n",
    "    r, mode, total = [], tuple(np.unique(stac)), stac.shape[0]\n",
    "    while n > 0:\n",
    "        if mode == (0,) or mode == (1,) or mode == (-1,):\n",
    "            if n >= total:\n",
    "                new = sum([list(range(total)) for _ in range(n // total)], [])\n",
    "                r += new\n",
    "                n -= len(new)\n",
    "            else:\n",
    "                new = np.random.choice(total, n, replace=False)\n",
    "                stac[new] = (mode[0] + 1) if mode != (1,) else -1\n",
    "                r += new.tolist()\n",
    "                n = 0\n",
    "        else:\n",
    "            old_val, new_val = mode[0] if mode != (-1, 1) else 1, mode[1] if mode != (-1, 1) else -1\n",
    "            old_i = np.nonzero(stac == old_val)[0]\n",
    "            if n >= old_i.shape[0]:\n",
    "                stac[old_i] = new_val\n",
    "                r += old_i.tolist()\n",
    "                n -= old_i.shape[0]\n",
    "            else:\n",
    "                new = np.random.choice(old_i, n, replace=False)\n",
    "                stac[new] = new_val\n",
    "                r += new.tolist()\n",
    "                n = 0\n",
    "        mode = tuple(np.unique(stac))\n",
    "    return r\n",
    "def gen_listname(cp, cp_cs, prompt, prmt, tknzr=tokenizer):\n",
    "    cat_ix = np.random.randint(len(cp_cs))  # First uniformly sample a category title\n",
    "    sing = cat_ix >= len(cp)                # Singular vs plural\n",
    "    if prompt is None:                      # Uniformly sample a list beginning phrase (\"A list of...\") if not given\n",
    "        lprmpts = ((lprompts_sing_encoded if sing else lprompts_encoded) if tknzr is tokenizer else \\\n",
    "                   (lprompts_sing_encoded3 if sing else lprompts_encoded3)) if tknzr is not None else \\\n",
    "                   (lprompts_sing if sing else lprompts)\n",
    "        prmt, _ = lprmpts[np.random.randint(len(lprmpts))]\n",
    "    return cp_cs[cat_ix], prmt\n",
    "def gen_listnames_uniform(xcp, xcs, xp, n, prompt=None, tknzr=tokenizer, verbose=False, stac=None):\n",
    "    prmts, cats, ps, j, prmt = [], [], [], 0, None\n",
    "    if prompt is not None and tknzr is not None: prmt = pt.tensor(tknzr.encode(prompt), device=d)\n",
    "    stac_, stac = stac_sample(stac, n) if (stac is not None) else None, stac is not None\n",
    "    if stac: np.random.shuffle(stac_)\n",
    "    for i in (range(len(xcp)) if not stac else stac_):\n",
    "        prmts_, cats_ = [], []\n",
    "        cp, cs, p = xcp[i], xcs[i], xp[i]\n",
    "        cp_cs = cp + cs\n",
    "        for m in range(n if not stac else 1):\n",
    "            cat, prmt = gen_listname(cp, cp_cs, prompt, prmt, tknzr=tknzr)\n",
    "            prmts_.append(prmt), cats_.append(cat)\n",
    "            j += 1\n",
    "            if verbose and j % 100 == 0: sys_print(\"\\rGenerating list names, done: \" + str(j))\n",
    "        ps.append(p), prmts.append(prmts_), cats.append(cats_)\n",
    "    if verbose: sys_print(\"\\rGenerating list names, done: \" + str(j) + \", finished!\\n\")\n",
    "    return prmts, cats, ps, stac\n",
    "def gen_samples_uniform(xcp, xcs, xp, n,              # Weight testing samples (word lists) exactly uniformly\n",
    "                        rng=rng_train, prompt=None, tknzr=tokenizer, verbose=False, inds=False, stac=None, mlen=1e9):\n",
    "    xs, ys, sqlens, j, prmt = [], [], [], 0, None\n",
    "    prmts, cats, ps, stac = gen_listnames_uniform(xcp, xcs, xp, n, prompt=prompt, tknzr=tknzr, verbose=verbose, stac=stac)\n",
    "    for i in range(len(prmts)):\n",
    "        x, y, sqlen = [], [], []\n",
    "        for k in range(len(prmts[i])):\n",
    "            x_, y_ = gen_truncated_list(pt.hstack([prmts[i][k], cats[i][k]]), ps[i], rng=rng, mlen=mlen)\n",
    "            x.append(x_), y.append(y_), sqlen.append(len(x_))\n",
    "            j += 1\n",
    "            if verbose and j % 100 == 0: sys_print(\"\\rGenerating list elements, done: \" + str(j))\n",
    "        xs.append(x), ys.append(y), sqlens.append(sqlen)\n",
    "    if inds or stac: xs, ys, sqlens = sum(xs, []), sum(ys, []), sum(sqlens, [])\n",
    "    if verbose: sys_print(\"\\rGenerating list elements, done: \" + str(j) + \", finished!\\n\")\n",
    "    return (xs, ys, sqlens, np.arange(len(xcp)).repeat(n)) if inds else (xs, ys, sqlens)\n",
    "def gen_samples(xcp, xcs, xp, n,\n",
    "                rng=rng_train, prompt=None, tknzr=tokenizer, inds=False, mlen=1e9):\n",
    "    xs, ys, sqlens, j, prmt = [], [], [], 0, None   \n",
    "    if prompt is not None and tknzr is not None: prmt = pt.tensor(tknzr.encode(prompt), device=d)\n",
    "    xs, ys, sqlens, j = [], [], [], 0\n",
    "    n_sets, indices = len(xcp), []\n",
    "    for m in range(n):  # Maximise per-batch training diversity by randomly sampling the word lists (experimental)\n",
    "        i = np.random.randint(n_sets)\n",
    "        cp, cs, p = xcp[i], xcs[i], xp[i]\n",
    "        cp_cs = cp + cs\n",
    "        cat_ix, prmt = gen_listname(cp, cp_cs, prompt, prmt)\n",
    "        x_, y_ = gen_truncated_list(pt.hstack([prmt, cp_cs[cat_ix]]), p, rng=rng, mlen=mlen)\n",
    "        xs.append(x_), ys.append(y_), sqlens.append(len(x_)), indices.append(i)\n",
    "    return (xs, ys, sqlens, np.asarray(indices)) if inds else (xs, ys, sqlens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2  8 21  4  7  5 10 25] \n",
      "Train:\n",
      "['round fruits', 'wild animals', 'microorganisms', 'outback experiences', 'buildings', 'hats', 'holed pasta', 'rod shaped pasta', 'construction sounds', 'sounds of a building', 'biological examples of math in nature', 'non-biological examples of math in nature', 'timbers', 'woodland ecoregions', 'handcrafts', 'communication media', 'storage media', 'winds', 'scientific principles behind showers', 'scientific principles behind rain showers', 'spacecraft types', 'real spacecrafts', 'interpersonal tokens of trust', 'physical tokens that confer trust', 'digital tokens that confer trust']\n",
      "Test:\n",
      "['chemical elements', 'dramatic and literature elements', 'vehicles referred to as crafts', 'music', 'scientific cycles', 'machine learning algorithms', 'glassware', 'windings']\n",
      "Generating list names, done: 160, finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/ipykernel_launcher.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating list elements, done: 160, finished!\n"
     ]
    }
   ],
   "source": [
    "N_test = int(test_set_frac * N_wordlists)\n",
    "N_train = N_wordlists - N_test\n",
    "test_idx = np.asarray([ 2,  8, 21,  4,  7,  5, 10, 25])\n",
    "save_ld(test_idx, \"test.data\")\n",
    "# test_idx = load_ld(\"test.data\")\n",
    "# test_idx = np.array([0, 2])  # Round fruits and chemical elements\n",
    "train_idx = [i for i in range(N_wordlists) if i not in test_idx]\n",
    "print(test_idx, \"\\nTrain:\")\n",
    "print([cats[i][0] for i in train_idx])\n",
    "print(\"Test:\")\n",
    "print([cats[i][0] for i in test_idx])\n",
    "cats_train, cats_test = [cats[i] for i in train_idx], [cats[i] for i in test_idx]\n",
    "cats_sing_train, cats_sing_test = [cats_sing[i] for i in train_idx], [cats_sing[i] for i in test_idx]\n",
    "phrases_train, phrases_test = [phrases[i] for i in train_idx], [phrases[i] for i in test_idx]\n",
    "cats_e_test, cats_sing_e_test = [cats_e[i] for i in test_idx], [cats_sing_e[i] for i in test_idx]\n",
    "phrases_e_test = [phrases_e[i] for i in test_idx]\n",
    "cats_e_train, cats_sing_e_train = [cats_e[i] for i in train_idx], [cats_sing_e[i] for i in train_idx]\n",
    "phrases_e_train = [phrases_e[i] for i in train_idx]\n",
    "test_cats = [cats[i][0] for i in test_idx]\n",
    "test_xs, test_ys, test_sqlens= gen_samples_uniform(cats_e_test, cats_sing_e_test, phrases_e_test, TsN, mlen=max_len,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write function that takes a prompt, existing list and sampling params and returns gpt3's next token probs\n",
    "default_msp = {\n",
    "  \"best_of\": 1,\n",
    "}\n",
    "default_sp = {\n",
    "  \"temperature\": 1.0,\n",
    "  \"top_p\": 1.0,\n",
    "#   \"top_k\": -1.0,                 # todo: add code to apply this to gpt3 output (top100), max k ~=90, min = 2?\n",
    "  \"presence_penalty\": 0.0,\n",
    "  \"frequency_penalty\": 0.0,\n",
    "}\n",
    "default_params = {\n",
    "  \"engine\": \"davinci\",\n",
    "  \"model\": None,\n",
    "  \"max_tokens\": 1,\n",
    "  \"temperature\": 1.0,\n",
    "  \"top_p\": 1.0,\n",
    "#   \"top_k\": -1.0,\n",
    "  \"presence_penalty\": 0.0,\n",
    "  \"frequency_penalty\": 0.0,\n",
    "  \"n\": 1,\n",
    "  \"stream\": False,\n",
    "  \"logprobs\": 100,\n",
    "#       \"logit_bias\": {\"50256\": -100},\n",
    "  \"stop\": [\",\", \"\\n\"],\n",
    "}\n",
    "stop_tokens_e = tokenizer.encode(''.join(default_params[\"stop\"]))\n",
    "# Define and test the OpenAI API next token probability request (response-token-efficient streaming version)\n",
    "def format_gpt3_probs(choice, tokenize):\n",
    "    res, r = [], sorted([(np.e**v, k) for (k, v) in choice[\"logprobs\"][\"top_logprobs\"][0].items()])[::-1]\n",
    "    for i in range(len(r)):\n",
    "        k = gpt3_tokenizer.encode(r[i][1])\n",
    "        if len(k) == 1: res.append((r[i][0], k if tokenize else r[i][1]))\n",
    "    return res\n",
    "def p_req(s, tokenize=False, **kwargs):\n",
    "    use_stream = \"max_tokens\" in kwargs and kwargs[\"max_tokens\"] != 1\n",
    "    kwargs[\"prompt\"], kwargs[\"stream\"] = s, use_stream\n",
    "    with io.capture_output() as captured:\n",
    "        response, result = openai.Completion.create(**{**default_params, **kwargs}), []\n",
    "    return [(np.e**resp[\"choices\"][0][\"logprobs\"][\"token_logprobs\"][0], resp[\"choices\"][0][\"logprobs\"][\"tokens\"][0],\n",
    "             format_gpt3_probs(resp[\"choices\"][0], tokenize)) for resp in (response if use_stream else [response])]\n",
    "# todo: version to handle multiple choices for phrase level evaluation (response-token-expensive)\n",
    "def p_req_m(s, tokenize=False, **kwargs):\n",
    "    if \"max_tokens\" not in kwargs: kwargs[\"max_tokens\"] = 8\n",
    "    if \"n\" not in kwargs: kwargs[\"n\"] = 5\n",
    "    if \"best_of\" in kwargs:\n",
    "        kwargs[\"best_of\"] = int(round(kwargs[\"best_of\"]))\n",
    "        if kwargs[\"n\"] != 1: kwargs[\"best_of\"], kwargs[\"n\"] = kwargs[\"n\"], kwargs[\"best_of\"]\n",
    "    kwargs[\"prompt\"] = s\n",
    "    with io.capture_output() as captured:\n",
    "        response, tokens, probs = openai.Completion.create(**{**default_params, **kwargs}), [], []\n",
    "    for choice in response[\"choices\"]:\n",
    "        tks = [np.e**v for v in choice[\"logprobs\"][\"token_logprobs\"]]\n",
    "        tks = [(choice[\"logprobs\"][\"tokens\"][i], tks[i]) for i in range(len(tks))]\n",
    "        tokens.append(tks)\n",
    "        probs.append(format_gpt3_probs(choice, tokenize))\n",
    "    return tokens, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# a = p_req(\"A list of cyclic phenomena: day and night, induction coil, self-oscillation, tornado, runaway greenhouse effect,\")\n",
    "# b = p_req_m(\"A list of cyclic phenomena: day and night, induction coil, self-oscillation, tornado, runaway greenhouse effect,\")\n",
    "# print(sum([a_[0] for a_ in a[0][2]]))\n",
    "# print(','.join([''.join([b__[0] for b__ in b_]) for b_ in b[0]] + [''.join([a_[1] for a_ in a])]).replace('\\n', '⏎'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that takes a completion distribution (top 100) and target next token distribution (multinomial) and computes the\n",
    "# probability that the completion produces a desired output token\n",
    "def prob_corr(pred_p, target_p):\n",
    "    r = 0\n",
    "    if isinstance(pred_p, list):\n",
    "        for (p, token) in pred_p:\n",
    "            if target_p[token] > (lid_val + 1e-10): r += p\n",
    "    else:\n",
    "        r = np.sum(target_p[np.nonzero(pred_p > (lid_val + 1e-10))[0]])\n",
    "    return r\n",
    "# directly computes the similarity between target and predicted token distributions\n",
    "def score_corr(pred_p, target_p, distance=\"cross-entropy\", redistribute_mass=False, include_negatives=False):  \n",
    "    r = 0\n",
    "    if isinstance(pred_p, list) and not redistribute_mass:\n",
    "        for (p, token) in pred_p:\n",
    "            targ = target_p[token]\n",
    "            if targ > (lid_val + 1e-10) or include_negatives:\n",
    "                if   distance == \"unnormalized\":  r -= p * targ\n",
    "                elif distance == \"cross-entropy\": r -= p * np.log(targ)\n",
    "                elif distance == \"kl-divergence\": r += p * np.log(p / targ)\n",
    "                elif distance == \"bhattacharyya\": r += np.sqrt(p * targ)\n",
    "        if distance == \"bhattacharyya\": r = -np.log(r)\n",
    "    else:\n",
    "        p = pred_p\n",
    "        if isinstance(pred_p, list):\n",
    "            p_ = np.asarray([p for (p, _) in pred_p])\n",
    "            ts = np.asarray([t for (_, t) in pred_p])\n",
    "            unaccounted_mass = 1.0 - sum(p_)\n",
    "            n_missing_tokens = N_tokens - len(pred_p)\n",
    "            p = np.repeat(unaccounted_mass / n_missing_tokens, N_tokens)\n",
    "            p[ts] = p_\n",
    "        if not include_negatives:\n",
    "            pos = np.nonzero(target_p > (lid_val + 1e-10))[0]\n",
    "            p, target_p = p[pos], target_p[pos]\n",
    "        if   distance == \"unnormalized\":  r = -np.sum(p * targ)\n",
    "        elif distance == \"cross-entropy\": r = -np.sum(p * np.log(target_p))\n",
    "        elif distance == \"kl-divergence\": r =  np.sum(p * np.log(p / target_p))\n",
    "        elif distance == \"bhattacharyya\": r = -np.log(np.sum(np.sqrt(p * target_p)))\n",
    "    return -r\n",
    "# probability that a completion phrase is a desired missing list entry\n",
    "def prob_msp(outs, missing):\n",
    "    correct = 0\n",
    "    missing = set([phrase.lower() for phrase in missing])\n",
    "    for i in range(len(outs)):\n",
    "        out = outs[i].strip().lower()\n",
    "        if out not in missing and (out[:4] == 'the '): out = out[4:]\n",
    "        if out in missing: correct += 1\n",
    "    return correct / len(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   (   ' Fermi-Pasta-Ulam problem,',\n",
      "        array([  376,  7780,    72,    12, 34533,    64,    12,    52,  2543,\n",
      "        1917,    11]),\n",
      "        11),\n",
      "    (   ' maccheroncini di campofilone,',\n",
      "        array([8352, 2044,  261,   66, 5362, 2566, 1413, 1659,  346,  505,   11]),\n",
      "        11)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens = sum([[len(p) for p in p_ if len(p) < 1000] for p_ in phrases_e], [])  # Print longest list elements in dataset, get max\n",
    "phrs = sum([[p for p in p_ if len(p) < 1000] for p_ in phrases_e], [])\n",
    "m = np.max(lens)\n",
    "inds = [i for i in range(len(phrs)) if lens[i] == m]\n",
    "ree = [(tokenizer.decode(phrs[i].cpu().detach().numpy()), phrs[i].cpu().detach().numpy(), lens[i]) for i in inds]\n",
    "phrl_max = len(ree[0][1]) - 1\n",
    "pr(ree)\n",
    "phrl_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that takes sampling parameters, then generates n random incomplete list prompts (of length l), obtains completion\n",
    "# distributions (top 100 tokens or full multinomial) and evaluates the average score across the n prompts. n = 20 by default\n",
    "# All samples generated are stored fully for later training of sample-dependent sampling parameter (mixture) distribution\n",
    "sps_ = [\"top_p\", \"temperature\", \"presence_penalty\", \"frequency_penalty\"]  # sampling params                          #top_k\n",
    "msps_= sps_#[\"best_of\"] + sps_                                                 # meta sampling params\n",
    "create_folder(data_dir + learning_data_dir)\n",
    "create_folder(data_dir + learning_data_dir + \"sp_samples\")\n",
    "create_folder(data_dir + learning_data_dir + \"sp_samples_test\")\n",
    "create_folder(data_dir + learning_data_dir + \"msp_samples_nb\")\n",
    "create_folder(data_dir + learning_data_dir + \"msp_samples_nb_test\")\n",
    "def save_modeloutput(idx, n, dname, pnames, params, r, min_l, max_l, mdl, xs=None, ys=None, sqlens=None, inds=None, d=None):\n",
    "    engine_str = ','.join([str(params[k]) for k in [\"engine\", \"model\"] if k in params])\n",
    "    for i in idx:\n",
    "        create_folder(data_dir + learning_data_dir + dname + \"/\" + str(i))\n",
    "        ix, mdl_name = range(i * n, (i + 1) * n) if n else np.nonzero(inds == i)[0], mdl if isinstance(mdl, str) else mdl[\"name\"]\n",
    "        fn = str(time.time()) + '_' + '_'.join([str(v) for v in [params[k] for k in msps_] + [min_l, max_l]]) + '_' + mdl_name\n",
    "        input_data = [d[j] for j in ix] if d else [xs[ix], ys[ix], sqlens[ix]]\n",
    "        save_ld((params, input_data, inds, [r[j] for j in ix], str(mdl)), dname + \"/\" + str(i) + \"/\" + fn, compress=9)\n",
    "def eval_sp(params, min_l=0, max_l=1e9, n=20, prmt=None, phase=\"train\", uniform=True, mdl='gpt3'):\n",
    "    res, max_l = [], int(max_l), \n",
    "    tknzr = gpt3_tokenizer if mdl == \"gpt3\" else tokenizer\n",
    "    xcp, xcs, xp = \\\n",
    "      ((cats_e3_test,  cats_sing_e3_test,  phrases_e3_test) if phase == \"test\" else \\\n",
    "       (cats_e3_train, cats_sing_e3_train, phrases_e3_train)) if mdl == \"gpt3\" else \\\n",
    "      ((cats_e_test,  cats_sing_e_test,  phrases_e_test) if phase == \"test\" else \\\n",
    "       (cats_e_train, cats_sing_e_train, phrases_e_train))\n",
    "    xs, ys, sqlens, inds = gen_samples_uniform(xcp, xcs, xp, n, prompt=prmt, tknzr=tknzr, inds=True, rng=[min_l, max_l]) \\\n",
    "           if uniform else gen_samples        (xcp, xcs, xp, n, prompt=prmt, tknzr=tknzr, inds=True, rng=[min_l, max_l])\n",
    "    if mdl == 'gpt3': r = [p_req(gpt3_tokenizer.decode(x_.detach().cpu().numpy()), **params) for x_ in xs] \n",
    "    else:             r = mdl[\"probabilities\"](xs, ys, sqlens, **params)\n",
    "    save_modeloutput(np.unique(inds), None, \"sp_samples\", sps_, params, r, min_l, max_l, mdl, xs, ys, sqlens, inds)\n",
    "    return np.mean([score_corr(r[i], ys[i]) for i in range(len(r))])\n",
    "def eval_sp_conv(params, tol=0.01, **kwargs):\n",
    "    center, samples = np.inf, []\n",
    "    while True:\n",
    "        samples.append(eval_sp(params, n=2, **kwargs))\n",
    "        new_center = np.mean(samples)\n",
    "        if abs(center - new_center) < tol: return new_center, samples\n",
    "        new_center = center\n",
    "# This metric differs depending on tokenisation, so for the testing of models, a full phrase accuracy function is required\n",
    "def gen_phraselevel_samples_uniform(phase, min_l, max_l, n, prmt):\n",
    "    xcp, xcs, xp = ((cats_test, cats_sing_test, phrases_test) if phase==\"test\" else (cats_train, cats_sing_train, phrases_train))\n",
    "    d, (prmts, cats, ps,) = [], gen_listnames_uniform(xcp, xcs, xp, n, prompt=prmt, tknzr=None)\n",
    "    for i in range(len(xcp)):\n",
    "        x, y, sqlen = [], [], []\n",
    "        for m in range(n):\n",
    "            prompt, cat, p = prmts[i][m], cats[i][m], ps[i][m]\n",
    "            phr_ix = np.random.choice(len(p), np.random.randint(min_l, min(max_l, len(p) - 1)), replace=False)\n",
    "            missing_ix = [i for i in range(len(p)) if i not in phr_ix]\n",
    "            prompt += ' ' + cat + ': ' + ''.join([p[j] + ', ' for j in phr_ix])\n",
    "            d.append([prompt[:-1], [p[j] for j in missing_ix]])\n",
    "    return d\n",
    "strip_the = lambda x: x[4:] if x.lower()[:4] == 'the ' else x\n",
    "strip_tl = lambda x: strip_the(x.strip().lower())\n",
    "strip_comma = lambda x: [x_.strip() for x_ in (x[:-1] if len(x) > 1 else x)]\n",
    "strip_lower = lambda x: [strip_tl(x_) for x_ in (x[:-1] if len(x) > 1 else x)]\n",
    "def ensemble_two_models_results(m2ensemble_frac, r, r2):\n",
    "    bof = len(r[0][0])\n",
    "    n_replace = int(bof) * m2ensemble_frac\n",
    "    r_new = [[strip_comma(''.join([r___[0] for r___ in r__]).split(',')) for r__ in r_] for r_ in r]\n",
    "    for i in range(len(r)):\n",
    "        cur_pool = set(sum([strip_lower(''.join([r__[0] for r__ in r_]).split(',')) for r_ in r[i][:bof - n_replace]], []))\n",
    "        new_pool = sum([strip_comma(''.join([r__[0] for r__ in r_]).split(',')) for r_ in r2[i]], [])\n",
    "        for j in range(n_replace):\n",
    "            n_phrases = len(r[i][j])\n",
    "            add = []\n",
    "            while len(add) < n_phrases and len(new_pool) > 0:\n",
    "                new_phr = new_pool[0]\n",
    "                stripped_new_phr = strip_tl(new_phr)\n",
    "                if stripped_new_phr not in cur_pool:\n",
    "                    add.append(new_phr)\n",
    "                    cur_pool.add(stripped_new_phr)\n",
    "                new_pool = new_pool[1:]\n",
    "            if len(add) > 0:\n",
    "                r_new[i][j + bof - n_replace] = add\n",
    "    return [sum(r_, []) for r_ in r_new]\n",
    "def test_sp(params, min_l=0, max_l=1e9, n1=3, n2=10, prmt=None, phase=\"train\", uniform=True, max_tokens=phrl_max,\n",
    "            mdl='gpt3', mdl2=None, m2ensemble_frac=0.4):\n",
    "    max_l, test_acc = int(max_l), None\n",
    "    d = gen_phraselevel_samples_uniform(phase, min_l, max_l, n1, prmt)\n",
    "    params['n'], params[\"max_tokens\"] = n2, max_tokens\n",
    "    if mdl == 'gpt3': r = [p_req_m(d_[0], **params)[0] for d_ in d]  # Request predictions from OpenAI\n",
    "    else:             r = mdl[\"completions\"]([d_[0] for d_ in d], **params)\n",
    "    save_modeloutput(range(len(cats_train)), n1, \"msp_samples_nb\", msps_, params, r, min_l, max_l, mdl, d=d)\n",
    "    if mdl2 is not None:\n",
    "        if mdl2 == 'gpt3': r2 = [p_req_m(d_[0], **params)[0] for d_ in d]  # Ensemble with 2nd model according to m2ensemble_frac\n",
    "        else:              r2 = mdl2[\"completions\"]([d_[0] for d_ in d], **params)  # This loads precomputed outputs so no resave\n",
    "        r = ensemble_two_models_results(m2ensemble_frac, r, r2)\n",
    "    else:\n",
    "        r = [sum([strip_comma(''.join([r___[0] for r___ in r__]).split(',')) for r__ in r_], []) for r_ in r]\n",
    "    #     r = [sum([strip_comma(''.join([r___[0] for r___ in r__]).split(','))[:max_l - min_l] for r__ in r_], []) for r_ in r]\n",
    "\n",
    "    if phase == \"train\":  # for validation, also output the test set accuracy\n",
    "        n1_ = n1 * int((1 - test_set_frac) / test_set_frac)  # Use approximately enough samples to converge\n",
    "        d_test = gen_phraselevel_samples_uniform(\"test\", min_l, max_l, n1_, prmt)\n",
    "        if mdl == 'gpt3': r_test = [p_req_m(d_[0], **params)[0] for d_ in d_test]\n",
    "        else:             r_test = mdl[\"completions\"]([d_[0] for d_ in d_test], **params)\n",
    "        save_modeloutput(range(len(cats_test)), n1_, \"msp_samples_nb_test\", msps_, params, r_test, min_l, max_l, mdl, d=d_test)\n",
    "        if mdl2 is not None:\n",
    "            if mdl2 == 'gpt3': r2_test = [p_req_m(d_[0], **params)[0] for d_ in d_test]\n",
    "            else:              r2_test = mdl2[\"completions\"]([d_[0] for d_ in d_test], **params)\n",
    "            r_test = ensemble_two_models_results(m2ensemble_frac, r_test, r2_test)\n",
    "        else:\n",
    "            r_test = [sum([strip_comma(''.join([r___[0] for r___ in r__]).split(',')) for r__ in r_], []) for r_ in r_test]\n",
    "#r_test =[sum([strip_comma(''.join([r___[0] for r___ in r__]).split(','))[:max_l - min_l] for r__ in r_], []) for r_ in r_test]\n",
    "        test_acc = np.mean([prob_msp(r_test[i], d_test[i][1]) for i in range(len(r_test))])\n",
    "\n",
    "    return np.mean([prob_msp(r[i], d[i][1]) for i in range(len(r))]), test_acc\n",
    "def test_sp_conv(params, tol=0.01, **kwargs):\n",
    "    center, ys, i = np.inf, [], 1\n",
    "    while True:\n",
    "        ys.append(test_sp(params, n1=1, **kwargs))\n",
    "        new_center = np.mean([s[0] for s in ys])\n",
    "        if abs(center - new_center) < tol and i >= 3:\n",
    "            print([s[1] for s in ys])\n",
    "            print([s[0] for s in ys])\n",
    "            sys_print(str((\"Test acc:\", 100*np.mean([s[1] for s in ys]), \"sd:\", np.std([100*s[1] for s in ys])))+\"\\n\", False)\n",
    "            return new_center, ys\n",
    "        center = new_center\n",
    "        i += 1\n",
    "# eval_sp(default_ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define next batch function\n",
    "def adapt_form(xs, ys, sqlens, mlen=max_len, repl_finalcomma=True):\n",
    "    xs = pt.vstack([F.pad(x, (0, max(0, mlen - len(x))), mode='constant', value=pad_token)[:mlen] for x in xs])\n",
    "    _ys = pt.vstack(ys) if ys is not None else None\n",
    "    if repl_finalcomma and (lastcomma_repl != ',') and ys is not None:\n",
    "        _ys[:, repl_token] += _ys[:, comma_token] - lid_val\n",
    "        _ys[:, comma_token] = lid_val\n",
    "    return xs, _ys, pt.tensor(sqlens, device=d)\n",
    "curr_ri = np.zeros(len(train_idx), dtype=int)\n",
    "def next_batch(sz):\n",
    "    global cats_e_train, cats_sing_e_train, phrases_e_train, curr_ri\n",
    "    return adapt_form(*gen_samples_uniform(cats_e_train, cats_sing_e_train, phrases_e_train, sz, stac=curr_ri, mlen=max_len))\n",
    "#     return adapt_form(*gen_samples(cats_e_train, cats_sing_e_train, phrases_e_train, sz, mlen=max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also create a gpt3 prompt-completion-based regression model to predict values/densities of sampling parameters (might work if\n",
    "# it's possible to find a humanlike transliteration of the problem statement that gpt3 can bootstrap on to find the params, or to\n",
    "# find some (possibly entirely textual) representation of a params-correlating multidimensional metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98e7209bb580419596b0a51462c16439",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/689 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5505fe31b9b34450973e5aea2ecaf68c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/5.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2 device: cuda:0\n",
      "Pretrained parameters loaded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1267"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if 'model' in locals():\n",
    "    del model\n",
    "pt.cuda.empty_cache()\n",
    "print(gc.collect())\n",
    "create_folder(\"models\")\n",
    "create_folder(\"models/pretrained\")\n",
    "create_folder(\"models/pretrained/GPT2LMHead\")\n",
    "model_ = GPT2LMHeadModel.from_pretrained(gpt2_modelkey,\n",
    "    output_hidden_states=False, output_attentions=False, \n",
    "    cache_dir=\"models/pretrained/GPT2LMHead\")\n",
    "if pt.cuda.device_count() > 1:\n",
    "    device_map = {0: [0, 1, 2],\n",
    "                  1: [3, 4, 5, 6, 7, 8],\n",
    "                  2: [9, 10, 11, 12, 13, 14],\n",
    "                  3: [15, 16, 17, 18, 19, 20],\n",
    "                  4: [21, 22, 23, 24, 25, 26, 27],\n",
    "                  5: [28, 29, 30, 31, 32, 33, 34],\n",
    "                  6: [35, 36, 37, 38, 39, 40, 41],\n",
    "                  7: [42, 43, 44, 45, 46, 47],\n",
    "                 }\n",
    "    model_.parallelize(device_map)\n",
    "else:\n",
    "    model_ = model_.to(d)\n",
    "print(\"GPT2 device:\", model_.device)\n",
    "model_.resize_token_embeddings(N_tokens)\n",
    "pad_token = model_.config.pad_token_id = model_.config.eos_token_id\n",
    "pad_token = pt.tensor(pad_token, device=d)\n",
    "repl_token = pt.tensor(tokenizer.encode(lastcomma_repl)[0], device=d) if lastcomma_repl != 'EOS' else pad_token\n",
    "n_embd = pt.tensor(model_.config.n_embd, device=d)\n",
    "# model = nn.parallel.DistributedDataParallel(model_, device_ids=[d])\n",
    "# model = nn.DataParallel model_, device_ids=list(range(pt.cuda.device_count()))) if dev != \"cpu\" else model_\n",
    "model = model_\n",
    "mname_fn = gpt2_modelkey\n",
    "print(\"Pretrained parameters loaded\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "llayer = None #nn.Linear(n_embd, N_tokens, bias=False).to(d)#.cpu()\n",
    "# nn.init.xavier_uniform_(llayer.weight)\n",
    "# llayer = nn.DataParallel(llayer, device_ids=list(range(pt.cuda.device_count()))) if dev != \"cpu\" else llayer\n",
    "# llayer = nn.parallel.DistributedDataParallel(llayer, device_ids=list(range(pt.cuda.device_count()))).to(d)\n",
    "# softmax = nn.Softmax()\n",
    "bcewl_loss = nn.BCEWithLogitsLoss()#.to(d)#.cpu()\n",
    "# bcewl_loss = nn.DataParallel(bcewl_loss, device_ids=list(range(pt.cuda.device_count()))).to(d) if dev != \"cpu\" else bcewl_loss\n",
    "# bcewl_loss = nn.parallel.DistributedDataParallel(bcewl_loss, device_ids=list(range(pt.cuda.device_count()))).to(d)\n",
    "# nll_loss = nn.NLLLoss()\n",
    "# kl_loss = nn.KLDivLoss()\n",
    "optimizer = pt.optim.AdamW(model.parameters(), lr=learning_rate, eps=adam_epsilon)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=n_sched_warmup, num_training_steps=N_train_batches)\n",
    "def sequence_mask(lengths, maxlen=None, dtype=pt.int):\n",
    "    if maxlen is None:\n",
    "        maxlen = lengths.max()\n",
    "    row_vector = pt.arange(0, maxlen, 1, device=d)\n",
    "    matrix = pt.unsqueeze(lengths, dim=-1)\n",
    "    mask = row_vector < matrix\n",
    "\n",
    "    mask = mask.type(dtype)\n",
    "    return mask\n",
    "def train_step():\n",
    "    global model, llayer, bcewl_loss, optimizer, bsz, scheduler\n",
    "    x_batch, y_batch, sqlens_batch = next_batch(bsz)\n",
    "\n",
    "    model.zero_grad()\n",
    "    mask = sequence_mask(sqlens_batch, max_len)\n",
    "    outputs = model(x_batch.long(), attention_mask=mask)  # Get logits\n",
    "    logits = outputs[0][[pt.arange(x_batch.shape[0]), sqlens_batch - 1]]\n",
    "\n",
    "#     logsofts = pt.log(softmax(logits))\n",
    "    loss = bcewl_loss(logits, y_batch.float())\n",
    "    loss = loss.mean()\n",
    "    correct = pt.mean((y_batch[pt.arange(batch_size), pt.argmax(logits, axis=1)] > (lid_val + 1e-10)).float())\n",
    "    loss_, correct_ = loss.detach().cpu().numpy(), correct.detach().cpu().numpy()\n",
    "    \n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    return loss_, correct_\n",
    "\n",
    "def inference(x, sqlens, past=None, return_states=False, seq_maxlen=max_len, add=0):\n",
    "    global model, llayer\n",
    "\n",
    "    multitoken = x.shape[1] > 1\n",
    "    mask = sequence_mask(sqlens, seq_maxlen) if (multitoken or add != 0) else None\n",
    "    if add != 0: mask = pt.cat([mask, pt.ones(mask.shape[0], add).to(d)], dim=1)  # Append mask entry for new stream token\n",
    "    outputs = model(x.long(), attention_mask=mask, use_cache=None if not return_states else True, past_key_values=past)\n",
    "    logits = outputs[0][[pt.arange(x.shape[0]), sqlens - 1]] if multitoken else outputs[0].squeeze(1)\n",
    "\n",
    "    return (logits, outputs[1]) if return_states else logits  # Optionally return the past states needed to restore the stream\n",
    "def eval_test(x, y, sqlens):\n",
    "    global bcewl_loss\n",
    "\n",
    "    with pt.no_grad():\n",
    "        logits = inference(x, sqlens)\n",
    "        loss = bcewl_loss(logits, y.float())\n",
    "        loss = loss.mean()\n",
    "        correct = pt.mean((y[pt.arange(x.shape[0]), pt.argmax(logits, axis=1)] > (lid_val + 1e-10)).float())\n",
    "        loss_, correct_ = loss.detach().cpu().numpy(), correct.detach().cpu().numpy()\n",
    "    return loss_, correct_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_i = -1  # Batch 0 is the first iteration, where testing occurs without any training\n",
    "best_acc, best_loss = 0, np.inf\n",
    "best_acc_idx = -1\n",
    "out_str = ''\n",
    "create_folder(\"models\")\n",
    "create_folder(\"model_logs\")\n",
    "create_folder(\"models/\" + model_name)\n",
    "graphs_folder = \"graphs\"\n",
    "create_folder(graphs_folder)\n",
    "train_loss, train_accuracy, test_loss, test_accuracy = [], [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_training(verbose=True):\n",
    "    global model, batch_i, best_acc, best_loss, best_acc_idx, train_loss, train_accuracy, test_loss, test_accuracy\n",
    "    \n",
    "    model.train()\n",
    "    iter_loss, iter_accuracy, b_no_inp = [], [], 0\n",
    "    while batch_i < N_train_batches:\n",
    "        batch_i += 1\n",
    "        if batch_i > 0:\n",
    "            gc.collect()\n",
    "            if dev != \"cpu\": pt.cuda.empty_cache()\n",
    "            b_loss, b_accuracy = train_step()\n",
    "            mname_fn = model_name\n",
    "            if verbose:\n",
    "                sys_print('\\rLoss, accuracy: ' + str(np.mean(b_loss)) + ', ' + str(np.mean(b_accuracy)) + \\\n",
    "                          ' @ batch '+ str(batch_i) + ' (' + str(batch_i * batch_size) + ' samples) complete.                  ')\n",
    "            iter_loss.append(b_loss)\n",
    "            iter_accuracy.append(b_accuracy)\n",
    "\n",
    "        if batch_i % log_period_batches == 0:  # Test on test set\n",
    "            model.eval()\n",
    "            loss, accuracy = [], []\n",
    "            out_str = '\\n'\n",
    "            for i in range(N_test):\n",
    "                test_X, test_Y, test_Sqlens = adapt_form(test_xs[i], test_ys[i], test_sqlens[i], repl_finalcomma=batch_i > 0)\n",
    "                feed_batches = [range(len(test_X))[i * bsz:(i + 1) * bsz] for i in range((len(test_X) // bsz) + \\\n",
    "                                                                                        (1 if (len(test_X) % bsz) != 0 else 0))]\n",
    "                if dev != \"cpu\": pt.cuda.empty_cache()\n",
    "                ls, cs = zip(*[eval_test(test_X[inds], test_Y[inds], test_Sqlens[inds]) for inds in feed_batches])\n",
    "                loss.append(np.mean(ls))\n",
    "                accuracy.append(np.mean(cs))\n",
    "                out_str += test_cats[i] + ': ' + str(loss[-1]) + ', ' + str(accuracy[-1]) + '\\n'\n",
    "            \n",
    "            test_l, test_a = np.mean(loss), np.mean(accuracy)\n",
    "            test_loss.append(test_l)\n",
    "            test_accuracy.append(test_a)\n",
    "            if batch_i == 0:\n",
    "                iter_loss, iter_accuracy = [test_l], [test_a]\n",
    "            train_l, train_a = np.mean(iter_loss), np.mean(iter_accuracy)\n",
    "            train_loss.append(train_l)\n",
    "            train_accuracy.append(train_a)\n",
    "            iter_loss, iter_accuracy = [], []\n",
    "            \n",
    "            val_a = 0\n",
    "            if ((test_a > best_acc and test_a >= train_a) or \\\n",
    "              (batch_i // log_period_batches) == 1) and batch_i > 0:      # Save best accuracy model\n",
    "                best_acc = test_a\n",
    "                best_loss = test_l\n",
    "                best_acc_idx = batch_i // log_period_batches\n",
    "                pt.save({\"model\": model.state_dict(),\n",
    "#                          \"llayer\": llayer.state_dict(),\n",
    "#                          \"softmax\": softrmax.state_dict(),\n",
    "                         \"bcewl_loss\": bcewl_loss.state_dict(),\n",
    "#                          \"nll_loss\": nll_loss.state_dict(),\n",
    "#                          \"kl_loss\": kl_loss.state_dict(),\n",
    "                         \"optimizer\": optimizer.state_dict(),\n",
    "                         \"scheduler\": scheduler.state_dict(),\n",
    "                         }, \"./models/\" + model_name + '/' + model_name)\n",
    "                b_no_inp = 0\n",
    "            else:\n",
    "                b_no_inp += log_period_batches\n",
    "                \n",
    "            if verbose:\n",
    "                clear_output()\n",
    "                print(out_str + \"Batch\", batch_i, ':', train_a, test_a, \"loss:\", train_l, test_l, \\\n",
    "                      \"Best:\", best_acc, best_loss, 'idx:', best_acc_idx)\n",
    "                fig = plt.figure()\n",
    "                fig.set_size_inches(16, 5)\n",
    "                g = fig.add_subplot(1,2,1)\n",
    "                g.grid()\n",
    "                g.plot(train_accuracy, label='train acc')\n",
    "                g.plot(test_accuracy, label='test acc')\n",
    "                g.legend(loc='lower right')\n",
    "#                 g.axhline(y=0.714, ls='--', color='grey')\n",
    "\n",
    "                g = fig.add_subplot(1,2,2)\n",
    "                g.grid()\n",
    "                g.plot(train_loss, label='train loss')\n",
    "                plt.yscale(\"log\")\n",
    "                g.plot(test_loss, label='test loss')\n",
    "                plt.yscale(\"log\")\n",
    "                g.legend(loc='upper right')\n",
    "\n",
    "                save_ld((train_accuracy, test_accuracy, train_loss, test_loss),\n",
    "                        \"model_logs/\" + model_name + '_log_latest', pad=False)\n",
    "                plt.savefig(graphs_folder + '/' + model_name + \"_curve_latest\" + '.pdf', format='pdf')\n",
    "                plt.show()\n",
    "\n",
    "            model.train()\n",
    "    return best_acc, best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "chemical elements: 0.0849455, 0.95\n",
      "dramatic and literature elements: 0.11221207, 0.75\n",
      "vehicles referred to as crafts: 0.14465162, 0.7\n",
      "music: 0.113717936, 1.0\n",
      "scientific cycles: 0.15556028, 0.65000004\n",
      "machine learning algorithms: 0.13153255, 0.7\n",
      "glassware: 0.12391133, 0.90000004\n",
      "windings: 0.12481538, 0.6\n",
      "Batch 0 : 0.78125 0.78125 loss: 0.12391833 0.12391833 Best: 0 inf idx: -1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/ipykernel_launcher.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss, accuracy: 0.00016544458, 0.328125 @ batch 10 (640 samples) complete.                  "
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at inline_container.cc:274] . unexpected pos 4768265088 vs 4768264984",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    371\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m                 \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0mnum_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melement_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m         \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-5ff61b84ca1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0miterate_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-23-9f01046cbd3f>\u001b[0m in \u001b[0;36miterate_training\u001b[0;34m(verbose)\u001b[0m\n\u001b[1;32m     55\u001b[0m                          \u001b[0;34m\"optimizer\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                          \u001b[0;34m\"scheduler\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m                          }, \"./models/\" + model_name + '/' + model_name)\n\u001b[0m\u001b[1;32m     58\u001b[0m                 \u001b[0mb_no_inp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    371\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m                 \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m         \u001b[0m_legacy_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_end_of_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:274] . unexpected pos 4768265088 vs 4768264984"
     ]
    }
   ],
   "source": [
    "print(gc.collect())\n",
    "iterate_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './models/ernst_one/ernst_one'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-f15f6e152fae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./models/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# cpoint = pt.load(\"./models/\" + 'ernst_one - 2k ts' + \"/\" + model_name)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# cpoint = pt.load(\"./models/\" + 'ernst_one - 3k ts' + \"/\" + model_name)  # ~3000 training samples observed has current optimum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './models/ernst_one/ernst_one'"
     ]
    }
   ],
   "source": [
    "# Load best model (checkpoint)\n",
    "pt.cuda.empty_cache()\n",
    "gc.collect()\n",
    "cpoint = pt.load(\"./models/\" + model_name + '/' + model_name)\n",
    "model.load_state_dict(cpoint['model'])\n",
    "bcewl_loss.load_state_dict(cpoint['bcewl_loss'])\n",
    "optimizer.load_state_dict(cpoint['optimizer'])\n",
    "scheduler.load_state_dict(cpoint['scheduler'])\n",
    "# llayer.load_state_dict(cpoint['llayer'])\n",
    "mname_fn = model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_filtering(logits, tcounts=None, filter_value=-float('Inf'),\n",
    "                  top_k=0, top_p=0.0, temperature=1.0, frequency_penalty=0.0, presence_penalty=0.0):\n",
    "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
    "        Args:\n",
    "            logits: logits distribution shape (vocabulary size)\n",
    "            top_k >0: keep only top k tokens with highest probability (top-k filtering).\n",
    "            top_p >0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
    "                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
    "    \"\"\"\n",
    "    assert logits.dim() == 1  # batch size 1 for now - could be updated for more but the code would be less clear\n",
    "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
    "    if tcounts is not None: logits -= (tcounts * frequency_penalty) + ((tcounts > 0) * presence_penalty)\n",
    "    logits /= temperature\n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the last token of the top-k\n",
    "        indices_to_remove = logits < pt.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p > 0.0:\n",
    "        sorted_logits, sorted_indices = pt.sort(logits, descending=True)\n",
    "        cumulative_probs = pt.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "        \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "padder = int(pad_token.detach().cpu().numpy())\n",
    "# stop_tokens_e = [tokenizer.encode(t)[0] for t in [\"<|endoftext|>\"]]\n",
    "def gprobs(s, past=None, return_sts=False, tcounts=None, add=0, **kwargs):  # Inference and sampling for tokens\n",
    "    global model\n",
    "    xs, mlen = None, None\n",
    "    if isinstance(s, tuple):    # s either list of token tensors or tuple of preformatted 2d tensors\n",
    "        xs, _, sqlen = s\n",
    "        mlen = max(sqlen)\n",
    "    else:\n",
    "        sqlen = [len(s_) for s_ in s]\n",
    "        mlen = max(sqlen)\n",
    "        xs, _, sqlen = adapt_form([pt.tensor(s_).to(d) for s_ in s], None, sqlen, mlen=mlen)\n",
    "    model.eval()\n",
    "    y_hat = inference(xs, sqlen, seq_maxlen=mlen, add=add, past=past, return_states=return_sts)\n",
    "    if return_sts: y_hat, states = y_hat\n",
    "    y_hat = pt.vstack([F.softmax(top_filtering(y_hat[i], tcounts[i] if tcounts is not None else None,\n",
    "                                               **kwargs), dim=0) for i in range(len(xs))])\n",
    "    return (y_hat, states) if return_sts else y_hat\n",
    "def append_next_token(sent, olen=None, top_k=-1, top_p=0.9, temperature=1.0):  # Interface for field testing\n",
    "    print(\"k =\", top_k, \", p =\", top_p, \", temp =\", temperature)\n",
    "    tokens = tokenizer.encode(sent)\n",
    "    ou = tokens.copy()\n",
    "    tcounts = pt.zeros(N_tokens, dtype=int).to(d)\n",
    "    for token in tokens: tcounts[token] += 1\n",
    "    probs = gprobs([tokens], top_k=top_k, top_p=top_p, temperature=temperature, tcounts=[tcounts])[0]\n",
    "    token = pt.multinomial(probs, 1).detach().cpu().numpy()[0]\n",
    "    ou += [token]\n",
    "    prev_len = len(sent) if olen is None else olen\n",
    "    sent_new = tokenizer.decode(ou)\n",
    "    print(sent[:prev_len] + '➡' + sent_new[prev_len:])\n",
    "    return sent_new\n",
    "def gen_probs(s, **kwargs):  # Adapter for strings\n",
    "    inp = [tokenizer.encode(s_) for s_ in s]\n",
    "    return gprobs(inp, **kwargs)\n",
    "# bszinf = 256\n",
    "bszinf = bsz * 16\n",
    "def shift_s(st, sql):\n",
    "    r = pt.ones(st.shape).to(d)\n",
    "    for k in range(r.shape[0]):\n",
    "        r[k, :, -sql[k]:] = st[k, :, :sql[k]]\n",
    "    return r\n",
    "def gen_completions(s, n=1, max_tokens=8, best_of=1, **kwargs):  # Completion generator equivalent to OpenAI's for GPT3\n",
    "    n_bats, best_of = int(np.ceil(len(s) / int(bsz // 2))), int(round(best_of))\n",
    "    if n == 1 and best_of != 1: n, best_of = best_of, n\n",
    "    if best_of == 1 and n != 1: best_of = n\n",
    "    gc.collect()\n",
    "    outputs = []\n",
    "    for i in range(n_bats):\n",
    "        s_batch = s[i * (bsz // 2):(i + 1) * (bsz // 2)]\n",
    "        tc_b = []\n",
    "        for s_ in s_batch:\n",
    "            tc = pt.zeros(N_tokens, dtype=int).to(d)\n",
    "            for t in tokenizer.encode(s_): tc[t] += 1\n",
    "            tc_b.append(tc)\n",
    "        p, sts = gen_probs(s_batch, return_sts=True, tcounts=tc_b, **kwargs)\n",
    "        sql_b = [len(tokenizer.encode(s_)) for s_ in s_batch]\n",
    "        mlen = max(sql_b)\n",
    "        sql_b = pt.tensor(sql_b).to(d)\n",
    "        tokens = pt.multinomial(p, n, replacement=True) \n",
    "        outs, avg_logprobs = [], [] # first use the (as yet undiverged) token distribution to generate n tokens for each sample\n",
    "        for j in range(n):\n",
    "            tks = tokens[:, j]\n",
    "            gc.collect(), pt.cuda.empty_cache()\n",
    "            tc_b_itr = [t.clone() for t in tc_b]\n",
    "            for j in range(len(tc_b_itr)): tc_b_itr[j][tks[j]] += 1\n",
    "            p, st = gprobs((pt.unsqueeze(tks, -1), None, sql_b), past=sts, return_sts=True, tcounts=tc_b_itr, add=1, **kwargs)\n",
    "            su, ls = pt.log(p[pt.arange(p.shape[0]), tks]), pt.ones(p.shape[0]).to(d)\n",
    "            out = [tks]\n",
    "            for token_i in range(max_tokens - 1):\n",
    "                t = pt.multinomial(p, 1).to(d)[:, 0]\n",
    "                out.append(t)\n",
    "                for j in range(len(tc_b_itr)): tc_b_itr[j][t[j]] += 1\n",
    "                cont = t != pad_token\n",
    "                ls += cont.int()\n",
    "                su += cont * pt.log(p[pt.arange(p.shape[0]), t])\n",
    "                if token_i == max_tokens - 1: break\n",
    "                p, st = gprobs((pt.unsqueeze(t,-1),None,sql_b), past=st,return_sts=True,tcounts=tc_b_itr,add=token_i+2,**kwargs)\n",
    "            outs.append(pt.vstack(out).T)\n",
    "            avg_logprobs.append(su / ls)\n",
    "        gc.collect()\n",
    "        pt.cuda.empty_cache()\n",
    "        outs = pt.stack(outs, 1)\n",
    "        avg_logprobs = pt.vstack(avg_logprobs).T\n",
    "        s1 = outs.shape[0]\n",
    "        idx = pt.argsort(avg_logprobs, axis=1)[:, :best_of].repeat_interleave(max_tokens, 1).reshape(s1, best_of, max_tokens)\n",
    "        outs = pt.gather(outs, 1, idx)\n",
    "        outputs += [[[(tokenizer.decode([x_]),) for x_ in x] for x in o] for o in outs.cpu().detach().numpy()]\n",
    "#     pr([[''.join([x_[0] for x_ in x]) for x in o] for o in outputs])\n",
    "    return outputs\n",
    "mdl = {\"completions\": gen_completions, \"probabilities\": gprobs, \"name\": mname_fn + ',' + gpt2_modelkey, \"mstr\": str(model)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | freque... | presen... | temper... |   top_p   |\n",
      "-------------------------------------------------------------------------\n",
      "[0.1015376984126984, 0.02619047619047619, 0.07531415343915344]\n",
      "[0.16047619047619047, 0.09333333333333332, 0.1255238095238095]\n",
      "('Test acc:', 6.768077601410933, 'sd:', 3.1230351821437146)\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.1264  \u001b[0m | \u001b[0m 1.742   \u001b[0m | \u001b[0m 0.9436  \u001b[0m | \u001b[0m 0.6675  \u001b[0m | \u001b[0m 0.4918  \u001b[0m |\n",
      "[0.2810515873015873, 0.3152749925727867, 0.3259983224621383, 0.2786822452215254, 0.2665308105565458, 0.17806937214831953]\n",
      "[0.4514128590707538, 0.43228088578088575, 0.495135414293309, 0.5435932009167304, 0.3957303515198252, 0.4674807380442055]\n",
      "('Test acc:', 27.426788837715048, 'sd:', 4.784896797983327)\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m 0.4643  \u001b[0m | \u001b[95m 0.1762  \u001b[0m | \u001b[95m 1.327   \u001b[0m | \u001b[95m 0.2361  \u001b[0m | \u001b[95m 0.6357  \u001b[0m |\n",
      "[0.023429232804232803, 0.025049603174603172, 0.03125, 0.049197330447330455, 0.034027777777777775]\n",
      "[0.057080808080808086, 0.035603174603174595, 0.06672150072150072, 0.07425974025974026, 0.04933333333333333]\n",
      "('Test acc:', 3.2590788840788845, 'sd:', 0.9169988727509437)\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.0566  \u001b[0m | \u001b[0m 0.3115  \u001b[0m | \u001b[0m 0.6549  \u001b[0m | \u001b[0m 0.9465  \u001b[0m | \u001b[0m 0.9964  \u001b[0m |\n",
      "[0.14314574314574316, 0.2347222222222222, 0.1597222222222222, 0.2341269841269841, 0.21140491452991453, 0.10992063492063492, 0.18560606060606064]\n",
      "[0.4762380952380953, 0.385, 0.33833333333333326, 0.30772294372294373, 0.30266666666666664, 0.3064444444444444, 0.3382222222222222]\n",
      "('Test acc:', 18.266411168196882, 'sd:', 4.403376639451091)\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.3507  \u001b[0m | \u001b[0m 1.118   \u001b[0m | \u001b[0m 1.397   \u001b[0m | \u001b[0m 0.06702 \u001b[0m | \u001b[0m 0.6078  \u001b[0m |\n",
      "[0.021230158730158728, 0.015277777777777779, 0.036111111111111115, 0.012896825396825398, 0.021230158730158728, 0.02361111111111111]\n",
      "[0.08266666666666665, 0.044000000000000004, 0.08800000000000001, 0.12, 0.05714285714285714, 0.05904761904761905]\n",
      "('Test acc:', 2.172619047619047, 'sd:', 0.7421934569154005)\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.07514 \u001b[0m | \u001b[0m 1.676   \u001b[0m | \u001b[0m 1.959   \u001b[0m | \u001b[0m 0.7672  \u001b[0m | \u001b[0m 0.3891  \u001b[0m |\n",
      "[0.08875661375661376, 0.07361111111111111, 0.08650793650793649, 0.022222222222222223]\n",
      "[0.144, 0.11504761904761905, 0.1920952380952381, 0.14600000000000002]\n",
      "('Test acc:', 6.777447089947089, 'sd:', 2.6927062507332438)\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.1493  \u001b[0m | \u001b[0m 1.879   \u001b[0m | \u001b[0m 0.6807  \u001b[0m | \u001b[0m 0.7717  \u001b[0m | \u001b[0m 0.2572  \u001b[0m |\n",
      "[0.13492664742664742, 0.15067421004921006, 0.15372185684685685, 0.15021205646205646]\n",
      "[0.3208590298590298, 0.37648906648906655, 0.3164775224775225, 0.3432872682872683]\n",
      "('Test acc:', 14.73836926961927, 'sd:', 0.7317401309392977)\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.3393  \u001b[0m | \u001b[0m 0.7822  \u001b[0m | \u001b[0m 1.35    \u001b[0m | \u001b[0m 0.8275  \u001b[0m | \u001b[0m 0.2914  \u001b[0m |\n",
      "[0.23766233766233769, 0.1635777417027417, 0.14207551707551705, 0.19709977522477526, 0.12287434787434787, 0.19543500481000478, 0.0956457893957894]\n",
      "[0.34885858585858587, 0.30842135642135643, 0.30555133755133757, 0.2975137085137085, 0.24698845598845598, 0.20688600288600287, 0.32052669552669555]\n",
      "('Test acc:', 16.491007339221625, 'sd:', 4.527467855186331)\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.2907  \u001b[0m | \u001b[0m 0.9718  \u001b[0m | \u001b[0m 1.077   \u001b[0m | \u001b[0m 0.5076  \u001b[0m | \u001b[0m 0.8548  \u001b[0m |\n",
      "[0.3055555555555555, 0.3506944444444444, 0.31597222222222215, 0.2222222222222222, 0.17361111111111108, 0.29166666666666663]\n",
      "[0.43333333333333335, 0.53, 0.5133333333333333, 0.42, 0.52, 0.4799999999999999]\n",
      "('Test acc:', 27.66203703703703, 'sd:', 6.009608789697763)\n",
      "| \u001b[95m 9       \u001b[0m | \u001b[95m 0.4828  \u001b[0m | \u001b[95m 0.2717  \u001b[0m | \u001b[95m 1.599   \u001b[0m | \u001b[95m 0.001   \u001b[0m | \u001b[95m 0.09368 \u001b[0m |\n",
      "[0.27777777777777773, 0.3229166666666667, 0.25, 0.24652777777777776, 0.3159722222222222]\n",
      "[0.4599999999999999, 0.37666666666666665, 0.5333333333333333, 0.38733333333333336, 0.42666666666666664]\n",
      "('Test acc:', 28.263888888888882, 'sd:', 3.201983856499586)\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.4368  \u001b[0m | \u001b[0m 0.001   \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.001   \u001b[0m | \u001b[0m 0.8579  \u001b[0m |\n",
      "[0.3333333333333333, 0.2743055555555555, 0.28472222222222215]\n",
      "[0.43333333333333335, 0.5333333333333333, 0.4766666666666666]\n",
      "('Test acc:', 29.745370370370367, 'sd:', 2.5724665244088962)\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.4811  \u001b[0m | \u001b[0m 0.001   \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.7579  \u001b[0m | \u001b[0m 0.001   \u001b[0m |\n",
      "[0.25347222222222215, 0.22152777777777777, 0.2673611111111111, 0.2673611111111111]\n",
      "[0.47, 0.4333333333333333, 0.3170476190476191, 0.4133333333333333]\n",
      "('Test acc:', 25.243055555555554, 'sd:', 1.87210424537619)\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.4084  \u001b[0m | \u001b[0m 0.001   \u001b[0m | \u001b[0m 0.6861  \u001b[0m | \u001b[0m 0.001   \u001b[0m | \u001b[0m 0.001   \u001b[0m |\n",
      "[0.033960599585599584, 0.05025703463203463, 0.04662698412698413, 0.04633214008214009, 0.059830447330447324]\n",
      "[0.1285238095238095, 0.09887878787878787, 0.0741017871017871, 0.07066810966810967, 0.0800952380952381]\n",
      "('Test acc:', 4.7401441151441155, 'sd:', 0.8307033395644623)\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.09045 \u001b[0m | \u001b[0m 0.001   \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 0.8434  \u001b[0m |\n",
      "[0.23958333333333334, 0.19791666666666666, 0.30902777777777773, 0.3125, 0.34375, 0.3194444444444444, 0.3125, 0.3194444444444444, 0.24652777777777776]\n",
      "[0.5133333333333333, 0.5793333333333333, 0.42, 0.4733333333333334, 0.4533333333333333, 0.3, 0.5099999999999999, 0.55, 0.46333333333333326]\n",
      "('Test acc:', 28.896604938271604, 'sd:', 4.582449929413964)\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.4736  \u001b[0m | \u001b[0m 0.001   \u001b[0m | \u001b[0m 1.361   \u001b[0m | \u001b[0m 0.5375  \u001b[0m | \u001b[0m 0.001   \u001b[0m |\n",
      "[0.17361111111111108, 0.2222222222222222, 0.17361111111111108, 0.15277777777777776, 0.16666666666666666, 0.23263888888888887, 0.2569444444444444]\n",
      "[0.3733333333333333, 0.42, 0.42, 0.4733333333333333, 0.37, 0.33333333333333326, 0.4066666666666666]\n",
      "('Test acc:', 19.692460317460316, 'sd:', 3.678000974586632)\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.3995  \u001b[0m | \u001b[0m 0.9334  \u001b[0m | \u001b[0m 0.001   \u001b[0m | \u001b[0m 0.001   \u001b[0m | \u001b[0m 0.001   \u001b[0m |\n",
      "[0.21527777777777776, 0.1840277777777778, 0.3020833333333333, 0.15972222222222224, 0.2604166666666667, 0.16666666666666666]\n",
      "[0.4066666666666666, 0.46, 0.5199999999999999, 0.49999999999999994, 0.41999999999999993, 0.43333333333333335]\n",
      "('Test acc:', 21.46990740740741, 'sd:', 5.16345116404962)\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m 0.4567  \u001b[0m | \u001b[0m 0.7382  \u001b[0m | \u001b[0m 0.9366  \u001b[0m | \u001b[0m 0.001   \u001b[0m | \u001b[0m 0.001   \u001b[0m |\n",
      "[0.22569444444444442, 0.19791666666666666, 0.2916666666666667, 0.2951388888888889]\n",
      "[0.5333333333333333, 0.5166666666666666, 0.4433333333333333, 0.4799999999999999]\n",
      "('Test acc:', 25.260416666666664, 'sd:', 4.198195239355469)\n",
      "| \u001b[95m 17      \u001b[0m | \u001b[95m 0.4933  \u001b[0m | \u001b[95m 0.001   \u001b[0m | \u001b[95m 2.0     \u001b[0m | \u001b[95m 0.001   \u001b[0m | \u001b[95m 0.001   \u001b[0m |\n",
      "[0.041666666666666664, 0.041666666666666664, 0.020833333333333332, 0.16666666666666666, 0.041666666666666664, 0.041666666666666664, 0.10416666666666667, 0.0625, 0.125]\n",
      "[0.32, 0.16, 0.18, 0.04, 0.14, 0.12, 0.3133333333333333, 0.14, 0.18]\n",
      "('Test acc:', 7.175925925925925, 'sd:', 4.618041051680556)\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m 0.177   \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 0.001   \u001b[0m | \u001b[0m 0.001   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "=========================================================================\n"
     ]
    }
   ],
   "source": [
    "# evaluate fine-tuned model\n",
    "lgroups_ft = [[4, 5]]\n",
    "bounds_gptxlf = {\n",
    "  \"temperature\": [0.001, 1.0],  # a temp of 0 selects the top completion every time due to the infinitely larger logit\n",
    "  \"top_p\": [0.001, 1.0],        # same with this but more obvious\n",
    "#   \"top_k\": [1, 100],        # fix this to be a log of the input value so that lower values are sampled with high resolution?\n",
    "  \"presence_penalty\": [0.001, 2.0],   # both presence and frequency penalty have optimal values\n",
    "  \"frequency_penalty\": [0.001, 2.0],  # this can also go down to -2.0, probably not useful for our case...\n",
    "#   \"best_of\": [0.51, 5.49], # to do\n",
    "}\n",
    "optimizers_gpt2xlf, results_gpt2xlf = [], []\n",
    "for lgroup in lgroups_ft:\n",
    "    min_l, max_l = min(lgroup) - 1, max(lgroup)\n",
    "    def fun(temperature, top_p, presence_penalty, frequency_penalty):\n",
    "        global min_l, max_l\n",
    "        ps = locals()\n",
    "        ps[\"best_of\"] = 5.0\n",
    "        return test_sp_conv(ps, min_l=min_l, max_l=max_l, tol=0.005, phase=\"train\", uniform=True, mdl=mdl, max_tokens=8)[0]\n",
    "    optimizers_gpt2xlf.append(BayesianOptimization(f=fun, pbounds=bounds_gptxlf, verbose=1000))\n",
    "    optimizers_gpt2xlf[-1].maximize(init_points=8, n_iter=10)\n",
    "    results_gpt2xlf.append(optimizers_gpt2xlf[-1].max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |   iter    |  target   | freque... | presen... | temper... |   top_p   |\n",
    "# -------------------------------------------------------------------------\n",
    "# [0.14608262108262107, 0.1544311013061013, 0.1774588258963259]\n",
    "# [0.5712862692862692, 0.5167355977355977, 0.5436751026751027]\n",
    "# ('Test acc:', 15.932418276168276, 'sd:', 1.326833930141234)\n",
    "# |  1        |  0.5439   |  0.7545   |  0.8837   |  1.164    |  0.1678   |\n",
    "# [0.006944444444444444, 0.008333333333333333, 0.025000000000000005]\n",
    "# [0.008, 0.0, 0.014666666666666668]\n",
    "# ('Test acc:', 1.3425925925925926, 'sd:', 0.8203724604939516)\n",
    "# |  2        |  0.007556 |  1.563    |  1.115    |  1.345    |  0.4973   |\n",
    "# [0.21354131979131977, 0.173926362988863, 0.13297558922558925, 0.25522775835275835]\n",
    "# [0.5312100122100122, 0.6170219780219779, 0.5921578421578422, 0.5759225219225219]\n",
    "# ('Test acc:', 19.39177575896326, 'sd:', 4.543568017003007)\n",
    "# |  3        |  0.5791   |  1.185    |  1.479    |  0.3418   |  0.9886   |\n",
    "# [0.35648148148148145, 0.3520833333333333, 0.3180147058823529, 0.332175925925926, 0.2954656862745098]\n",
    "# [0.6432820512820513, 0.7113333333333333, 0.7075555555555556, 0.8067179487179486, 0.738952380952381]\n",
    "# ('Test acc:', 33.08442265795207, 'sd:', 2.247834350297864)\n",
    "# |  4        |  0.7216   |  0.5497   |  0.4131   |  0.04958  |  0.5878   |\n",
    "# [0.24212986087986085, 0.20677008177008174, 0.2121680402930403, 0.24574632543382546]\n",
    "# [0.5814134754134754, 0.5753290043290044, 0.6005385725385726, 0.5997070707070707]\n",
    "# ('Test acc:', 22.67035770942021, 'sd:', 1.73869387881864)\n",
    "# |  5        |  0.5892   |  0.989    |  1.323    |  0.6668   |  0.5829   |\n",
    "# [0.006944444444444444, 0.0, 0.0]\n",
    "# [0.0, 0.0, 0.0]\n",
    "# ('Test acc:', 0.23148148148148145, 'sd:', 0.32736425054932755)\n",
    "# |  6        |  0.0      |  1.268    |  1.645    |  1.636    |  0.3853   |\n",
    "# [0.029563492063492066, 0.05555555555555556, 0.03511904761904762, 0.024537037037037038, 0.01636904761904762]\n",
    "# [0.07957142857142857, 0.13923809523809524, 0.07619047619047618, 0.15666666666666665, 0.1219047619047619]\n",
    "# ('Test acc:', 3.222883597883598, 'sd:', 1.319310339155671)\n",
    "# |  7        |  0.1147   |  1.617    |  0.524    |  0.9253   |  0.9548   |\n",
    "# [0.0, 0.0, 0.0]\n",
    "# [0.0, 0.008, 0.008]\n",
    "# ('Test acc:', 0.0, 'sd:', 0.0)\n",
    "# |  8        |  0.005333 |  1.573    |  0.1914   |  1.962    |  0.2006   |\n",
    "# [0.29166666666666663, 0.3055555555555555, 0.3819444444444444, 0.24305555555555555, 0.3506944444444444]\n",
    "# [0.6366666666666666, 0.7466666666666666, 0.7399999999999999, 0.7999999999999998, 0.73]\n",
    "# ('Test acc:', 31.458333333333332, 'sd:', 4.809247136994663)\n",
    "# |  9        |  0.7307   |  0.001    |  1.477    |  0.001    |  0.001    |\n",
    "# [0.2643718553548275, 0.2827585030710031, 0.27731273356273356, 0.2543742511573394]\n",
    "# [0.5516450836744954, 0.5830235059058588, 0.544432178932179, 0.5589042819925172]\n",
    "# ('Test acc:', 26.97043357864759, 'sd:', 1.1087671560167434)\n",
    "# |  10       |  0.5595   |  0.001    |  1.146    |  0.5046   |  1.0      |\n",
    "# [0.40625, 0.2916666666666667, 0.34722222222222215]\n",
    "# [0.55, 0.6666666666666665, 0.6066666666666667]\n",
    "# ('Test acc:', 34.83796296296296, 'sd:', 4.678560863751791)\n",
    "# |  11       |  0.6078   |  0.001    |  0.001    |  0.4932   |  0.001    |\n",
    "# [0.32638888888888884, 0.2222222222222222, 0.2916666666666667, 0.2534722222222222, 0.34722222222222215, 0.20138888888888887, 0.21527777777777776, 0.23611111111111108, 0.2847222222222222]\n",
    "# [0.74, 0.6666666666666665, 0.6866666666666665, 0.6133333333333333, 0.7266666666666666, 0.7866666666666666, 0.78, 0.6466666666666666, 0.6933333333333332]\n",
    "# ('Test acc:', 26.427469135802472, 'sd:', 4.8236109901505495)\n",
    "# |  12       |  0.7044   |  1.045    |  1.107    |  0.001    |  0.001    |\n",
    "# [0.2708333333333333, 0.2569444444444444, 0.25, 0.18055555555555555]\n",
    "# [0.62, 0.7666666666666666, 0.6266666666666666, 0.6666666666666665]\n",
    "# ('Test acc:', 23.958333333333332, 'sd:', 3.4895401462225317)\n",
    "# |  13       |  0.67     |  0.7444   |  2.0      |  0.001    |  0.001    |\n",
    "# [0.3333333333333333, 0.3263888888888889, 0.37152777777777773, 0.3020833333333333]\n",
    "# [0.6166666666666667, 0.72, 0.7941025641025641, 0.7266666666666666]\n",
    "# ('Test acc:', 33.33333333333333, 'sd:', 2.4917882108346023)\n",
    "# |  14       |  0.7144   |  0.2284   |  2.0      |  0.001    |  1.0      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate un-fine-tuned model\n",
    "lgroups_ft = [[4, 5]]\n",
    "bounds_gptxl = {\n",
    "  \"temperature\": [0.01, 2.0],  # a temp of 0 selects the top completion every time due to the infinitely larger logit\n",
    "  \"top_p\": [0.01, 1.0],        # same with this but more obvious\n",
    "#   \"top_k\": [1, 100],        # fix this to be a log of the input value so that lower values are sampled with high resolution?\n",
    "  \"presence_penalty\": [0.01, 2.0],   # both presence and frequency penalty have optimal values\n",
    "  \"frequency_penalty\": [0.01, 2.0],  # this can also go down to -2.0, probably not useful for our case...\n",
    "#   \"best_of\": [0.51, 5.49], # to do\n",
    "}\n",
    "optimizers_gpt2xl, results_gpt2xl = [], []\n",
    "for lgroup in lgroups_ft:\n",
    "    min_l, max_l = min(lgroup) - 1, max(lgroup)\n",
    "    def fun(temperature, top_p, presence_penalty, frequency_penalty):\n",
    "        global min_l, max_l\n",
    "        ps = locals()\n",
    "        ps[\"best_of\"] = 5.0\n",
    "        return test_sp_conv(ps, min_l=min_l, max_l=max_l, tol=0.005, phase=\"train\", uniform=True, mdl=mdl, max_tokens=8)[0]\n",
    "    optimizers_gpt2xl.append(BayesianOptimization(f=fun, pbounds=bounds_gptxl, verbose=1000))\n",
    "    optimizers_gpt2xl[-1].maximize(init_points=8, n_iter=10)\n",
    "    results_gpt2xl.append(optimizers_gpt2xl[-1].max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test accuracy for the top performing (training accuracy ) sampling parameters for gpt2-\n",
    "# \n",
    "# |   iter    |  target   | freque... | presen... | temper... |   top_p   |\n",
    "# -------------------------------------------------------------------------\n",
    "# [0.049999999999999996, 0.025000000000000005, 0.061111111111111116]\n",
    "# [0.008, 0.03333333333333333, 0.029333333333333336]\n",
    "# ('Test acc:', 4.537037037037037, 'sd:', 1.5101394842870455)\n",
    "# |  1        |  0.02356  |  0.7232   |  0.9207   |  1.792    |  0.1562   |\n",
    "# [0.2111111111111111, 0.13333333333333333, 0.23576388888888888, 0.3333333333333333]\n",
    "# [0.1965714285714286, 0.11199999999999999, 0.111, 0.12]\n",
    "# ('Test acc:', 22.838541666666668, 'sd:', 7.141744752411932)\n",
    "# |  2        |  0.1349   |  0.6561   |  0.6181   |  0.6707   |  0.1418   |\n",
    "# [0.0, 0.016666666666666666, 0.0]\n",
    "# [0.0, 0.0, 0.0]\n",
    "# ('Test acc:', 0.5555555555555556, 'sd:', 0.7856742013183862)\n",
    "# |  3        |  0.0      |  1.035    |  0.6253   |  1.287    |  0.9439   |\n",
    "# [0.5208333333333334, 0.375, 0.3854166666666667, 0.4930555555555555, 0.34027777777777773, 0.40625, 0.3194444444444444]\n",
    "# [0.32, 0.38, 0.21333333333333332, 0.28, 0.16666666666666663, 0.2, 0.2733333333333333]\n",
    "# ('Test acc:', 40.57539682539682, 'sd:', 6.965317270864225)\n",
    "# |  4        |  0.2619   |  0.4573   |  0.1734   |  0.6163   |  0.02537  |\n",
    "# [0.025000000000000005, 0.0, 0.03333333333333333]\n",
    "# [0.008, 0.024000000000000004, 0.008]\n",
    "# ('Test acc:', 1.9444444444444444, 'sd:', 1.4163943093313291)\n",
    "# |  5        |  0.01333  |  1.27     |  1.156    |  1.195    |  0.7833   |\n",
    "# [0.008333333333333333, 0.0, 0.0]\n",
    "# [0.0, 0.0, 0.0]\n",
    "# ('Test acc:', 0.2777777777777778, 'sd:', 0.3928371006591931)\n",
    "# |  6        |  0.0      |  1.297    |  1.846    |  1.938    |  0.7382   |\n",
    "# [0.013888888888888888, 0.0, 0.0]\n",
    "# [0.014666666666666668, 0.008, 0.0]\n",
    "# ('Test acc:', 0.4629629629629629, 'sd:', 0.6547285010986551)\n",
    "# |  7        |  0.007556 |  0.2306   |  1.936    |  1.592    |  0.7024   |\n",
    "# [0.2511354386354386, 0.3434765466015466, 0.30643372830872834, 0.3152858715358715]\n",
    "# [0.32786868686868686, 0.3295645530939648, 0.3073928293928294, 0.3104887334887335]\n",
    "# ('Test acc:', 30.408289627039625, 'sd:', 3.349002098569766)\n",
    "# |  8        |  0.3188   |  0.1998   |  0.08897  |  0.9428   |  0.6384   |\n",
    "# [0.4106481481481481, 0.26304563492063493, 0.31493055555555555, 0.3008207070707071, 0.37310600279350276, 0.38941300733580136]\n",
    "# [0.3358236208236208, 0.1989130869130869, 0.21833333333333332, 0.34352380952380945, 0.3320714285714286, 0.3045044955044955]\n",
    "# ('Test acc:', 34.19940093040583, 'sd:', 5.258394155542585)\n",
    "# |  9        |  0.2889   |  0.01     |  0.01     |  0.1864   |  0.669    |\n",
    "# [0.4861111111111111, 0.3020833333333333, 0.2604166666666667, 0.2916666666666667, 0.3055555555555555, 0.34722222222222215, 0.3541666666666667, 0.2708333333333333]\n",
    "# [0.3833333333333333, 0.20666666666666664, 0.3161904761904762, 0.38, 0.3466666666666666, 0.4137777777777778, 0.29333333333333333, 0.35]\n",
    "# ('Test acc:', 32.72569444444444, 'sd:', 6.743512364375678)\n",
    "# |  10       |  0.3362   |  0.01     |  0.01     |  1.212    |  0.01     |\n",
    "# [0.0, 0.0, 0.0]\n",
    "# [0.0, 0.0, 0.0]\n",
    "# ('Test acc:', 0.0, 'sd:', 0.0)\n",
    "# |  11       |  0.0      |  0.01     |  0.01     |  2.0      |  1.0      |\n",
    "# [0.5181517556517556, 0.2377946127946128, 0.4213624338624338, 0.3735119047619048]\n",
    "# [0.27763369963369966, 0.32315873015873015, 0.23523076923076924, 0.2697142857142857]\n",
    "# ('Test acc:', 38.77051767676768, 'sd:', 10.102443647288354)\n",
    "# |  12       |  0.2764   |  0.01     |  0.01     |  0.6837   |  0.1847   |\n",
    "# [0.19999999999999998, 0.23750000000000002, 0.2722222222222222, 0.375, 0.20833333333333334, 0.0625, 0.16666666666666666, 0.325]\n",
    "# [0.22, 0.08, 0.10400000000000001, 0.264, 0.12, 0.096, 0.2, 0.16]\n",
    "# ('Test acc:', 23.09027777777778, 'sd:', 9.035987186191242)\n",
    "# |  13       |  0.1555   |  0.8694   |  0.01     |  0.01     |  1.0      |\n",
    "# [0.5590277777777778, 0.3819444444444444, 0.31527777777777777, 0.3541666666666667, 0.37152777777777773, 0.375, 0.518287037037037, 0.34027777777777773, 0.4236111111111111]\n",
    "# [0.2338095238095238, 0.19422222222222224, 0.26, 0.25666666666666665, 0.3053333333333333, 0.31, 0.305, 0.36, 0.256]\n",
    "# ('Test acc:', 40.434670781893004, 'sd:', 7.765741054904079)\n",
    "# |  14       |  0.2757   |  0.01     |  0.8582   |  0.01     |  1.0      |\n",
    "# [0.35763888888888884, 0.3756944444444444, 0.33796296296296297, 0.38055555555555554, 0.2824074074074074, 0.3171296296296296, 0.43402777777777773]\n",
    "# [0.3680952380952381, 0.23466666666666666, 0.2222222222222222, 0.22, 0.32, 0.14, 0.22666666666666666]\n",
    "# ('Test acc:', 35.50595238095238, 'sd:', 4.524186608251292)\n",
    "# |  15       |  0.2474   |  0.01     |  2.0      |  0.01     |  1.0      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test accuracy for the top performing (training accuracy 0.2026) sampling parameters for gpt2-small (after the full 18 runs)\n",
    "# np.mean([0.3353320494864612,0.3277777777777778,0.21805555555555556,0.3402514152514152,0.28348214285714285,0.13194444444444445])# = 0.27280723089546616"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With about half an hour of fine-tuning using 7 training samples on an NVidia 980 Ti (showing improvements):\n",
    "# Initial input words on first line, those eventually found by repeatedly querying model + recursively adding words on 2nd & 3rd\n",
    "input_sentence = (\"pace, silence, diversion, attention, plot twist, cliffhanger, surprise, fate, fourth wall, monologue\" + \\\n",
    "\"reinterpretation, harmony, character progression, reading circle\")\n",
    "# \"monolith, elevator effect, time loop, survival, desert resort, town watchman\")\n",
    "# \"monolith, allegro, soundtrack, chord, classical, opera\")\n",
    "input_sentence = input_sentence.split(', ')\n",
    "np.random.shuffle(input_sentence)\n",
    "input_sentence = \"A list of types of element of drama and writing: \"+', '.join(input_sentence[:10])+', '\n",
    "olen = len(input_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_sentence = append_next_token(input_sentence, top_k=25, top_p=0.6, temperature=15.0, olen=olen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # With about half an hour of fine-tuning using 7 training samples on an NVidia 980 Ti (showing improvements):\n",
    "# # Initial input words on first line, those eventually found by repeatedly querying model + recursively adding words on 2nd & 3rd\n",
    "# input_sentence = (\"pace, silence, diversion, attention, plot twist, cliffhanger, surprise, fate, fourth wall, monologue\" + \\\n",
    "# \"reinterpretation, harmony, character progression, reading circle\")\n",
    "# # \"monolith, elevator effect, time loop, survival, desert resort, town watchman\")\n",
    "# # \"monolith, allegro, soundtrack, chord, classical, opera\")\n",
    "# input_sentence = input_sentence.split(', ')\n",
    "# np.random.shuffle(input_sentence)\n",
    "# input_sentence = \"A list of types of element of drama and writing: \"+', '.join(input_sentence[:10])+', '\n",
    "# olen = len(input_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# input_sentence = append_next_token(input_sentence, top_k=25, top_p=0.6, temperature=15.0, olen=olen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # With about half an hour of fine-tuning using 7 training samples on an NVidia 980 Ti (showing improvements):\n",
    "# # Initial input words on first line, those eventually found by repeatedly querying model + recursively adding words on 2nd & 3rd\n",
    "# input_sentence = (\"coffee, water, tea, coke, lemonade, milkshake, \" + \\\n",
    "# \"watermelon juice, cherry juice, blackcurrant mixture, kava, orangeade, lemon juice, cherryade, cranberry juice\")\n",
    "# input_sentence = input_sentence.split(', ')\n",
    "# np.random.shuffle(input_sentence)\n",
    "# input_sentence = \"A list of types of element of drama and writing: \"+', '.join(input_sentence[:10])+', '\n",
    "# olen = len(input_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# input_sentence = append_next_token(input_sentence, top_k=10, top_p=0.8, temperature=5.0, olen=olen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # With about half an hour of fine-tuning using 7 training samples on an NVidia 980 Ti (showing improvements):\n",
    "# # Initial input words on first line, those eventually found by repeatedly querying model + recursively adding words on 2nd & 3rd\n",
    "# input_sentence = (\"pace, silence, diversion, attention, plot twist, cliffhanger, surprise, fate, fourth wall, monologue\" + \\\n",
    "# \"\")\n",
    "# input_sentence = input_sentence.split(', ')\n",
    "# np.random.shuffle(input_sentence)\n",
    "# input_sentence = \"A list of types of element of drama and writing: \"+', '.join(input_sentence[:10])+', ' # Randomly generate shuffled drinks sublist\n",
    "# olen = len(input_sentence)\n",
    "# input_sentence = append_next_token(input_sentence, top_k=5, top_p=0.9, temperature=5.0, olen=olen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With about half an hour of fine-tuning using 7 training samples on an NVidia 980 Ti (showing improvements):\n",
    "# Initial input words on first line, those eventually found by repeatedly querying model + recursively adding words on 2nd & 3rd\n",
    "# input_sentence = (\"coffee, water, tea, coke, lemonade, milkshake, \" + \\\n",
    "# \"milk, soda\")\n",
    "# input_sentence = input_sentence.split(', ')\n",
    "# np.random.shuffle(input_sentence)\n",
    "# input_sentence = \"A list of types of drink: \"+', '.join(input_sentence[:10])+', ' # Randomly generate shuffled drinks sublist\n",
    "# olen = len(input_sentence)\n",
    "# input_sentence = append_next_token(input_sentence, top_k=25, top_p=0.7, temperature=2.0, olen=olen)\n",
    "# input_sentence = append_next_token(input_sentence, top_k=5, top_p=0.9, temperature=5.0, olen=olen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # With about half an hour of fine-tuning using 7 training samples on an NVidia 980 Ti (showing improvements):\n",
    "# # Initial input words on first line, those eventually found by repeatedly querying model + recursively adding words on 2nd & 3rd\n",
    "# input_sentence = (\"coffee, water, tea, coke, lemonade, milkshake\" + \\\n",
    "# \", liquor, wine, juice, beer, milk, soft drink, whiskey, vodka, spirits, soda, ice water, ice cold beer, cider, yoghurt, soda pop, rum, chocolate milk, hot cocoa, alcohol\")\n",
    "# input_sentence = input_sentence.split(', ')\n",
    "# np.random.shuffle(input_sentence)\n",
    "# input_sentence = \"A list of types of drink: \"+', '.join(input_sentence[:10])+', ' # Randomly generate shuffled drinks sublist\n",
    "# olen = len(input_sentence)\n",
    "# input_sentence = append_next_token(input_sentence, top_k=25, top_p=0.7, temperature=2.0, olen=olen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # With about half an hour of fine-tuning using 7 training samples on an NVidia 980 Ti (showing improvements):\n",
    "# # Initial input words on first line, those eventually found by repeatedly querying model + recursively adding words on 2nd & 3rd\n",
    "# input_sentence = (\"coffee, water, tea, coke, lemonade, milkshake\" + \\\n",
    "# \", liquor, wine, juice, beer, milk, soft drink, whiskey, vodka, spirits, soda, ice water, ice cold beer, cider\" + \\\n",
    "# \", yoghurt, soda pop, rum, chocolate milk, hot cocoa, alcohol\")\n",
    "# input_sentence = input_sentence.split(', ')\n",
    "# np.random.shuffle(input_sentence)\n",
    "# input_sentence = \"A list of types of drink: \"+', '.join(input_sentence[:10])+', ' # Randomly generate shuffled drinks sublist\n",
    "# olen = len(input_sentence)\n",
    "# input_sentence = append_next_token(input_sentence, top_k=25, top_p=0.7, temperature=2.0, olen=olen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changelog 16/04/2021 5pm: Using log_period=1 (max_len=96) reliably finds improvement, need more data and larger gpt2 (medium+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With about half an hour of fine-tuning using 7 training samples on an NVidia 980 Ti (showing improvements):\n",
    "# Initial input words on first line, those eventually found by repeatedly querying model + recursively adding words on 2nd & 3rd\n",
    "# input_sentence = (\"coffee, water, tea, coke, lemonade, milkshake,\" + \\\n",
    "# \" milk, vodka, beer, ice water, soda, lassi, juice, alcohol, whiskey\")\n",
    "# input_sentence = input_sentence.split(', ')\n",
    "# np.random.shuffle(input_sentence)\n",
    "# input_sentence = \"A list of types of drink: \"+', '.join(input_sentence[:10])+', ' # Randomly generate shuffled drinks sublist\n",
    "# olen = len(input_sentence)\n",
    "# input_sentence = append_next_token(input_sentence, top_k=25, top_p=0.9, temperature=2.0, olen=olen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With about half an hour of fine-tuning using 7 training samples on an NVidia 980 Ti (showing improvements):\n",
    "# Initial input words on first line, those eventually found by repeatedly querying model + recursively adding words on 2nd & 3rd\n",
    "# input_sentence = (\"coffee, water, tea, coke, lemonade, milkshake,\" + \\\n",
    "# \" wine, soda, alcohol, beer, liquor, apple cider, whiskey, milk, bourbon, vodka, cider, lemon juice\")\n",
    "# input_sentence = input_sentence.split(', ')\n",
    "# np.random.shuffle(input_sentence)\n",
    "# input_sentence = \"A list of types of drink: \"+', '.join(input_sentence[:6])+', ' # Randomly generate shuffled drinks sublist\n",
    "# olen = len(input_sentence)\n",
    "# input_sentence = append_next_token(input_sentence, top_k=30, top_p=0.7, temperature=3.0, olen=olen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With about half an hour of fine-tuning using 7 training samples on an NVidia 980 Ti (showing improvements):\n",
    "# Initial input words on first line, those eventually found by repeatedly querying model + recursively adding words on 2nd & 3rd\n",
    "# input_sentence = (\"coffee, water, tea, coke, lemonade, milkshake,\" + \\\n",
    "# \" beer, milk, juice, wine, spirits, alcohol, soda, whiskey, brandy, apple juice, liquor, ice tea, watermelon juice, vodka,\" + \\\n",
    "# \" lemon tea, apple cider, ale, lager, fruit tea, lime cider, cocktail, mocha, red wine, apple soda\")\n",
    "# input_sentence = input_sentence.split(', ')\n",
    "# np.random.shuffle(input_sentence)\n",
    "# input_sentence = \"A list of types of drink: \"+', '.join(input_sentence[:10])+', ' # Randomly generate shuffled drinks sublist\n",
    "# olen = len(input_sentence)\n",
    "# input_sentence = append_next_token(input_sentence, top_k=30, top_p=0.7, temperature=2.0, olen=olen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changelog 16/04/2021 5am: lr=1e-5, max_len=80, log_period_batches=5 increased accuracy by 8%. Need to add a bunch more data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With about half an hour of fine-tuning using 5 training samples on an NVidia 980 Ti (showing improvements):\n",
    "# Initial input words on first line, those eventually found by repeatedly querying model + recursively adding words on 2nd & 3rd\n",
    "# input_sentence = (\"coffee, water, tea, coke, lemonade, milkshake, \" + \\\n",
    "# \"soda, milk, juice, wine, vodka, gin, lime juice, beer, hot chocolate, cider, whiskey, fruit juice, cocktail, liquor, \" + \\\n",
    "# \"spirits, watermelon juice, martini, rum, chocolate milk, orangeade\")\n",
    "# input_sentence = input_sentence.split(', ')\n",
    "# np.random.shuffle(input_sentence)\n",
    "# input_sentence = \"A list of types of drink: \"+', '.join(input_sentence[:10])+', ' # Randomly generate shuffled drinks sublist\n",
    "# olen = len(input_sentence)\n",
    "# input_sentence = append_next_token(input_sentence, top_k=25, top_p=0.9, temperature=1.89, olen=olen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without fine tuning (regular GPT-2):\n",
    "# Initial input words on first line, those eventually found by repeatedly querying model on 2nd\n",
    "# input_sentence = (\"coffee, water, tea, coke, lemonade, milkshake, \" + \\\n",
    "# \"milk, juice, fruit juice, soda, wine, beer, hot chocolate, chocolate milk, alcohol, cider, ice tea, liquor, spirit\")\n",
    "# input_sentence.split(', ')\n",
    "# np.random.shuffle(input_sentence)\n",
    "# input_sentence = \"A list of types of drink: \"+', '.join(input_sentence[:10])+', ' # Randomly generate shuffled drinks sublist\n",
    "# olen = len(input_sentence)\n",
    "# input_sentence = append_next_token(input_sentence, top_k=25, top_p=0.75, temperature=1.89, olen=olen) # k = 25 also used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training for too long (in this case ~6:30 hours) overfits\n",
    "# input_sentence = (\"coffee, water, tea, coke, lemonade, milkshake, \" + \\\n",
    "# \"beer, juice, soda, liquor, wine, tequila, spirits, alcohol, cocktail, martini, whiskey, rum, vodka\")\n",
    "# input_sentence = input_sentence.split(', ')\n",
    "# np.random.shuffle(input_sentence)\n",
    "# input_sentence = \"A list of types of drink: \"+', '.join(input_sentence[:10])+', ' # Randomly generate shuffled drinks sublist\n",
    "# olen = len(input_sentence)\n",
    "# input_sentence = append_next_token(input_sentence, top_k=25, top_p=0.9, temperature=1.89, olen=olen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changelog 15/04/2021 11am: Found learning rate 1e-7, max_listlen 15, min_nw 0.7, max_nw 0.9, lidstone_e 0.01 ACTUALLY WORKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No fine tuning (testing gpt2) (old append function):\n",
    "# input_sentence = \"A list of types of drink: coffee, water, tea, coke, lemonade, milkshake\"\n",
    "# input_sentence = append_next_token(input_sentence, top_k=25, top_p=0.75, temperature=1.89)\n",
    "# A list of types of drink: juice, tea, cider, lemonade, milk, beer, hot chocolate,➡ ice cold milk. Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working (reproducible) examples using various non-fine-tuned models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT3 (via AI Dungeon) (Randomness = 2.0, model = Dragon):\n",
    "# sentence = \"A list of ML algorithms: inverse reinforcement learning, ELMo, decision tree, LDA, \"\n",
    "# expected_completion = \"MLP, MLL, MMM. You can't believe you're actually using these things!\"\n",
    "\n",
    "# sentence = \"A list of animals seen in the wild: wolffish, woodlouse, sheep, zebra, yak, \"\n",
    "# expected_completion = \"goat, fox, dog, rat. You're guessing that a lot of other animals have been seen as well; maybe even all the animals on your list except for wolf and rat?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT2 (via Write with Transformer) (Top-p = 0.67, temperature = 1.89, max time = 1.9):\n",
    "# sentence = \"A list of round fruits: peach, apricot, lime, plum, blackberry, cantaloupe, nectarine, pitaya, persimmon, \"\n",
    "# expected_completion = \"mango, papaya and raspberry, as also many\"\n",
    "\n",
    "# sentence = \"A list of chemical elements: hydrogen, carbon, oxygen, nitrogen, gold, \"\n",
    "# expected_completion = \"silver, aluminum, potassium and phosphorus; atomic number.\"\n",
    "\n",
    "# sentence = \"A list of microbes found on earth: bacteria, virus, prokaryote, amoeba, \"\n",
    "# expected_completion = \"archaea, algae, nematode, euk\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This result shows why we need not redistribute the mass when evaluating gpt3 accuracy\n",
    "# response = openai.Completion.create(**{**default_params,\n",
    "#   \"prompt\": \"A list of cyclic phenomena: day and night, induction coil, self-oscillation, tornado, runaway greenhouse effect,\",\n",
    "#   \"temperature\": 1.5,\n",
    "#   \"top_p\": 1.0,\n",
    "#   \"n\": 5,\n",
    "#   \"best_of\": 20,\n",
    "#   \"max_tokens\": 7,\n",
    "#   \"stop\": [\",\", \"\\n\"],\n",
    "# })\n",
    "# for choice in response[\"choices\"]:\n",
    "#     d = {}\n",
    "#     tokens = choice[\"logprobs\"][\"tokens\"]\n",
    "#     t_i = -1\n",
    "#     for t in tokens:\n",
    "#         t_i += 1\n",
    "#         r = [(np.e**v, k) for (k, v) in choice[\"logprobs\"][\"top_logprobs\"][t_i].items()]\n",
    "#         r.sort(reverse=True)\n",
    "#         print(sum([v for (v, k) in r]))\n",
    "#         rd = dict([(k, v) for (v, k) in r])\n",
    "#         r = [k for (v, k) in r]\n",
    "#         d[t] = (rd, r, np.e**choice[\"logprobs\"][\"token_logprobs\"][t_i])\n",
    "#     print('|'.join([' '.join((s.replace(\"\\n\", \"⏎\"),'%.2f' % (d[s][2] * 100),\n",
    "#                               str(d[s][1].index(s) + 1) if s in d[s][1] else \"<100\")) for s in tokens]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
